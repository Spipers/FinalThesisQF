{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load in the specific data needed\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import datetime\n",
    "import hvplot.polars\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.interpolate import bisplrep, bisplev\n",
    "from datetime import timedelta\n",
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>T</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>spread_option</th>\n",
       "      <th>volume_option</th>\n",
       "      <th>...</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>2Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.344281</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-1.542073</td>\n",
       "      <td>-0.465828</td>\n",
       "      <td>0.582528</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.348925</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.062773</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.759052</td>\n",
       "      <td>-0.487390</td>\n",
       "      <td>1.099089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.346461</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-0.482466</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.716917</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>1.904057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.349742</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>0.088029</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.693295</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.355280</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>0.650114</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.674080</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.554578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72342</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.623205</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.060814</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>-0.411923</td>\n",
       "      <td>-0.288466</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72343</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.629719</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.156130</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>2.220414</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72344</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.639880</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.345361</td>\n",
       "      <td>-0.071801</td>\n",
       "      <td>0.110670</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>-0.200060</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72345</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.533190</td>\n",
       "      <td>-0.037014</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>-0.046135</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72346</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.659719</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.718215</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.208890</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>-0.206643</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72347 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker       date cp_flag  impl_volatility         T  moneyness  \\\n",
       "0       AAPL 2019-01-02       C         0.344281 -0.854054  -1.651490   \n",
       "1       AAPL 2019-01-02       C         0.348925 -0.854054  -1.062773   \n",
       "2       AAPL 2019-01-02       C         0.346461 -0.854054  -0.482466   \n",
       "3       AAPL 2019-01-02       C         0.349742 -0.854054   0.088029   \n",
       "4       AAPL 2019-01-02       C         0.355280 -0.854054   0.650114   \n",
       "...      ...        ...     ...              ...       ...        ...   \n",
       "72342   TSLA 2020-12-31       C         0.623205  1.331335   1.060814   \n",
       "72343   TSLA 2020-12-31       C         0.629719  1.331335   1.156130   \n",
       "72344   TSLA 2020-12-31       C         0.639880  1.331335   1.345361   \n",
       "72345   TSLA 2020-12-31       C         0.648805  1.331335   1.533190   \n",
       "72346   TSLA 2020-12-31       C         0.659719  1.331335   1.718215   \n",
       "\n",
       "       prev2_day_iv  prev_day_iv  spread_option  volume_option  ...   1Y_bond  \\\n",
       "0         -1.519873    -1.542073      -0.465828       0.582528  ...  1.371916   \n",
       "1         -1.519873    -0.759052      -0.487390       1.099089  ...  1.371916   \n",
       "2         -1.519873    -0.716917      -0.498171       1.904057  ...  1.371916   \n",
       "3         -1.519873    -0.693295      -0.508952       0.981316  ...  1.371916   \n",
       "4         -1.519873    -0.674080      -0.508952       0.554578  ...  1.371916   \n",
       "...             ...          ...            ...            ...  ...       ...   \n",
       "72342     -0.140819     0.024429      -0.411923      -0.288466  ... -1.213802   \n",
       "72343     -0.140819     0.024429      -0.455047       2.220414  ... -1.213802   \n",
       "72344     -0.071801     0.110670      -0.476609      -0.200060  ... -1.213802   \n",
       "72345     -0.037014     0.158024      -0.455047      -0.046135  ... -1.213802   \n",
       "72346      0.001623     0.208890      -0.455047      -0.206643  ... -1.213802   \n",
       "\n",
       "        2Y_bond  CLOSE_vix   FF_rate  HIGH_vix   LOW_vix  OPEN_vix  \\\n",
       "0      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "1      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "2      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "3      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "4      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "...         ...        ...       ...       ...       ...       ...   \n",
       "72342 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72343 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72344 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72345 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72346 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "\n",
       "       gold_price  reces_indi  spread_vix  \n",
       "0       -1.353864   -0.709827   -0.133918  \n",
       "1       -1.353864   -0.709827   -0.133918  \n",
       "2       -1.353864   -0.709827   -0.133918  \n",
       "3       -1.353864   -0.709827   -0.133918  \n",
       "4       -1.353864   -0.709827   -0.133918  \n",
       "...           ...         ...         ...  \n",
       "72342    1.438222    0.268029   -0.552678  \n",
       "72343    1.438222    0.268029   -0.552678  \n",
       "72344    1.438222    0.268029   -0.552678  \n",
       "72345    1.438222    0.268029   -0.552678  \n",
       "72346    1.438222    0.268029   -0.552678  \n",
       "\n",
       "[72347 rows x 30 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_scaled_p.parquet')\n",
    "data_train_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_scaled_c.parquet')\n",
    "\n",
    "data_val_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_val_tech_scaled_p.parquet')\n",
    "data_val_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_val_tech_scaled_c.parquet')\n",
    "\n",
    "# data_test_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_p.parquet')\n",
    "# data_test_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_c.parquet')\n",
    "\n",
    "data_test_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_p_total.parquet')\n",
    "data_test_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_c_total.parquet')\n",
    "\n",
    "tot_data_train_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_val_tech_scaled_c.parquet')\n",
    "tot_data_train_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_val_tech_scaled_p.parquet')\n",
    "\n",
    "firm_data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Firm_variables/daily_firm_data_median_new.parquet')\n",
    "\n",
    "data_train = pd.concat([data_train_p, data_train_c], axis=0)\n",
    "data_val = pd.concat([data_val_p, data_val_c], axis=0)\n",
    "data_test = pd.concat([data_test_p, data_test_c], axis=0)\n",
    "data_train_tot = pd.concat([tot_data_train_c, tot_data_train_p], axis=0)\n",
    "\n",
    "# # List of columns to drop\n",
    "columns_to_drop = ['trading_days_till_exp'] + list(firm_data.columns[2:]) + ['moneyness_squared','tau_squared', 'moneyness_tau', 'best_offer_option', 'best_bid_option', 'ASKHI', 'AKS', 'BID', 'BIDLO', 'PRC', 'RETX']\n",
    "# # columns_to_drop = ['trading_days_till_exp']\n",
    "\n",
    "# # Drop columns from datasets if they exist\n",
    "data_train = data_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "data_val = data_val.drop(columns=columns_to_drop, errors='ignore')\n",
    "data_test = data_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "data_train_tot = data_train_tot.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure that data_val and data_test have the same column order as data_train\n",
    "data_val = data_val[data_train.columns]\n",
    "data_test = data_test[data_train.columns]\n",
    "tot_data_train = data_train_tot[data_train.columns]\n",
    "\n",
    "data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C']\n",
    "data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P']\n",
    "\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv', 'BIDLO', 'OPEN_vix','hi-lo_stock','FF_rate', 'gold_price', 'reces_indi', 'cumulative_return','spread_vix', 'vol_stock','5_day_rolling_return_stock','spread_stock','1Y_bond','CLOSE_vix','RET','moneyness','10Y_RIR' ]\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv', 'cumulative_return', 'gold_price', 'reces_indi','FF_rate','hi-lo_stock', 'PRC_actual' , 'CLOSE_vix' ]\n",
    "\n",
    "# data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C'][top_features_c]\n",
    "# data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "\n",
    "\n",
    "data_train_tot_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'AMZN', 'META', 'MSFT', 'NVDA', 'TSLA'], dtype=object)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = data_test['Ticker'].unique()\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Set random seeds for reproducibility\n",
    "# def set_random_seed(seed=42):\n",
    "#     np.random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "# def build_nn(input_shape, **kwargs):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(1))  # Single output for regression task\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "# def feature_permutation_importance_nn(train_data, verbose=True):\n",
    "#     set_random_seed(42)  # Ensure the seed is set before training\n",
    "    \n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Drop unwanted columns\n",
    "#     y_train = train_data['impl_volatility']  # Target variable (implied volatility)\n",
    "\n",
    "#     # Wrapping the Keras model with KerasRegressor\n",
    "#     nn_model = KerasRegressor(model=build_nn, input_shape=X_train.shape[1], verbose=0)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Training Neural Network model...\")\n",
    "\n",
    "#     # Fit the NN model\n",
    "#     nn_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "#     # Perform permutation importance\n",
    "#     perm_importance = permutation_importance(\n",
    "#         estimator=nn_model, \n",
    "#         X=X_train, \n",
    "#         y=y_train, \n",
    "#         n_repeats=10, \n",
    "#         random_state=42,  # Ensure random_state for permutation importance is set\n",
    "#         scoring='r2'\n",
    "#     )\n",
    "\n",
    "#     # Create a DataFrame for feature importances\n",
    "#     feature_importances = pd.DataFrame({\n",
    "#         'Feature': X_train.columns,\n",
    "#         'Importance': perm_importance.importances_mean,\n",
    "#         'Importance_std': perm_importance.importances_std\n",
    "#     })\n",
    "\n",
    "#     # Extract and return feature importances\n",
    "#     feature_importances = pd.Series(nn_model.feature_importances_, index=X_train.columns)\n",
    "#     return feature_importances\n",
    "\n",
    "#     # feature_importances = feature_importances.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "#     # return feature_importances\n",
    "\n",
    "# # Call options feature importance\n",
    "# print(\"Evaluating features for Call options (NN model)...\")\n",
    "# feature_importances_call_nn = feature_permutation_importance_nn(data_train_tot_c, verbose=True)\n",
    "# print(feature_importances_call_nn)\n",
    "\n",
    "# # Put options feature importance\n",
    "# print(\"\\nEvaluating features for Put options (NN model)...\")\n",
    "# feature_importances_put_nn = feature_permutation_importance_nn(data_train_tot_p, verbose=True)\n",
    "# print(feature_importances_put_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>T</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>spread_option</th>\n",
       "      <th>volume_option</th>\n",
       "      <th>...</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>2Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.344281</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-1.542073</td>\n",
       "      <td>-0.465828</td>\n",
       "      <td>0.582528</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.348925</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.062773</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.759052</td>\n",
       "      <td>-0.487390</td>\n",
       "      <td>1.099089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.346461</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-0.482466</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.716917</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>1.904057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.349742</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>0.088029</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.693295</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>C</td>\n",
       "      <td>0.355280</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>0.650114</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.674080</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.554578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72342</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.623205</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.060814</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>-0.411923</td>\n",
       "      <td>-0.288466</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72343</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.629719</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.156130</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>2.220414</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72344</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.639880</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.345361</td>\n",
       "      <td>-0.071801</td>\n",
       "      <td>0.110670</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>-0.200060</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72345</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.533190</td>\n",
       "      <td>-0.037014</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>-0.046135</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72346</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>C</td>\n",
       "      <td>0.659719</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>1.718215</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.208890</td>\n",
       "      <td>-0.455047</td>\n",
       "      <td>-0.206643</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-1.207416</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>-0.145889</td>\n",
       "      <td>0.015395</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>-0.552678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72347 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker       date cp_flag  impl_volatility         T  moneyness  \\\n",
       "0       AAPL 2019-01-02       C         0.344281 -0.854054  -1.651490   \n",
       "1       AAPL 2019-01-02       C         0.348925 -0.854054  -1.062773   \n",
       "2       AAPL 2019-01-02       C         0.346461 -0.854054  -0.482466   \n",
       "3       AAPL 2019-01-02       C         0.349742 -0.854054   0.088029   \n",
       "4       AAPL 2019-01-02       C         0.355280 -0.854054   0.650114   \n",
       "...      ...        ...     ...              ...       ...        ...   \n",
       "72342   TSLA 2020-12-31       C         0.623205  1.331335   1.060814   \n",
       "72343   TSLA 2020-12-31       C         0.629719  1.331335   1.156130   \n",
       "72344   TSLA 2020-12-31       C         0.639880  1.331335   1.345361   \n",
       "72345   TSLA 2020-12-31       C         0.648805  1.331335   1.533190   \n",
       "72346   TSLA 2020-12-31       C         0.659719  1.331335   1.718215   \n",
       "\n",
       "       prev2_day_iv  prev_day_iv  spread_option  volume_option  ...   1Y_bond  \\\n",
       "0         -1.519873    -1.542073      -0.465828       0.582528  ...  1.371916   \n",
       "1         -1.519873    -0.759052      -0.487390       1.099089  ...  1.371916   \n",
       "2         -1.519873    -0.716917      -0.498171       1.904057  ...  1.371916   \n",
       "3         -1.519873    -0.693295      -0.508952       0.981316  ...  1.371916   \n",
       "4         -1.519873    -0.674080      -0.508952       0.554578  ...  1.371916   \n",
       "...             ...          ...            ...            ...  ...       ...   \n",
       "72342     -0.140819     0.024429      -0.411923      -0.288466  ... -1.213802   \n",
       "72343     -0.140819     0.024429      -0.455047       2.220414  ... -1.213802   \n",
       "72344     -0.071801     0.110670      -0.476609      -0.200060  ... -1.213802   \n",
       "72345     -0.037014     0.158024      -0.455047      -0.046135  ... -1.213802   \n",
       "72346      0.001623     0.208890      -0.455047      -0.206643  ... -1.213802   \n",
       "\n",
       "        2Y_bond  CLOSE_vix   FF_rate  HIGH_vix   LOW_vix  OPEN_vix  \\\n",
       "0      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "1      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "2      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "3      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "4      1.469739   0.476238  1.124610  0.475155  0.662651  0.674867   \n",
       "...         ...        ...       ...       ...       ...       ...   \n",
       "72342 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72343 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72344 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72345 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "72346 -1.207416  -0.034979 -1.160795 -0.145889  0.015395 -0.057404   \n",
       "\n",
       "       gold_price  reces_indi  spread_vix  \n",
       "0       -1.353864   -0.709827   -0.133918  \n",
       "1       -1.353864   -0.709827   -0.133918  \n",
       "2       -1.353864   -0.709827   -0.133918  \n",
       "3       -1.353864   -0.709827   -0.133918  \n",
       "4       -1.353864   -0.709827   -0.133918  \n",
       "...           ...         ...         ...  \n",
       "72342    1.438222    0.268029   -0.552678  \n",
       "72343    1.438222    0.268029   -0.552678  \n",
       "72344    1.438222    0.268029   -0.552678  \n",
       "72345    1.438222    0.268029   -0.552678  \n",
       "72346    1.438222    0.268029   -0.552678  \n",
       "\n",
       "[72347 rows x 30 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_tot_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# def build_nn(input_dim, **kwargs):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(1))  # Single output for regression task\n",
    "    \n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def feature_permutation_importance_nn(train_data, verbose=True):\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Drop unwanted columns\n",
    "#     y_train = train_data['impl_volatility']  # Target variable (implied volatility)\n",
    "\n",
    "#     # Wrapping the Keras model with KerasRegressor\n",
    "#     nn_model = KerasRegressor(model=build_nn, input_dim=X_train.shape[1], verbose=0)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Training Neural Network model for Call options...\")\n",
    "\n",
    "#     # Fit the NN model\n",
    "#     nn_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "#     # Perform permutation importance\n",
    "#     perm_importance = permutation_importance(\n",
    "#         estimator=nn_model, \n",
    "#         X=X_train, \n",
    "#         y=y_train, \n",
    "#         n_repeats=10, \n",
    "#         random_state=42, \n",
    "#         scoring='r2'\n",
    "#     )\n",
    "\n",
    "#     # Create a DataFrame for feature importances\n",
    "#     feature_importances = pd.DataFrame({\n",
    "#         'Feature': X_train.columns,\n",
    "#         'Importance': perm_importance.importances_mean,\n",
    "#         'Importance_std': perm_importance.importances_std\n",
    "#     })\n",
    "\n",
    "#     feature_importances = feature_importances.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "#     return feature_importances\n",
    "\n",
    "# # Call options feature importance\n",
    "# print(\"Evaluating features for Call options (NN model)...\")\n",
    "# feature_importances_call_nn = feature_permutation_importance_nn(data_train_tot_c, verbose=True)\n",
    "# print(feature_importances_call_nn)\n",
    "\n",
    "# # Put options feature importance\n",
    "# print(\"\\nEvaluating features for Put options (NN model)...\")\n",
    "# feature_importances_put_nn = feature_permutation_importance_nn(data_train_tot_p, verbose=True)\n",
    "# print(feature_importances_put_nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating features for Call options (NN model)...\n",
      "Training Neural Network model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Call options\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating features for Call options (NN model)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m feature_importances_call_nn \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_permutation_importance_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_tot_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# selected_features_call_nn = select_top_features(feature_importances_call_nn, threshold=0.85)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Put options\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating features for Put options (NN model)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 49\u001b[0m, in \u001b[0;36mfeature_permutation_importance_nn\u001b[0;34m(train_data, verbose)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Neural Network model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Fit the NN model (with verbose = 0 to avoid too much output)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Perform permutation importance using the custom prediction function\u001b[39;00m\n\u001b[1;32m     52\u001b[0m perm_importance \u001b[38;5;241m=\u001b[39m permutation_importance(\n\u001b[1;32m     53\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mnn_model, \n\u001b[1;32m     54\u001b[0m     X\u001b[38;5;241m=\u001b[39mX_train, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_nn(input_shape):\n",
    "    \"\"\"\n",
    "    Build a simple Neural Network model using Keras.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape (int): Number of input features.\n",
    "    \n",
    "    Returns:\n",
    "    - model: Compiled Neural Network model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Single output for regression task\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def feature_permutation_importance_nn(train_data, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform permutation importance for a Neural Network on the training data to identify the top features.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data (pd.DataFrame): The training dataset.\n",
    "    - verbose (bool): If True, prints progress information.\n",
    "    \n",
    "    Returns:\n",
    "    - feature_importances (pd.DataFrame): A dataframe containing feature names and their importance.\n",
    "    \"\"\"\n",
    "    # Prepare the train data\n",
    "    X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Drop unwanted columns\n",
    "    y_train = train_data['impl_volatility']  # Target variable (implied volatility)\n",
    "\n",
    "    # Build and train the Neural Network model\n",
    "    nn_model = build_nn(X_train.shape[1])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training Neural Network model...\")\n",
    "\n",
    "    # Fit the NN model (with verbose = 0 to avoid too much output)\n",
    "    nn_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Perform permutation importance using the custom prediction function\n",
    "    perm_importance = permutation_importance(\n",
    "        estimator=nn_model, \n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        n_repeats=10, \n",
    "        random_state=42, \n",
    "        scoring='r2'\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Importance_std': perm_importance.importances_std\n",
    "    })\n",
    "\n",
    "    # Sort by importance\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# Call options\n",
    "print(\"Evaluating features for Call options (NN model)...\")\n",
    "feature_importances_call_nn = feature_permutation_importance_nn(data_train_tot_c, verbose=True)\n",
    "# selected_features_call_nn = select_top_features(feature_importances_call_nn, threshold=0.85)\n",
    "\n",
    "# Put options\n",
    "print(\"\\nEvaluating features for Put options (NN model)...\")\n",
    "feature_importances_put_nn = feature_permutation_importance_nn(data_train_tot_p, verbose=True)\n",
    "# selected_features_put_nn = select_top_features(feature_importances_put_nn, threshold=0.85)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6MAAAIjCAYAAAD2oc1wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACHB0lEQVR4nOzde3zP9f//8ft72/v9toNtzDTTbGMOy/kQOYUQSVGJzCeHiEiSJCpMJRSRDnT45FBG0cmngxyiJKcwFXNsDkWRmmGzvb33+v3hu/fPux1s7P1+83a7Xi77fHq/Xs/X6/V4vR57b7t7vd6vl8kwDEMAAAAAALiRj6cLAAAAAABcewijAAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAHCV6tu3r2JiYjxdxiUxmUxKTEz0dBleafPmzWrWrJkCAwNlMpmUnJzs6ZK8zoEDB2QymTR37lxPl1Kg06dPq3z58lqwYIGnS/GomJgY9e3b95KW/ffPqdmzZ6tSpUrKysoqmeIAEEYBwF1+/vlndevWTdHR0SpVqpQqVqyo9u3b69VXX/V0aflKSkrSjBkzPLb93D/48/u66aabXLLNI0eOKDEx8aoNcDabTffee6/+/vtvTZ8+Xe+9956io6Ndtr01a9Y49cVsNqty5crq3bu3fv3110ta5wsvvKBPP/20WMucOHFCTzzxhKpXr65SpUqpbNmy6tChgz7//PNLqiGXp98Dl+OVV15R6dKldd999zmmJSYmymQy6brrrlNGRkaeZWJiYtS5c2enabm9nTZtWp7xc+fOlclk0o8//lhoLRd+n7z//vv5jmnevLlMJpNq1apVlN3ziL59+yo7O1tvvvmmp0sBvIafpwsAgGvBDz/8oDZt2qhSpUp68MEHFRERocOHD2vDhg165ZVX9Mgjj3i6xDySkpL0yy+/aPjw4R6to2fPnurUqZPTtPDwcJds68iRI5owYYJiYmJUr149l2zDlfbv36+DBw/q7bff1oABA9y23WHDhunGG2+UzWbT1q1b9dZbb+mLL77Qzz//rMjIyGKt64UXXlC3bt3UtWvXIo3fvXu32rZtq+PHj6tfv35q1KiR0tLStGDBAt1xxx0aOXKkXnrppUvYq4LfA9HR0crMzJTZbL6k9bqazWbTK6+8oscee0y+vr555h87dkyzZs3S448/XuR1vvTSSxo8eLACAgIuua5SpUopKSlJ//nPf5ymHzhwQD/88INKlSp1yet2h1KlSqlPnz56+eWX9cgjj8hkMnm6JOCqRxgFADeYOHGiQkJCtHnzZoWGhjrNO3bsmGeKuko0aNAgzx+vV5uzZ8/KYrHIx8e1FyTlfi/9+3vscpw5c0aBgYGFjmnZsqW6desmSerXr5+qVaumYcOGad68eRozZkyJ1fJvNptN3bp10z///KPvvvtOTZo0ccx77LHH1KtXL02dOlWNGjVSjx49Smy7JpPpig5On3/+uY4fP67u3bvnO79evXp66aWXNGTIEPn7+190ffXq1VNycrJmz56tESNGXHJdnTp10tKlS/XXX3+pXLlyjulJSUm67rrrVLVqVf3zzz+XvH536N69u1588UWtXr1at9xyi6fLAa56XKYLAG6wf/9+1axZM9+QUL58+TzT3n//fTVs2FD+/v4qW7as7rvvPh0+fPii28nJydGMGTNUs2ZNlSpVStddd50GDRqU7x94X331lVq1aqXSpUsrODhYN954o5KSkiRJrVu31hdffKGDBw86Lq+78POpWVlZGj9+vOLi4mS1WhUVFaVRo0bl+SxVVlaWHnvsMYWHh6t06dK688479dtvv110P4pj165d6tatm8qWLatSpUqpUaNGWrp0qdOYv//+WyNHjlTt2rUVFBSk4OBg3Xbbbdq+fbtjzJo1a3TjjTdKOh+ocvc793OBBX32rHXr1mrdurXTekwmkxYtWqRnnnlGFStWVEBAgNLT0yVJGzduVMeOHRUSEqKAgAC1atVK69atc1rnqVOnNHz4cMXExMhqtap8+fJq3769tm7dWuBx6Nu3r1q1aiVJuvfee2UymZzq+uabb9SyZUsFBgYqNDRUXbp0UUpKitM6ci/j3LlzpxISElSmTBm1aNGiwG0WJPeP9NTUVEdt+X2+OXd7uUwmk86cOaN58+Y5jn9hn/f76KOP9Msvv2j06NFOQVSSfH199eabbyo0NNTpc3+5/fnggw/01FNPKSIiQoGBgbrzzjud3mOFvQcK+sxocY7xvn371LdvX4WGhiokJET9+vXLc+nsihUr1KJFC4WGhiooKEjVq1fXU089VeDxyPXpp58qJiZGVapUyXf+uHHj9Oeff2rWrFkXXZd0/hLaW265RS+++KIyMzOLtEx+unTpIqvVqsWLFztNT0pKUvfu3fM9i3vu3Dk999xzqlKliqxWq2JiYvTUU0/l+VljGIaef/55XX/99QoICFCbNm20Y8eOfOtIS0vT8OHDFRUVJavVqri4OE2ZMkU5OTkX3YeGDRuqbNmy+uyzz4qx5wAKwplRAHCD6OhorV+/Xr/88stFPxM1ceJEjR07Vt27d9eAAQN0/Phxvfrqq7r55pu1bdu2Qs96DRo0SHPnzlW/fv00bNgwpaam6rXXXtO2bdu0bt06x2WFc+fO1QMPPKCaNWtqzJgxCg0N1bZt27Rs2TIlJCTo6aef1smTJ/Xbb79p+vTpkqSgoCBJ5wPvnXfeqe+//14DBw5UfHy8fv75Z02fPl179uxx+rzfgAED9P777yshIUHNmjXTN998o9tvv71Yxy4jI0N//fWX07SQkBCZzWbt2LFDzZs3V8WKFTV69GgFBgbqww8/VNeuXfXRRx/prrvukiT9+uuv+vTTT3XvvfcqNjZWf/75p9588021atVKO3fuVGRkpOLj4/Xss89q3LhxGjhwoFq2bClJatasWbHqzfXcc8/JYrFo5MiRysrKksVi0TfffKPbbrtNDRs21Pjx4+Xj46M5c+bolltu0dq1a9W4cWNJ0kMPPaQlS5Zo6NChuuGGG3TixAl9//33SklJUYMGDfLd3qBBg1SxYkW98MILjstmr7vuOknSypUrddttt6ly5cpKTExUZmamXn31VTVv3lxbt27NExTvvfdeVa1aVS+88IIMwyj2vu/fv1+SFBYWVqzl3nvvPQ0YMECNGzfWwIEDJanAQCVJ//vf/yRJvXv3znd+SEiIunTponnz5mnfvn2Ki4tzzJs4caJMJpOefPJJHTt2TDNmzFC7du2UnJwsf3//Qt8D+SnuMe7evbtiY2M1adIkbd26Ve+8847Kly+vKVOmSJJ27Nihzp07q06dOnr22WdltVq1b9++PP9wkZ8ffvihwO8T6fyZ7NxwOXjw4CKdHU1MTNTNN9+sWbNmXfLZ0YCAAHXp0kULFy7U4MGDJUnbt2/Xjh079M477+inn37Ks8yAAQM0b948devWTY8//rg2btyoSZMmKSUlRZ988olj3Lhx4/T888+rU6dO6tSpk7Zu3apbb71V2dnZTuvLyMhQq1at9Pvvv2vQoEGqVKmSfvjhB40ZM0ZHjx4t0meEGzRoUKQ+ACgCAwDgcsuXLzd8fX0NX19fo2nTpsaoUaOMr7/+2sjOznYad+DAAcPX19eYOHGi0/Sff/7Z8PPzc5rep08fIzo62vF67dq1hiRjwYIFTssuW7bMaXpaWppRunRpo0mTJkZmZqbT2JycHMd/33777U7rz/Xee+8ZPj4+xtq1a52mz54925BkrFu3zjAMw0hOTjYkGUOGDHEal5CQYEgyxo8fn8+R+v9SU1MNSfl+rV692jAMw2jbtq1Ru3Zt4+zZs0770KxZM6Nq1aqOaWfPnjXsdnue9VutVuPZZ591TNu8ebMhyZgzZ06eeqKjo40+ffrkmd6qVSujVatWjterV682JBmVK1c2MjIynOqqWrWq0aFDB6fjnJGRYcTGxhrt27d3TAsJCTEefvjhQo9PfnK3vXjxYqfp9erVM8qXL2+cOHHCMW379u2Gj4+P0bt3b8e08ePHG5KMnj17Fmt77777rnH8+HHjyJEjxhdffGHExMQYJpPJ2Lx5s2EYeb9X/729CwUGBuZ7nPNTr149IyQkpNAxL7/8siHJWLp0qVPNFStWNNLT0x3jPvzwQ0OS8corrzimFfQeyP3evPD7pLjH+IEHHnBa51133WWEhYU5Xk+fPt2QZBw/frzQ/fs3m81mmEwm4/HHH88zL3fbx48fN7799ltDkvHyyy875kdHRxu333670zKSHN+Lbdq0MSIiIhzf13PmzDEkOfpckAu/Lz///HPDZDIZhw4dMgzDMJ544gmjcuXKhmGcfy/VrFnTsVzuz5ABAwY4rW/kyJGGJOObb74xDMMwjh07ZlgsFuP22293em899dRThiSn76fnnnvOCAwMNPbs2eO0ztGjRxu+vr6OunL3Pb+fUwMHDjT8/f0L3WcARcNlugDgBu3bt9f69et15513avv27XrxxRfVoUMHVaxY0emS0o8//lg5OTnq3r27/vrrL8dXRESEqlatqtWrVxe4jcWLFyskJETt27d3WrZhw4YKCgpyLLtixQqdOnVKo0ePzvO5t6LckGPx4sWKj49XjRo1nLaTe2lm7na+/PJLSedvbnOh4t4QaeDAgVqxYoXTV926dfX333/rm2++Uffu3XXq1ClHHSdOnFCHDh20d+9e/f7775Ikq9Xq+Lym3W7XiRMnHJc9Fnbp6+Xo06eP0xmn5ORk7d27VwkJCTpx4oSj3jNnzqht27b67rvvHJcJhoaGauPGjTpy5Mhl13H06FElJyerb9++Klu2rGN6nTp11L59e0efLvTQQw8VaxsPPPCAwsPDFRkZqdtvv91xqW2jRo0uu/7CnDp1SqVLly50TO783Mukc/Xu3dtp2W7duqlChQr5Ho+LKYlj3LJlS504ccJRZ+4VEJ999lmRLh/N9ffff8swDJUpU6bQcTfffLPatGlTrEtvExMT9ccff2j27NlFruffbr31VpUtW1aLFi2SYRhatGiRevbsme/Y3OP27zOxuTde+uKLLySdPyudnZ2d56ZC+f2sWbx4sVq2bKkyZco4/fxq166d7Ha7vvvuu4vuQ5kyZZSZmZnvHYkBFA+X6QKAm9x44436+OOPlZ2dre3bt+uTTz7R9OnT1a1bNyUnJ+uGG27Q3r17ZRiGqlatmu86Crt75969e3Xy5Ml8P4Mq/f+b2+ReQnmpj1DYu3evUlJSCryjbe52Dh48KB8fnzyXWVavXr1Y26tataratWuXZ/qmTZtkGIbGjh2rsWPHFlhLxYoVlZOTo1deeUVvvPGGUlNTZbfbHWOKeylpUcXGxjq93rt3r6TzIbUgJ0+eVJkyZfTiiy+qT58+ioqKUsOGDdWpUyf17t1blStXLnYdBw8elJT/cY+Pj9fXX3+d5yZF/679YsaNG6eWLVvK19dX5cqVU3x8vPz8XP8nRunSpfNcwv1vp06dcoy90L/fYyaTSXFxcTpw4ECx67iUY1ypUiWncbnh8Z9//lFwcLB69Oihd955RwMGDNDo0aPVtm1b3X333erWrVuRboRlFOHy6sTERLVq1UqzZ8/WY489dtHxFwbY4v6DRS6z2ax7771XSUlJaty4sQ4fPqyEhIR8x+b+DLnw8mpJioiIUGhoqOO45/7/v3saHh6eJ5Tv3btXP/3000V/fhUm99hyN13g8hFGAcDNLBaLbrzxRt14442qVq2a+vXrp8WLF2v8+PHKycmRyWTSV199le/NPAr7zFpOTk6hD7kvqceh5OTkqHbt2nr55ZfznR8VFVUi2ylKHZI0cuRIdejQId8xuX/EvvDCCxo7dqweeOABPffccypbtqx8fHw0fPjwIp91KugPT7vdnm+v/v05vNztvPTSSwU+Nia3v927d1fLli31ySefaPny5XrppZc0ZcoUffzxx7rtttuKVO/lKMpnCC9Uu3btfP/BIFdhx+5yxMfHKzk5WYcOHcoT7nLlfg7xhhtuuKxtlbT8vmek/x90/P399d1332n16tX64osvtGzZMn3wwQe65ZZbtHz58gKXL1u2rEwmU5HuSnvzzTerdevWxQqX48ePV+vWrR03h7oUCQkJmj17thITE1W3bt2L9qYkQ19OTo7at2+vUaNG5Tu/WrVqF13HP//8o4CAgGK/TwDkRRgFAA/KvYzx6NGjks7frMUwDMXGxhbpj6ILValSRStXrlTz5s0L/SMp90zlL7/8kueMw4UK+gOwSpUq2r59u9q2bVvoH4nR0dHKycnR/v37nc4Y7d69+2K7UiS5ZwnNZnOhQUiSlixZojZt2ui///2v0/S0tDSnR0wUtj9lypRRWlpanukHDx4s0hnL3OMeHBx80XolqUKFChoyZIiGDBmiY8eOqUGDBpo4cWKxw2h0dLSk/I/7rl27VK5cuYs+uuVyFXbs/q04waNz585auHCh5s+fr2eeeSbP/PT0dH322WeqUaNGnu/13DPVuQzD0L59+1SnTp1i1+KqY+zj46O2bduqbdu2evnll/XCCy/o6aef1urVqwv8HvLz81OVKlUcdzK+mMTEREe4LIpWrVqpdevWmjJlisaNG1fkfblQixYtVKlSJa1Zs8Zxw6b85P4M2bt3r+Lj4x3T//zzT6WlpTmOe+7/79271+m9ePz48TyhvEqVKjp9+nSR3oMFSU1NdaoHwKXjM6MA4AarV6/O97K53M9E5Ya1u+++W76+vpowYUKe8YZh6MSJEwVuo3v37rLb7XruuefyzDt37pwjDNx6660qXbq0Jk2apLNnz+bZRq7AwECdPHky3+38/vvvevvtt/PMy8zM1JkzZyTJEZpmzpzpNKYod6ssivLlyzv+iM4N8xc6fvy44799fX3zHM/Fixc7PlOaKzcw5BecqlSpog0bNjjdnfPzzz8v0iN3pPOPhKhSpYqmTp2q06dPF1iv3W7Pc9zLly+vyMjIPI+zKIoKFSqoXr16mjdvntN+/fLLL1q+fLk6depU7HUWV5UqVXTy5Emnu6UePXrU6W6ouQIDA/M9/vnp1q2bbrjhBk2ePFk//vij07ycnBwNHjxY//zzj8aPH59n2fnz5zsu4ZXO/4PF0aNHncJ+Qe+Bf3PFMf7777/zTMs9o36x74OmTZvmOR4FuTBc/vvnQUFyPzv61ltvFWn8v5lMJs2cOVPjx4/X/fffX+C43OP2758ZuVdl5N6Zu127djKbzXr11Ved3uf5/azp3r271q9fr6+//jrPvLS0NJ07d+6i9W/duvWS77INwBlnRgHADR555BFlZGTorrvuUo0aNZSdna0ffvhBH3zwgWJiYtSvXz9J5/9of/755zVmzBgdOHBAXbt2VenSpZWamqpPPvlEAwcO1MiRI/PdRqtWrTRo0CBNmjRJycnJuvXWW2U2m7V3714tXrxYr7zyirp166bg4GBNnz5dAwYM0I033uh4nuT27duVkZGhefPmSTofnj744AONGDFCN954o4KCgnTHHXfo/vvv14cffqiHHnpIq1evVvPmzWW327Vr1y59+OGH+vrrr9WoUSPVq1dPPXv21BtvvKGTJ0+qWbNmWrVqlfbt21dix/X1119XixYtVLt2bT344IOqXLmy/vzzT61fv16//fab4zminTt31rPPPqt+/fqpWbNm+vnnn7VgwYI8ZzSrVKmi0NBQzZ49W6VLl1ZgYKCaNGmi2NhYDRgwQEuWLFHHjh3VvXt37d+/X++//36hjx65kI+Pj9555x3ddtttqlmzpvr166eKFSvq999/1+rVqxUcHKz//e9/OnXqlK6//np169ZNdevWVVBQkFauXKnNmzdr2rRpl3ScXnrpJd12221q2rSp+vfv73jsSEhIiNMzOF3lvvvu05NPPqm77rpLw4YNU0ZGhmbNmqVq1arluYFUw4YNtXLlSr388suKjIxUbGxsnmeI5rJYLFqyZInatm2rFi1aqF+/fmrUqJHS0tKUlJSkrVu36vHHH9d9992XZ9myZcs6lvnzzz81Y8YMxcXF6cEHH3SqJb/3QH5K+hg/++yz+u6773T77bcrOjpax44d0xtvvKHrr7/+os9+7dKli9577z3t2bOnSFdYjB8/Xm3atClyba1atVKrVq307bffFnmZ/Grs0qVLoWPq1q2rPn366K233lJaWppatWqlTZs2ad68eeratauj5vDwcI0cOVKTJk1S586d1alTJ23btk1fffWV05UPkvTEE09o6dKl6ty5s/r27auGDRvqzJkz+vnnn7VkyRIdOHAgzzIX2rJli/7++++L1g6giDxwB18AuOZ89dVXxgMPPGDUqFHDCAoKMiwWixEXF2c88sgjxp9//pln/EcffWS0aNHCCAwMNAIDA40aNWoYDz/8sLF7927HmIIel/HWW28ZDRs2NPz9/Y3SpUsbtWvXNkaNGmUcOXLEadzSpUuNZs2aGf7+/kZwcLDRuHFjY+HChY75p0+fNhISEozQ0FBDktO2srOzjSlTphg1a9Y0rFarUaZMGaNhw4bGhAkTjJMnTzrGZWZmGsOGDTPCwsKMwMBA44477jAOHz5crEe7vPTSS4WO279/v9G7d28jIiLCMJvNRsWKFY3OnTsbS5YscYw5e/as8fjjjxsVKlQw/P39jebNmxvr16/P81gWwzCMzz77zLjhhhsMPz+/PI/vmDZtmlGxYkXDarUazZs3N3788ccCH+3y78er5Nq2bZtx9913G2FhYYbVajWio6ON7t27G6tWrTIMwzCysrKMJ554wqhbt65RunRpIzAw0Khbt67xxhtvFHocLrbtlStXGs2bN3f0+4477jB27tzpNObCR38UxcX29ULLly83atWqZVgsFqN69erG+++/n++jXXbt2mXcfPPNhr+/f57HchTk2LFjxogRI4y4uDjDarUaoaGhRrt27RyPc8mv5oULFxpjxowxypcvb/j7+xu33367cfDgQaexBb0H8nu0i2Fc3jHOfUxKamqqYRiGsWrVKqNLly5GZGSkYbFYjMjISKNnz555HkmSn6ysLKNcuXLGc889V6RtG8b5x6pIKvTRLhfKPY4q5qNdCvPvR7sYxvlH1UyYMMGIjY01zGazERUVZYwZM8bpcU6GYRh2u92YMGGC4z3eunVr45dffsn3kUynTp0yxowZY8TFxRkWi8UoV66c0axZM2Pq1KlOj9vK7+fUk08+aVSqVMnpETIALp3JMC7hadYAAABXoTVr1qhNmzZavHixunXr5ulyXOa5557TnDlztHfv3gJvdoTiycrKUkxMjEaPHq1HH33U0+UAXoHPjAIAAHiZxx57TKdPn9aiRYs8XYrXmDNnjsxm8yU/1gZAXnxmFAAAwMsEBQUV6ZmZKLqHHnqIIAqUMM6MAgAAAADcjs+MAgAAAADcjjOjAAAAAAC3I4wCAAAAANyOGxjhsuXk5OjIkSMqXbq0TCaTp8sBAAAA4CGGYejUqVOKjIyUj0/h5z4Jo7hsR44cUVRUlKfLAAAAAHCFOHz4sK6//vpCxxBGcdlKly4tSUpNTVXZsmU9XA1cwWazafny5br11ltlNps9XQ5cgB57N/rr/eixd6O/3s+bepyenq6oqChHRigMYRSXLffS3NKlSys4ONjD1cAVbDabAgICFBwcfNX/gET+6LF3o7/ejx57N/rr/byxx0X5+B43MAIAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuJ2fpwuA9zh48KDS0tI8XQZcwG63S5JSU1Pl6+vr4WrgCvTYu9Ff70ePvRv99X6X2+Pg4GCFh4eXdFkuRxhFiRk4cLxycjxdBVzBYjFrzJg+6t17tLKzbZ4uBy5Aj70b/fV+9Ni70V/vd7k9DguzKilp1lUXSAmjKDFW6yBZrfGeLgMuYLHYJaUoNHSysrP5F1lvRI+9G/31fvTYu9Ff73c5Pc7MPKwTJ6YpPT2dMIprl79/pPz9q3i6DLiA2WyTlKKAgFiZzWZPlwMXoMfejf56P3rs3eiv97vcHmdllXxN7sANjAAAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYvQL07dtXXbt2ddv2YmJiNGPGDLdtDwAAAAD+zc/TBcD9Nm/erMDAQE+XAQAAAOAaRhi9QHZ2tiwWi6fLcLnw8HBPlwAAAADgGufVl+m2bt1aQ4cO1dChQxUSEqJy5cpp7NixMgxD0vnLVZ977jn17t1bwcHBGjhwoCTp+++/V8uWLeXv76+oqCgNGzZMZ86ckSQ99dRTatKkSZ5t1a1bV88+++xFa7Lb7RoxYoRCQ0MVFhamUaNGOerJtWzZMrVo0cIxpnPnztq/f79j/i233KKhQ4c6LXP8+HFZLBatWrXqojVceJluQkKCevTo4TTfZrOpXLlymj9//kXXBQAAAACXwuvPjM6bN0/9+/fXpk2b9OOPP2rgwIGqVKmSHnzwQUnS1KlTNW7cOI0fP16StH//fnXs2FHPP/+83n33XR0/ftwRaOfMmaNevXpp0qRJ2r9/v6pUqSJJ2rFjh3766Sd99NFHF61n2rRpmjt3rt59913Fx8dr2rRp+uSTT3TLLbc4xpw5c0YjRoxQnTp1dPr0aY0bN0533XWXkpOT5ePjowEDBmjo0KGaNm2arFarJOn9999XxYoVndZTFL169dK9996r06dPKygoSJL09ddfKyMjQ3fddVe+y2RlZSkrK8vxOj09XZLk52eX2Wwr1vZxdcjtK/31XvTYu9Ff70ePvRv99X6X02OLxS6LxSy73S6bzfPfI8WpwWT8+7ScF2ndurWOHTumHTt2yGQySZJGjx6tpUuXaufOnYqJiVH9+vX1ySefOJYZMGCAfH199eabbzqmff/992rVqpXOnDmjUqVKqV69errnnns0duxYSefPln7zzTfasGHDRWuKjIzUY489pieeeEKSdO7cOcXGxqphw4b69NNP813mr7/+Unh4uH7++WfVqlVLZ8+eVWRkpGbPnq3u3btLOn9m9u6773aE6sLExMRo+PDhGj58uM6dO6cKFSro5Zdf1v333y/p/NnSnJwcLVq0KN/lExMTNWHChDzTk5KSFBAQcNHtAwAAAPBOGRkZSkhI0MmTJxUcHFzoWK8/M3rTTTc5gqgkNW3aVNOmTZPdbpckNWrUyGn89u3b9dNPP2nBggWOaYZhKCcnR6mpqYqPj1evXr307rvvOi75XbhwoUaMGHHRWk6ePKmjR486Xebr5+enRo0aOV2qu3fvXo0bN04bN27UX3/9pZycHEnSoUOHVKtWLZUqVUr333+/3n33XXXv3l1bt27VL7/8oqVLlxb7+Pj5+al79+5asGCB7r//fp05c0afffZZgUFUksaMGeO0v+np6YqKitK8eXEKCKhX7Bpw5TObbUpIWKGkpPay2cyeLgcuQI+9G/31fvTYu9Ff73c5Pc7ISFVa2mjNnz9ZsbGxLqqw6HKvmiwKrw+jF/Pvu8qePn1agwYN0rBhw/KMrVSpkiSpZ8+eevLJJ7V161ZlZmbq8OHDeT53eTnuuOMORUdH6+2331ZkZKRycnJUq1YtZWdnO8YMGDBA9erV02+//aY5c+bolltuUXR09CVtr1evXmrVqpWOHTumFStWyN/fXx07dixwvNVqdVwefKFz53z5AenlbDYzPfZy9Ni70V/vR4+9G/31fpfS4+xsX2Vn2+Tr6yuz2fPfH8WpwevD6MaNG51eb9iwQVWrVpWvr2++4xs0aKCdO3cqLi6uwHVef/31atWqlRYsWKDMzEy1b99e5cuXv2gtISEhqlChgjZu3Kibb75Z0vnLdLds2aIGDRpIkk6cOKHdu3fr7bffVsuWLSWdv0z432rXrq1GjRrp7bffVlJSkl577bWLbr8gzZo1U1RUlD744AN99dVXuvfee6+Ib2QAAAAA3svrw+ihQ4c0YsQIDRo0SFu3btWrr76qadOmFTj+ySef1E033aShQ4dqwIABCgwM1M6dO7VixQqnwNerVy+NHz9e2dnZmj59epHrefTRRzV58mRVrVpVNWrU0Msvv6y0tDTH/DJlyigsLExvvfWWKlSooEOHDmn06NH5riv3RkaBgYEF3myoqBISEjR79mzt2bNHq1evvqx1AQAAAMDFePWjXSSpd+/eyszMVOPGjfXwww/r0UcfdTzCJT916tTRt99+qz179qhly5aqX7++xo0bp8jISKdx3bp104kTJ5SRkaGuXbsWuZ7HH39c999/v/r06aOmTZuqdOnSTkHSx8dHixYt0pYtW1SrVi099thjeumll/JdV8+ePeXn56eePXuqVKlSRa4hP7169dLOnTtVsWJFNW/e/LLWBQAAAAAX4/VnRs1ms2bMmKFZs2blmXfgwIF8l7nxxhu1fPnyQtcbGhqqs2fPFrsePz8/zZgxw/Gcz/y0a9dOO3fudJqW302P//rrL509e1b9+/cvVg357Xd8fHy+2wAAAAAAV/D6MOqNbDabTpw4oWeeeUY33XST4/OmAAAAAHC18PrLdN0tKCiowK+1a9eWyDbWrVunChUqaPPmzZo9e7bTvLVr1xZaAwAAAABcCbz6zOiaNWvcvs3k5OQC51WsWLFEttG6desCL6lt1KhRoTUAAAAAwJXAq8OoJxT2SBh38Pf393gNAAAAAHAxXKYLAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA77qaLEpOZeUQ5OSGeLgMuYLHYJUkZGanKzvb1cDVwBXrs3eiv96PH3o3+er/L6XFm5mFXlOQWhFGUmKysN5WZ6ekq4AoWi1lSH6WljVZ2ts3T5cAF6LF3o7/ejx57N/rr/S63x2FhVgUHB5d8YS5mMgzD8HQRuLqlp6crJCREW7ZsUUgIZ0a9kd1uV0pKiuLj4+Xry7/IeiN67N3or/ejx96N/nq/y+1xcHCwwsPDXVBZ8eVmg5MnT140IHNmFCUmOjpaYWFhni4DLmCz2ZSSkqLY2FiZzWZPlwMXoMfejf56P3rs3eiv97tWe8wNjAAAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbsfddFFiDh48qLS0NE+XARew288/iDk1NZVbynupa6HHV9Jt7wEAAGEUJWjgwPHKyfF0FXAFi8WsMWP6qHdvHrbtra6FHoeFWZWUNItACgDAFYIwihJjtQ6S1Rrv6TLgAhaLXVKKQkMnKzvbO8+aXeu8vceZmYd14sQ0paenE0YBALhCEEZRYvz9I+XvX8XTZcAFzGabpBQFBFxbD2K+llwLPc7K8nQFAADgQtzACAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG5HGAUAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUZdqG/fvjKZTHm+9u3bV+g8V5k7d65CQ0Ndtn4AAAAAKCo/Txfg7Tp27Kg5c+Y4TQsPD7/ovOLIzs6WxWK59CIBAAAAwM04M+piVqtVERERTl++vr4XnVeY1q1ba+jQoRo+fLjKlSunDh06SJJefvll1a5dW4GBgYqKitKQIUN0+vRpSdKaNWvUr18/nTx50nEWNjExUZKUlZWlkSNHqmLFigoMDFSTJk20Zs0alxwPAAAAAJA4M3rVmjdvngYPHqx169Y5pvn4+GjmzJmKjY3Vr7/+qiFDhmjUqFF644031KxZM82YMUPjxo3T7t27JUlBQUGSpKFDh2rnzp1atGiRIiMj9cknn6hjx476+eefVbVq1TzbzsrKUlZWluN1enq6JMnPzy6z2ebK3YaH5PaV/novb++xxWKXxWKW3W6Xzead+1iY3H2+Fvf9WkGPvRv99X7e1OPi7IPJMAzDhbVc0/r27av3339fpUqVcky77bbbtHjx4kLnXUzr1q2Vnp6urVu3FjpuyZIleuihh/TXX39JOv+Z0eHDhystLc0x5tChQ6pcubIOHTqkyMhIx/R27dqpcePGeuGFF/KsNzExURMmTMgzPSkpSQEBARetHwAAAIB3ysjIUEJCgk6ePKng4OBCx3Jm1MXatGmjWbNmOV4HBgYWad7FNGzYMM+0lStXatKkSdq1a5fS09N17tw5nT17VhkZGQWGxJ9//ll2u13VqlVzmp6VlaWwsLB8lxkzZoxGjBjheJ2enq6oqCjNmxengIB6Rd4HXD3MZpsSElYoKam9bDazp8uBC3h7jzMyUpWWNlrz509WbGysp8txO5vNphUrVqh9+/Yym72vv6DH3o7+ej9v6nHuVZNFQRh1scDAQMXFxRV7XlHWe6EDBw6oc+fOGjx4sCZOnKiyZcvq+++/V//+/ZWdnV1gGD19+rR8fX21ZcuWPJ9Xzb2M99+sVqusVmue6efO+XrlH7H4/2w2Mz32ct7a4+xsX2Vn2+Tr63vV/5K/HGaz+Zre/2sBPfZu9Nf7eUOPi1M/YdRLbNmyRTk5OZo2bZp8fM7fl+rDDz90GmOxWGS3252m1a9fX3a7XceOHVPLli3dVi8AAACAaxt30/UScXFxstlsevXVV/Xrr7/qvffe0+zZs53GxMTE6PTp01q1apX++usvZWRkqFq1aurVq5d69+6tjz/+WKmpqdq0aZMmTZqkL774wkN7AwAAAMDbEUa9RN26dfXyyy9rypQpqlWrlhYsWKBJkyY5jWnWrJkeeugh9ejRQ+Hh4XrxxRclSXPmzFHv3r31+OOPq3r16uratas2b96sSpUqeWJXAAAAAFwDuEzXhebOnXtJ8y6moGeAPvbYY3rsscecpt1///1Or2fNmuV00yTp/HXdEyZMyPcOuQAAAADgCpwZBQAAAAC4HWH0CnPo0CEFBQUV+HXo0CFPlwgAAAAAl43LdK8wkZGRSk5OLnQ+AAAAAFztCKNXGD8/v0t+9igAAAAAXC24TBcAAAAA4HaEUQAAAACA2xFGAQAAAABuRxgFAAAAALgdYRQAAAAA4HbcTRclJjPziHJyQjxdBlzAYrFLkjIyUpWd7evhauAK3t7jzMzDni4BAAD8C2EUJSYr601lZnq6CriCxWKW1EdpaaOVnW3zdDlwgWuhx2FhVgUHB3u6DAAA8H8Ioygxb701QSEhnBn1Rna7XSkpKZo/f7J8fb3vrBmujR4HBwcrPDzc02UAAID/QxhFiYmOjlZYWJiny4AL2Gw2paSkKDY2Vmaz2dPlwAXoMQAAcDduYAQAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOGxihxBw8eFBpaWmeLgMuYLeffwZlamqq195p9VpXWI+5Cy0AAHAFwihKzMCB45WT4+kq4AoWi1ljxvRR797e+wzKa11hPQ4LsyopaRaBFAAAlCjCKEqM1TpIVmu8p8uAC1gsdkkpCg2drOxszox6o4J6nJl5WCdOTFN6ejphFAAAlCjCKEqMv3+k/P2reLoMuIDZbJOUooAAnkHprQrrcVaWZ2oCAADejRsYAQAAAADcjjAKAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wugVICYmRjNmzHDLtg4cOCCTyaTk5GS3bA8AAAAA8kMYLaa3335bLVu2VJkyZVSmTBm1a9dOmzZt8nRZRRYVFaWjR4+qVq1ani4FAAAAwDXsmgmj2dnZJbKeNWvWqGfPnlq9erXWr1+vqKgo3Xrrrfr9999LZP2u5uvrq4iICPn5+Xm6FAAAAADXsKs2jLZu3VpDhw7V0KFDFRISonLlymns2LEyDEPS+Utfn3vuOfXu3VvBwcEaOHCgJOn7779Xy5Yt5e/vr6ioKA0bNkxnzpyRJD311FNq0qRJnm3VrVtXzz77rCRpwYIFGjJkiOrVq6caNWronXfeUU5OjlatWlWkuo8dO6Y77rhD/v7+io2N1YIFC/KMefnll1W7dm0FBgYqKipKQ4YM0enTpyVJZ86cUXBwsJYsWeK0zKeffqrAwECdOnWq0O1feJluTk6Orr/+es2aNctpzLZt2+Tj46ODBw8WaZ8AAAAAoLiu6tNj8+bNU//+/bVp0yb9+OOPGjhwoCpVqqQHH3xQkjR16lSNGzdO48ePlyTt379fHTt21PPPP693331Xx48fdwTaOXPmqFevXpo0aZL279+vKlWqSJJ27Nihn376SR999FG+NWRkZMhms6ls2bJFqrlv3746cuSIVq9eLbPZrGHDhunYsWNOY3x8fDRz5kzFxsbq119/1ZAhQzRq1Ci98cYbCgwM1H333ac5c+aoW7dujmVyX5cuXbrIx8/Hx0c9e/ZUUlKSBg8e7Ji+YMECNW/eXNHR0fkul5WVpaysLMfr9PR0SZKfn11ms63I28fVI7ev9Nd7FdRji8Uui8Usu90um43+X61ye0cPvRc99m701/t5U4+Lsw8mI/dU4lWmdevWOnbsmHbs2CGTySRJGj16tJYuXaqdO3cqJiZG9evX1yeffOJYZsCAAfL19dWbb77pmPb999+rVatWOnPmjEqVKqV69erpnnvu0dixYyWdP1v6zTffaMOGDfnWMWTIEH399dfasWOHSpUqVWjNe/bsUfXq1bVp0ybdeOONkqRdu3YpPj5e06dP1/Dhw/NdbsmSJXrooYf0119/SZI2bdqkZs2a6fDhw6pQoYKOHTumihUrauXKlWrVqlWhNRw4cECxsbHatm2b6tWrp+TkZDVo0EAHDhxQpUqVlJOTo0qVKumZZ57RQw89lO86EhMTNWHChDzTk5KSFBAQUOj2AQAAAHivjIwMJSQk6OTJkwoODi507FV9ZvSmm25yBFFJatq0qaZNmya73S5JatSokdP47du366effnK6NNYwDOXk5Cg1NVXx8fHq1auX3n33XcclvwsXLtSIESPy3f7kyZO1aNEirVmz5qJBVJJSUlLk5+enhg0bOqbVqFFDoaGhTuNWrlypSZMmadeuXUpPT9e5c+d09uxZZWRkKCAgQI0bN1bNmjU1b948jR49Wu+//76io6N18803X7SGf6tXr57i4+OVlJSk0aNH69tvv9WxY8d07733FrjMmDFjnI5Jenq6oqKiNG9enAIC6hW7Blz5zGabEhJWKCmpvWw2s6fLgQsU1OOMjFSlpY3W/PmTFRsb68EKcTlsNptWrFih9u3by2zmPeyN6LF3o7/ez5t6nHvVZFFc1WH0YgIDA51enz59WoMGDdKwYcPyjK1UqZIkqWfPnnryySe1detWZWZm6vDhw+rRo0ee8VOnTtXkyZO1cuVK1alTp8RqPnDggDp37qzBgwdr4sSJKlu2rL7//nv1799f2dnZjjOPAwYM0Ouvv67Ro0drzpw56tevn1MwL45evXo5wmhSUpI6duyosLCwAsdbrVZZrdY808+d8yWoeDmbzUyPvdy/e5yd7avsbJt8fX2v+l+OkMxmM330cvTYu9Ff7+cNPS5O/Vd1GN24caPT6w0bNqhq1ary9fXNd3yDBg20c+dOxcXFFbjO66+/Xq1atdKCBQuUmZmp9u3bq3z58k5jXnzxRU2cOFFff/11nrOvhalRo4bOnTunLVu2OC7T3b17t9LS0hxjtmzZopycHE2bNk0+PufvL/Xhhx/mWdd//vMfjRo1SjNnztTOnTvVp0+fItfxbwkJCXrmmWe0ZcsWLVmyRLNnz77kdQEAAABAUVy1d9OVpEOHDmnEiBHavXu3Fi5cqFdffVWPPvpogeOffPJJ/fDDDxo6dKiSk5O1d+9effbZZxo6dKjTuF69emnRokVavHixevXq5TRvypQpGjt2rN59913FxMTojz/+0B9//OG4221hqlevro4dO2rQoEHauHGjtmzZogEDBsjf398xJi4uTjabTa+++qp+/fVXvffee/mGwzJlyujuu+/WE088oVtvvVXXX3/9RbdfkJiYGDVr1kz9+/eX3W7XnXfeecnrAgAAAICiuKrDaO/evZWZmanGjRvr4Ycf1qOPPup4hEt+6tSpo2+//VZ79uxRy5YtVb9+fY0bN06RkZFO47p166YTJ04oIyNDXbt2dZo3a9YsZWdnq1u3bqpQoYLja+rUqUWqec6cOYqMjFSrVq109913a+DAgU5nXuvWrauXX35ZU6ZMUa1atbRgwQJNmjQp33XlXrr7wAMPFGnbhenVq5e2b9+uu+66yykcAwAAAIArXNWX6ZrNZs2YMSPPczKl85+9zM+NN96o5cuXF7re0NBQnT17Nt95Ba23qCIiIvT55587Tbv//vudXj/22GN67LHHCh0jSb///rvCwsLUpUuXIm8/JiZG+d1AefDgwU6PdwEAAAAAV7qqw+i1KiMjQ0ePHtXkyZM1aNAgWSwWT5cEAAAAAMVyVV+me6VZu3atgoKCCvwqKS+++KJq1KihiIgIjRkzxmneCy+8UOD2b7vtthKrAQAAAAAux1V7ZnTNmjWeLiGPRo0aKTk52eXbSUxMVGJiYr7zHnroIXXv3j3feXwWFAAAAMCV4qoNo1cif3//Qh8b4w5ly5ZV2bJlPVoDAAAAAFwMl+kCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOu+mixGRmHlFOToiny4ALWCx2SVJGRqqys309XA1coaAeZ2Ye9lRJAADAyxFGUWKyst5UZqanq4ArWCxmSX2UljZa2dk2T5cDFyisx2FhVgUHB3umMAAA4LUIoygxb701QSEhnBn1Rna7XSkpKZo/f7J8fTkz6o0K63FwcLDCw8M9VBkAAPBWhFGUmOjoaIWFhXm6DLiAzWZTSkqKYmNjZTabPV0OXIAeAwAAd+MGRgAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigAAAAAwO24gRFKzMGDB5WWlubpMuACdvv5Z1CmpqZyN10vldtjAAAAdyGMosQMHDheOTmergKuYLGYNWZMH/XuzXNGvVVuj//66y9VqFDB0+UAAIBrAGEUJcZqHSSrNd7TZcAFLBa7pBSFhk5WdjZnRr2R3X5IUrpOnTpFGAUAAG5BGEWJ8fePlL9/FU+XARcwm22SUhQQwDMovZXNZpeU7ukyAADANYQbGAEAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8LoVSwxMVH16tW77PXExMRoxowZjtcmk0mffvrpZa8XAAAAAAri5+kCcOlGjhypRx55pMTXe/ToUZUpU6bE1wsAAAAAuQijF5GdnS2LxeLpMvIVFBSkoKCgEl9vREREia8TAAAAAC5EGP2X1q1bq1atWvLz89P777+v2rVr69VXX9UTTzyhtWvXKjAwULfeequmT5+ucuXKSZJycnI0depUvfXWWzp8+LCuu+46DRo0SE8//bQk6fDhw3r88ce1fPly+fj4qGXLlnrllVcUExMjSVqzZo1GjRqlHTt2yGw2q2bNmkpKSlJ0dHShtSYmJurTTz9VcnKyJKlv375KS0tTixYtNG3aNGVnZ+u+++7TjBkzZDabJUnHjh1T//79tXLlSkVEROj555/Ps16TyaRPPvlEXbt2zXe7WVlZysrKcrxOT0+XJPn52WU224p8rHH1yO0r/fVm9vP/a7fLZqPP3ia3p/TWe9Fj70Z/vZ839bg4+0AYzce8efM0ePBgrVu3Tmlpabrllls0YMAATZ8+XZmZmXryySfVvXt3ffPNN5KkMWPG6O2339b06dPVokULHT16VLt27ZJ0vhkdOnRQ06ZNtXbtWvn5+en5559Xx44d9dNPP8nHx0ddu3bVgw8+qIULFyo7O1ubNm2SyWS6pNpXr16tChUqaPXq1dq3b5969OihevXq6cEHH5R0PrAeOXJEq1evltls1rBhw3Ts2LFibWPSpEmaMGFCnul9+uxTQMCRS6obV4eEhBWeLgEutmfPHu3Zs8fTZcBFVqzgPezt6LF3o7/ezxt6nJGRUeSxJsMwDBfWctVp3bq10tPTtXXrVknS888/r7Vr1+rrr792jPntt98UFRWl3bt3q0KFCgoPD9drr72mAQMG5Fnf+++/r+eff14pKSmOgJmdna3Q0FB9+umnatSokcLCwrRmzRq1atWqWLXmd2Z0zZo12r9/v3x9fSVJ3bt3l4+PjxYtWqQ9e/aoevXq2rRpk2688UZJ0q5duxQfH6/p06dr+PDhki7tzGhUVJQ6dNikgIB6xdoHXB3MZpsSElYoKam9bDazp8uBC9hs+zRgwB5Vq1ZNcXFxni4HJcxms2nFihVq376940oZeBd67N3or/fzph6np6erXLlyOnnypIKDgwsdy5nRfDRs2NDx39u3b9fq1avz/Wzm/v37lZaWpqysLLVt2zbfdW3fvl379u1T6dKlnaafPXtW+/fv16233qq+ffuqQ4cOat++vdq1a6fu3burQoUKl1R7zZo1HUFUkipUqKCff/5ZkpSSkiI/Pz+n/atRo4ZCQ0OLtQ2r1Sqr1Zpn+rlzvgQVL2ezmemxl7LZzv/c8PX1vep/CaJgZrOZ/no5euzd6K/384YeF6d+wmg+AgMDHf99+vRp3XHHHZoyZUqecRUqVNCvv/5a6LpOnz6thg0basGCBXnmhYeHS5LmzJmjYcOGadmyZfrggw/0zDPPaMWKFbrpppuKXfu/m28ymZSTk1Ps9QAAAACAK/Gc0Yto0KCBduzYoZiYGMXFxTl9BQYGqmrVqvL399eqVasKXH7v3r0qX758nuVDQkIc4+rXr68xY8bohx9+UK1atZSUlFTi+1KjRg2dO3dOW7ZscUzbvXu30tLSSnxbAAAAAFAYwuhFPPzww/r777/Vs2dPbd68Wfv379fXX3+tfv36yW63q1SpUnryySc1atQozZ8/X/v379eGDRv03//+V5LUq1cvlStXTl26dNHatWuVmpqqNWvWaNiwYfrtt9+UmpqqMWPGaP369Tp48KCWL1+uvXv3Kj4+vsT3pXr16urYsaMGDRqkjRs3asuWLRowYID8/f1LfFsAAAAAUBjC6EVERkZq3bp1stvtuvXWW1W7dm0NHz5coaGh8vE5f/jGjh2rxx9/XOPGjVN8fLx69OjhuENtQECAvvvuO1WqVEl333234uPj1b9/f509e1bBwcEKCAjQrl27dM8996hatWoaOHCgHn74YQ0aNMgl+zNnzhxFRkaqVatWuvvuuzVw4ECVL1/eJdsCAAAAgIJwN11ctvT0dIWEhKht2y3y92/g6XLgAmazTX36fKl58zpxAyMvZbPt0eDBKYqPj1e1atU8XQ5KmM1m05dffqlOnTpd9TfGQP7osXejv97Pm3qcmw2KcjddzowCAAAAANyOMHoFq1mzpoKCgvL9yu/uvAAAAABwteDRLlewL7/8UjabLd951113nZurAQAAAICSQxi9gkVHR3u6BAAAAABwCS7TBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4HXfTRYnJzDyinJwQT5cBF7BY7JKkjIxUZWf7ergauILd/runSwAAANcYwihKTFbWm8rM9HQVcAWLxSypj9LSRis7O/9n3+Lqltvj0qVLe7oUAABwjSCMosS89dYEhYRwZtQb2e12paSkaP78yfL15cyoN8rtcbly5TxdCgAAuEYQRlFioqOjFRYW5uky4AI2m00pKSmKjY2V2Wz2dDlwgdweAwAAuAs3MAIAAAAAuB1hFAAAAADgdoRRAAAAAIDbEUYBAAAAAG7HDYxQYg4ePKi0tDRPlwEXsNvPP2c0NTXVK++mGxwcrPDwcE+XAQAAcE0hjKLEDBw4Xjk5nq4CrmCxmDVmTB/17u2dzxkNC7MqKWkWgRQAAMCNCKMoMVbrIFmt8Z4uAy5gsdglpSg0dLKys73rzGhm5mGdODFN6enphFEAAAA3IoyixPj7R8rfv4qny4ALmM02SSkKCPDO54xmZXm6AgAAgGsPNzACAAAAALgdYRQAAAAA4HaEUQAAAACA2xFGAQAAAABuRxgFAAAAALgdYRQAAAAA4HaEUQAAAACA2xFGAQAAAABuRxgFAAAAALgdYRQAAAAA4HaE0RI0d+5chYaGFjomMTFR9erVc3ktJpNJn376qcu3AwAAAACXws/TBcA1jh49qjJlyni6DAAAAADIF2HUy2RnZ8tisSgiIsLTpQAAAABAgQijFzh16pQeeughffrppwoODtaoUaP02WefqV69epoxY4b++ecfPfroo/rf//6nrKwstWrVSjNnzlTVqlULXOfkyZM1ffp0ZWRkqHv37goPDy9yPX379lVaWprq16+v1157TVlZWUpISNDMmTNlsVgkSa1bt1atWrXk5+en999/X7Vr19bq1atlMpn0ySefqGvXrpKk3377TU888YS+/vprZWVlKT4+Xq+//rqaNGkiSfrss880YcIE7dy5U5GRkerTp4+efvpp+fnl/RbJyspSVlaW43V6erokyc/PLrPZVuT9w9Ujt6/e2F+LxS6LxSy73S6bzfv2r6hy9/1aPgbejP56P3rs3eiv9/OmHhdnHwijFxgxYoTWrVunpUuX6rrrrtO4ceO0detWx2c8+/btq71792rp0qUKDg7Wk08+qU6dOmnnzp0ym8151vfhhx8qMTFRr7/+ulq0aKH33ntPM2fOVOXKlYtc06pVq1SqVCmtWbNGBw4cUL9+/RQWFqaJEyc6xsybN0+DBw/WunXr8l3H6dOn1apVK1WsWFFLly5VRESEtm7dqpycHEnS2rVr1bt3b82cOVMtW7bU/v37NXDgQEnS+PHj86xv0qRJmjBhQp7pffrsU0DAkSLvG64+CQkrPF2Ci/RRSkqKUlJSPF2Ix61Y4a09hkR/rwX02LvRX+/nDT3OyMgo8liTYRiGC2u5apw6dUphYWFKSkpSt27dJEknT55UZGSkHnzwQT388MOqVq2a1q1bp2bNmkmSTpw4oaioKM2bN0/33nuv5s6dq+HDhystLU2S1KxZM9WvX1+vv/66Yzs33XSTzp49q+Tk5IvW1LdvX/3vf//T4cOHFRAQIEmaPXu2nnjiCZ08eVI+Pj5q3bq10tPTtXXrVqdlLzwz+tZbb2nkyJE6cOCAypYtm2c77dq1U9u2bTVmzBjHtPfff1+jRo3SkSN5w2V+Z0ajoqLUocMmBQTUu+h+4epjNtuUkLBCSUntZbPl/YeXq1lGRqrS0kZr/vzJio2N9XQ5HmOz2bRixQq1b98+339cw9WN/no/euzd6K/386Yep6enq1y5cjp58qSCg4MLHcuZ0f/z66+/ymazqXHjxo5pISEhql69uiQpJSVFfn5+jstaJSksLEzVq1cv8GxKSkqKHnroIadpTZs21erVq4tcV926dR1BNHf506dP6/Dhw4qOjpYkNWzYsNB1JCcnq379+vkGUUnavn271q1b53S21W636+zZs8rIyHDaviRZrVZZrdY86zl3ztfrggqc2Wxmr+txdravsrNt8vX1vep/+JcEs9nMcfBi9Nf70WPvRn+9nzf0uDj1E0a9QGBgYKHz/f39C51/+vRpTZgwQXfffXeeeaVKlbqs2gAAAAAgPzxn9P9UrlxZZrNZmzdvdkw7efKk9uzZI0mKj4/XuXPntHHjRsf8EydOaPfu3brhhhvyXWd8fLzTeEnasGFDseravn27MjMznZYPCgpSVFRUkddRp04dJScn6++//853foMGDbR7927FxcXl+fLx4VsEAAAAQMkjafyf0qVLq0+fPnriiSe0evVq7dixQ/3795ePj49MJpOqVq2qLl266MEHH9T333+v7du36z//+Y8qVqyoLl265LvORx99VO+++67mzJmjPXv2aPz48dqxY0ex6srOzlb//v21c+dOffnllxo/fryGDh1arJDYs2dPRUREqGvXrlq3bp1+/fVXffTRR1q/fr0kady4cZo/f74mTJigHTt2KCUlRYsWLdIzzzxTrFoBAAAAoKgIoxd4+eWX1bRpU3Xu3Fnt2rVT8+bNFR8f77hUdc6cOWrYsKE6d+6spk2byjAMffnllwVeF92jRw+NHTtWo0aNUsOGDXXw4EENHjy4WDW1bdtWVatW1c0336wePXrozjvvVGJiYrHWYbFYtHz5cpUvX16dOnVS7dq1NXnyZPn6+kqSOnTooM8//1zLly/XjTfeqJtuuknTp093fCYVAAAAAEoanxm9QOnSpbVgwQLH6zNnzmjChAmOx5yUKVNG8+fPL3D5vn37qm/fvk7TnnrqKT311FNO06ZMmVKsuiZMmJDvo1Qkac2aNflO//dNkqOjo7VkyZICt9GhQwd16NChWHUBAAAAwKUijF5g27Zt2rVrlxo3bqyTJ0/q2WeflaQCL8MFAAAAAFwawui/TJ06Vbt375bFYlHDhg21du1alStXziXbCgoKKnDeV1995ZJtAgAAAMCVgDB6gfr162vLli1u215ycnKB8ypWrKiWLVu6rRYAAAAAcCfCqAfFxcV5ugQAAAAA8AjupgsAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOGxihxGRmHlFOToiny4ALWCx2SVJGRqqys309XE3Jysw87OkSAAAArkmEUZSYrKw3lZnp6SrgChaLWVIfpaWNVna2zdPllLiwMKuCg4M9XQYAAMA1hTCKEvPWWxMUEsKZUW9kt9uVkpKi+fMny9fXu86MSlJwcLDCw8M9XQYAAMA1hTCKEhMdHa2wsDBPlwEXsNlsSklJUWxsrMxms6fLAQAAgBfgBkYAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALfjbrooMQcPHlRaWpqny4AL2O12SVJqaqrXPNqFx7kAAAB4FmEUJWbgwPHKyfF0FXAFi8WsMWP6qHfv0crOtnm6nBIRFmZVUtIsAikAAICHEEZRYqzWQbJa4z1dBlzAYrFLSlFo6GRlZ1/9Z0YzMw/rxIlpSk9PJ4wCAAB4CGEUJcbfP1L+/lU8XQZcwGy2SUpRQECszGazp8spEVlZnq4AAADg2sYNjAAAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4HWH0GmcymQr9SkxM9HSJAAAAALyQn6cLgGcdPXrU8d8ffPCBxo0bp927dzumBQUFeaIsAAAAAF6OMHqNi4iIcPx3SEiITCaT0zQAAAAAcAXCKIotKytLWVlZjtfp6emSJD8/u8xmm6fKggvl9tVb+mux2GWxmGW322Wzecc+Xa7c48Dx8E701/vRY+9Gf72fN/W4OPtgMgzDcGEtuIrMnTtXw4cPV1paWqHjEhMTNWHChDzTk5KSFBAQ4KLqAAAAAFzpMjIylJCQoJMnTyo4OLjQsZwZRbGNGTNGI0aMcLxOT09XVFSU5s2LU0BAPc8VBpcxm21KSFihpKT2stnMni7nsmVkpCotbbTmz5+s2NhYT5dzRbDZbFqxYoXat28vs/nq7zGc0V/vR4+9G/31ft7U49yrJouCMIpis1qtslqteaafO+frFUEFBbPZzF7R4+xsX2Vn2+Tr63vV/8AvaWazmWPixeiv96PH3o3+ej9v6HFx6ufRLgAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigc+vbte9E76QIAAABASSCMAgAAAADcjjAKAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADczs/TBcB7ZGYeUU5OiKfLgAtYLHZJUkZGqrKzfT1czeXLzDzs6RIAAACueYRRlJisrDeVmenpKuAKFotZUh+lpY1WdrbN0+WUiLAwq4KDgz1dBgAAwDWLMIoS89ZbExQSwplRb2S325WSkqL58yfL1/fqPzMqScHBwQoPD/d0GQAAANcswihKTHR0tMLCwjxdBlzAZrMpJSVFsbGxMpvNni4HAAAAXoAbGAEAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I676aLEHDx4UGlpaZ4uAy5gt9slSampqR55tAuPYQEAAPA+hFGUmIEDxysnx9NVwBUsFrPGjOmj3r1HKzvb5vbth4VZlZQ0i0AKAADgRQijKDFW6yBZrfGeLgMuYLHYJaUoNHSysrPde2Y0M/OwTpyYpvT0dMIoAACAFyGMosT4+0fK37+Kp8uAC5jNNkkpCgiIldlsdvv2s7LcvkkAAAC4GDcwAgAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYBQAAAAC4nVeE0QMHDshkMik5OfmKWA8AAAAAoHBeEUYvRd++fdW1a1enaVFRUTp69Khq1arlmaKKKDExUfXq1fN0GQAAAABwya7ZMJofX19fRUREyM/PzyPbz87Oduv2DMPQuXPn3LpNAAAAAJAuIYzm5OToxRdfVFxcnKxWqypVqqSJEydqzZo1MplMSktLc4xNTk6WyWTSgQMHJElz585VaGioPv/8c1WvXl0BAQHq1q2bMjIyNG/ePMXExKhMmTIaNmyY7Ha7Yz0mk0mffvqpUx2hoaGaO3duvjXa7Xb1799fsbGx8vf3V/Xq1fXKK6845icmJmrevHn67LPPZDKZZDKZtGbNGqfLdHNycnT99ddr1qxZTuvetm2bfHx8dPDgQUlSWlqaBgwYoPDwcAUHB+uWW27R9u3bi3Qsc89wvvPOO4qNjVWpUqUuus65c+dqwoQJ2r59u6P2uXPn5nuJcVpammPfJDl69NVXX6lhw4ayWq36/vvv1bp1aw0bNkyjRo1S2bJlFRERocTExCLtAwAAAABcimKfAhwzZozefvttTZ8+XS1atNDRo0e1a9euIi+fkZGhmTNnatGiRTp16pTuvvtu3XXXXQoNDdWXX36pX3/9Vffcc4+aN2+uHj16FLc8SXIEycWLFyssLEw//PCDBg4cqAoVKqh79+4aOXKkUlJSlJ6erjlz5kiSypYtqyNHjjjW4ePjo549eyopKUmDBw92TF+wYIGaN2+u6OhoSdK9994rf39/ffXVVwoJCdGbb76ptm3bas+ePSpbtuxFa923b58++ugjffzxx/L19b3oOnv06KFffvlFy5Yt08qVKyVJISEh+vPPP4t8fEaPHq2pU6eqcuXKKlOmjCRp3rx5GjFihDZu3Kj169erb9++at68udq3b59n+aysLGVlZTlep6enS5L8/Owym21FrgNXj9y+eqK/FotdFotZdrtdNhvfX66Se2w5xt6J/no/euzd6K/386YeF2cfihVGT506pVdeeUWvvfaa+vTpI0mqUqWKWrRo4Tj7VpTiZs2apSpVqkiSunXrpvfee09//vmngoKCdMMNN6hNmzZavXr1JYdRs9msCRMmOF7HxsZq/fr1+vDDD9W9e3cFBQXJ399fWVlZioiIKHA9vXr10rRp03To0CFVqlRJOTk5WrRokZ555hlJ0vfff69Nmzbp2LFjslqtkqSpU6fq008/1ZIlSzRw4MCL1pqdna358+crPDy8yOsMCgqSn59fobUX5tlnn80TMuvUqaPx48dLkqpWrarXXntNq1atyjeMTpo0yen45urTZ58CAo7kmQ7vkZCwwkNb7qOUlBSlpKR4aPvXjhUrPNVjuAP99X702LvRX+/nDT3OyMgo8thihdGUlBRlZWWpbdu2xS4qV0BAgCOIStJ1112nmJgYBQUFOU07duzYJW9Dkl5//XW9++67OnTokDIzM5WdnV3sm/7Uq1dP8fHxSkpK0ujRo/Xtt9/q2LFjuvfeeyVJ27dv1+nTpxUWFua0XGZmpvbv31+kbURHRzuCaEmt82IaNWqUZ1qdOnWcXleoUKHAHowZM0YjRoxwvE5PT1dUVJTmzYtTQEC9EqkRVxaz2aaEhBVKSmovm83s1m1nZKQqLW205s+frNjYWLdu+1pis9m0YsUKtW/fXmaze3sM16O/3o8eezf66/28qce5V00WRbHCqL+/f4HzfHzOf/zUMAzHtPxO0f774JpMpnyn5eTkOL2+cL0FrTvXokWLNHLkSE2bNk1NmzZV6dKl9dJLL2njxo0FLlOQXr16OcJoUlKSOnbs6AiKp0+fVoUKFfI9KxwaGlqk9QcGBjq9vtR1FvX457dNKf++XNiDC1mtVsdZ2wudO+fr9qAC97LZzG7vcXa2r7KzbfL19b3qfzhfDcxmM8fZi9Ff70ePvRv99X7e0OPi1F+sMFq1alX5+/tr1apVGjBggNO83LN7R48edXwOsaSe1xkeHq6jR486Xu/du7fQ07/r1q1Ts2bNNGTIEMe0f59VtFgsTjdJKkhCQoKeeeYZbdmyRUuWLNHs2bMd8xo0aKA//vhDfn5+iomJKcYeFawo68yv9guPf/369SWV3PEHAAAAgJJWrLvplipVSk8++aRGjRql+fPna//+/dqwYYP++9//Ki4uTlFRUUpMTNTevXv1xRdfaNq0aSVS5C233KLXXntN27Zt048//qiHHnqo0MRdtWpV/fjjj/r666+1Z88ejR07Vps3b3YaExMTo59++km7d+/WX3/9VeBZxJiYGDVr1kz9+/eX3W7XnXfe6ZjXrl07NW3aVF27dtXy5ct14MAB/fDDD3r66af1448/XtK+FmWdMTExSk1NVXJysv766y9lZWXJ399fN910kyZPnqyUlBR9++23js+2AgAAAMCVptiPdhk7dqwef/xxjRs3TvHx8erRo4eOHTsms9mshQsXateuXapTp46mTJmi559/vkSKnDZtmqKiotSyZUslJCRo5MiRCggIKHD8oEGDdPfdd6tHjx5q0qSJTpw44XSWVJIefPBBVa9eXY0aNVJ4eLjWrVtX4Pp69eql7du366677nK6VNlkMunLL7/UzTffrH79+qlatWq67777dPDgQV133XWXtK9FWec999yjjh07qk2bNgoPD9fChQslSe+++67OnTunhg0bavjw4SV2/AEAAACgpJmMf38YEyim9PR0hYSEqG3bLfL3b+DpcuACZrNNffp8qXnzOrn9M6NnzuxXWtpwLV48w+nmZyhZNptNX375pTp16nTVf1YFedFf70ePvRv99X7e1OPcbHDy5EkFBwcXOrbYZ0YBAAAAALhchFEXqlmzpoKCgvL9WrBggafLAwAAAACPKdbddFE8X375ZYE3RrrUz5QCAAAAgDcgjLpQdHS0p0sAAAAAgCsSl+kCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOu+mixGRmHlFOToiny4ALWCx2SVJGRqqys33duu3MzMNu3R4AAADcgzCKEpOV9aYyMz1dBVzBYjFL6qO0tNHKzs7/2bmuFBZmVXBwsNu3CwAAANchjKLEvPXWBIWEcGbUG9ntdqWkpGj+/Mny9XXvmVFJCg4OVnh4uNu3CwAAANchjKLEREdHKywszNNlwAVsNptSUlIUGxsrs9ns6XIAAADgBbiBEQAAAADA7QijAAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7bibLkrMwYMHlZaW5uky4AJ2u12SlJqaetmPduExLQAAAJAIoyhBAweOV06Op6uAK1gsZo0Z00e9e49WdrbtstYVFmZVUtIsAikAAMA1jjCKEmO1DpLVGu/pMuACFotdUopCQycrO/vSz4xmZh7WiRPTlJ6eThgFAAC4xhFGUWL8/SPl71/F02XABcxmm6QUBQTEymw2X9a6srJKpiYAAABc3biBEQAAAADA7QijAAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7Qij16ADBw7IZDIpOTnZ06UAAAAAuEYRRi/wxx9/6JFHHlHlypVltVoVFRWlO+64Q6tWrZIkxcTEaMaMGQUuf/jwYT3wwAOKjIyUxWJRdHS0Hn30UZ04ccJpXGpqqhISEhQZGalSpUrp+uuvV5cuXbRr1y7HGJPJlO/XokWLLns/o6KidPToUdWqVeuy1wUAAAAAl8LP0wVcKQ4cOKDmzZsrNDRUL730kmrXri2bzaavv/5aDz/8sFNQzM+vv/6qpk2bqlq1alq4cKFiY2O1Y8cOPfHEE/rqq6+0YcMGlS1bVjabTe3bt1f16tX18ccfq0KFCvrtt9/01VdfKS0tzWmdc+bMUceOHZ2mhYaGXva++vr6KiIi4rLXAwAAAACXijD6f4YMGSKTyaRNmzYpMDDQMb1mzZp64IEHLrr8ww8/LIvFouXLl8vf31+SVKlSJdWvX19VqlTR008/rVmzZmnHjh3av3+/Vq1apejoaElSdHS0mjdvnmedoaGhxQ6N6enpuu666/Txxx/rtttuc0z/5JNP1Lt3b/355586duyYYmNjtW3bNtWrV0/PPvusZs+erZ9//llhYWGSpNtvv10ZGRlatWqVfHw4gQ4AAACgZBFGJf39999atmyZJk6c6BREc13sbOTff/+tr7/+WhMnTnQE0VwRERHq1auXPvjgA73xxhsKDw+Xj4+PlixZouHDh8vX17ckd0XBwcHq3LmzkpKSnMLoggUL1LVrVwUEBORZ5umnn9ayZcs0YMAAffLJJ3r99df1ww8/aPv27fkG0aysLGVlZTlep6enS5L8/Owym20luj+4MuT29XL7a7HYZbGYZbfbZbPxvXIlye0HffFO9Nf70WPvRn+9nzf1uDj7QBiVtG/fPhmGoRo1alzS8nv37pVhGIqPj893fnx8vP755x8dP35cFStW1MyZMzVq1ChNmDBBjRo1Ups2bdSrVy9VrlzZabmePXvmCas7d+5UpUqVCq2nV69euv/++5WRkaGAgAClp6friy++0CeffJLveF9fX73//vuqV6+eRo8erZkzZ+qdd94pcDuTJk3ShAkT8kzv02efAgKOFFobrm4JCStKYC19lJKSopSUlBJYF0raihUl0WNcqeiv96PH3o3+ej9v6HFGRkaRxxJGJRmG4db1PPzww+rdu7fWrFmjDRs2aPHixXrhhRe0dOlStW/f3jFu+vTpateundOykZGRF11/p06dZDabtXTpUt1333366KOPFBwcnGddF6pcubKmTp2qQYMGqUePHkpISChw7JgxYzRixAjH6/T0dEVFRWnevDgFBNS7aH24+pjNNiUkrFBSUnvZbOZLXk9GRqrS0kZr/vzJio2NLcEKcblsNptWrFih9u3by2y+9B7jykR/vR899m701/t5U49zr5osCsKopKpVq8pkMl30JkUFiYuLk8lkUkpKiu66664881NSUlSmTBmFh4c7ppUuXVp33HGH7rjjDj3//PPq0KGDnn/+eacwGhERobi4uGLXY7FY1K1bNyUlJem+++5TUlKSevToIT+/wtv93XffydfXVwcOHNC5c+cKHG+1WmW1WvNMP3fO97KCCq58Npv5snqcne2r7GybfH19r/oftN7KbDbTGy9Gf70fPfZu9Nf7eUOPi1M/d6aRVLZsWXXo0EGvv/66zpw5k2f+v+9y+29hYWFq37693njjDWVmZjrN++OPP7RgwQL16NFDJpMp3+VNJpNq1KiR77YvVa9evbRs2TLt2LFD33zzjXr16lXo+A8++EAff/yx1qxZo0OHDum5554rsVoAAAAA4N8Io//n9ddfl91uV+PGjfXRRx9p7969SklJ0cyZM9W0aVPHuN9//13JyclOX//8849ee+01ZWVlqUOHDvruu+90+PBhLVu2TO3bt1fFihU1ceJESVJycrK6dOmiJUuWaOfOndq3b5/++9//6t1331WXLl2cakpLS9Mff/zh9FXUwHrzzTc7bp4UGxurJk2aFDj2t99+0+DBgzVlyhS1aNFCc+bM0QsvvKANGzZcwpEEAAAAgIsjjP6fypUra+vWrWrTpo0ef/xx1apVS+3bt9eqVas0a9Ysx7ipU6eqfv36Tl9ffPGFqlatqh9//FGVK1dW9+7dVaVKFQ0cOFBt2rTR+vXrVbZsWUnS9ddfr5iYGE2YMEFNmjRRgwYN9Morr2jChAl6+umnnWrq16+fKlSo4PT16quvFml/TCaTevbsqe3btxd6VtQwDPXt21eNGzfW0KFDJUkdOnTQ4MGD9Z///EenT58u7qEEAAAAgIviM6MXqFChgl577TW99tpr+c4/cOBAoctHR0dr7ty5hY4pV66cXnnllYvWUhI3VZoyZYqmTJmSZ3pMTIzT+leuXJlnzMyZMzVz5szLrgEAAAAA8sOZUQAAAACA2xFGr0K33XabgoKC8v164YUXPF0eAAAAAFwUl+lehd555508d+3NlfvZVAAAAAC4khFGr0IVK1b0dAkAAAAAcFm4TBcAAAAA4HaEUQAAAACA2xFGAQAAAABuRxgFAAAAALgdYRQAAAAA4HbcTRclJjPziHJyQjxdBlzAYrFLkjIyUpWd7XvJ68nMPFxSJQEAAOAqRxhFicnKelMFPP4UVzmLxSypj9LSRis723ZZ6woLsyo4OLhkCgMAAMBVizCKEvPWWxMUEsKZUW9kt9uVkpKi+fMny9f30s+MSlJwcLDCw8NLqDIAAABcrQijKDHR0dEKCwvzdBlwAZvNppSUFMXGxspsNnu6HAAAAHgBbmAEAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA77qaLEnPw4EGlpaV5ugy4gN1ulySlpqYW6dEuPL4FAAAAF0MYRYkZOHC8cnI8XQVcwWIxa8yYPurde7Sys20XHR8WZlVS0iwCKQAAAApEGEWJsVoHyWqN93QZcAGLxS4pRaGhk5WdXfiZ0czMwzpxYprS09MJowAAACgQYRQlxt8/Uv7+VTxdBlzAbLZJSlFAQKzMZvNFx2dlub4mAAAAXN24gREAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigAAAAAwO0IowAAAAAAtyOMAgAAAADcjjAKAAAAAHA7wigAAAAAwO2u+TDaunVrDR8+vMD5JpNJn376aYHz16xZI5PJpLS0tBKvzR0OHDggk8mk5ORkT5cCAAAA4Bri5+kCrnRHjx5VmTJlPF2GpPPBuV69epoxY4anSwEAAACAy0IYvYiIiAhPlwAAAAAAXueav0xXknJycjRq1CiVLVtWERERSkxMdMy72GW6+fnoo49Us2ZNWa1WxcTEaNq0aUVe9o033lDVqlVVqlQpXXfdderWrZskqW/fvvr222/1yiuvyGQyyWQy6cCBA5Kkb7/9Vo0bN5bValWFChU0evRonTt3zmn/XnzxRcXFxclqtapSpUqaOHFivtu32+164IEHVKNGDR06dKhY+w0AAAAARcWZUUnz5s3TiBEjtHHjRq1fv159+/ZV8+bN1b59+2Kva8uWLerevbsSExPVo0cP/fDDDxoyZIjCwsLUt2/fQpf98ccfNWzYML333ntq1qyZ/v77b61du1aS9Morr2jPnj2qVauWnn32WUlSeHi4fv/9d3Xq1El9+/bV/PnztWvXLj344IMqVaqUI1SPGTNGb7/9tqZPn64WLVro6NGj2rVrV57tZ2VlqWfPnjpw4IDWrl2r8PDwfOvMyspSVlaW43V6erokyc/PLrPZVtxDhqtAbl+L0l+LxS6LxSy73S6bje+Hq0Vur+iZd6K/3o8eezf66/28qcfF2QeTYRiGC2u54rVu3Vp2u90R+iSpcePGuuWWWzR58mSZTCZ98skn6tq1a77Lr1mzRm3atNE///yj0NBQ9erVS8ePH9fy5csdY0aNGqUvvvhCO3bsKLSWjz/+WP369dNvv/2m0qVL51vrvz8z+vTTT+ujjz5SSkqKTCaTpPNnV5988kmdPHlSZ86cUXh4uF577TUNGDAgzzoPHDig2NhYrV27VomJicrKytLnn3+ukJCQAutMTEzUhAkT8kxPSkpSQEBAofsIAAAAwHtlZGQoISFBJ0+eVHBwcKFjOTMqqU6dOk6vK1SooGPHjuUZd9tttzlCa3R0dL7hMiUlRV26dHGa1rx5c82YMUN2u12+vr4F1tG+fXtFR0ercuXK6tixozp27Ki77rqr0ICXkpKipk2bOoJo7vZOnz6t3377TX/88YeysrLUtm3bAtchST179tT111+vb775Rv7+/oWOHTNmjEaMGOF4nZ6erqioKM2bF6eAgHqFLourk9lsU0LCCiUltZfNZi50bEZGqtLSRmv+/MmKjY11U4W4XDabTStWrFD79u1lNhfeY1x96K/3o8fejf56P2/qce5Vk0VBGJXyNNxkMiknJyfPuHfeeUeZmZn5LlMSSpcura1bt2rNmjVavny5xo0bp8TERG3evFmhoaGXtM6LBctcnTp10vvvv6/169frlltuKXSs1WqV1WrNM/3cOd+LBhVc3Ww280V7nJ3tq+xsm3x9fa/6H6bXIrPZTN+8GP31fvTYu9Ff7+cNPS5O/dzAqBgqVqyouLg4xcXFKTo6Ot8x8fHxWrdundO0devWqVq1aoWeFc3l5+endu3a6cUXX9RPP/2kAwcO6JtvvpEkWSwW2e32PNtbv369Lrzaet26dSpdurSuv/56Va1aVf7+/lq1alWh2x08eLAmT56sO++8U99+++1F6wQAAACAy8GZ0RL2+OOP68Ybb9Rzzz2nHj16aP369Xrttdf0xhtvXHTZzz//XL/++qtuvvlmlSlTRl9++aVycnJUvXp1SVJMTIw2btyoAwcOKCgoSGXLltWQIUM0Y8YMPfLIIxo6dKh2796t8ePHa8SIEfLx8VGpUqX05JNPatSoUbJYLGrevLmOHz+uHTt2qH///k7bf+SRR2S329W5c2d99dVXatGihUuOEQAAAAAQRktYgwYN9OGHH2rcuHF67rnnVKFCBT377LMXvZOuJIWGhurjjz9WYmKizp49q6pVq2rhwoWqWbOmJGnkyJHq06ePbrjhBmVmZio1NVUxMTH68ssv9cQTT6hu3boqW7as+vfvr2eeecax3rFjx8rPz0/jxo3TkSNHVKFCBT300EP51jB8+HDl5OSoU6dOWrZsmZo1a1YixwUAAAAALnTNh9E1a9bkmXbhc0UvdrPh1q1b5xlzzz336J577il2LS1atMi3nlzVqlXT+vXr80xv1aqVNm3aVOByPj4+evrpp/X000/nmRcTE5On/hEjRjjdoAgAAAAAShqfGQUAAAAAuB1h1I3Wrl2roKCgAr8AAAAA4FpxzV+m606NGjVScnKyp8sAAAAAAI8jjLqRv7+/4uLiPF0GAAAAAHgcl+kCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC3I4wCAAAAANyOu+mixGRmHlFOToiny4ALWCx2SVJGRqqys30LHZuZedgdJQEAAOAqRxhFicnKelOZmZ6uAq5gsZgl9VFa2mhlZ9suOj4szKrg4GDXFwYAAICrFmEUJeattyYoJIQzo97IbrcrJSVF8+dPlq9v4WdGJSk4OFjh4eFuqAwAAABXK8IoSkx0dLTCwsI8XQZcwGazKSUlRbGxsTKbzZ4uBwAAAF6AGxgBAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC34wZGKDEHDx5UWlqap8tACQsODlZoaKinywAAAICXIYyixAwcOF45OZ6uAiUtLMyq+fNf9XQZAAAA8DKEUZQYq3WQrNZ4T5eBEpSZeVgnTkzTqVOnPF0KAAAAvAxhFCXG3z9S/v5VPF0GSlhWlqcrAAAAgDfiBkYAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wijz69u2rrl27eroMAAAAAF7smgyjffv2lclkkslkksViUVxcnJ599lmdO3dOa9asccwzmUwKDw9Xp06d9PPPPzutIzs7Wy+++KLq1q2rgIAAlStXTs2bN9ecOXNks9lcvg+JiYmqV6+ey7cDAAAAAK7g5+kCPKVjx46aM2eOsrKy9OWXX+rhhx+W2WxW06ZNJUm7d+9WcHCwjhw5oieeeEK333679u3bJ4vFouzsbHXo0EHbt2/Xc889p+bNmys4OFgbNmzQ1KlTVb9+fYIiAAAAABTimjwzKklWq1URERGKjo7W4MGD1a5dOy1dutQxv3z58oqIiFCDBg00fPhwHT58WLt27ZIkzZgxQ999951WrVqlhx9+WPXq1VPlypWVkJCgjRs3qmrVqhfd/rJly9SiRQuFhoYqLCxMnTt31v79+53G/Pbbb+rZs6fKli2rwMBANWrUSBs3btTcuXM1YcIEbd++3XEGd+7cuTpw4IBMJpOSk5Md60hLS5PJZNKaNWskSXa7Xf3791dsbKz8/f1VvXp1vfLKK5d/QAEAAACgGK7ZM6P/5u/vrxMnTuSZfvLkSS1atEiSZLFYJEkLFixQu3btVL9+/TzjzWazzGbzRbd35swZjRgxQnXq1NHp06c1btw43XXXXUpOTpaPj49Onz6tVq1aqWLFilq6dKkiIiK0detW5eTkqEePHvrll1+0bNkyrVy5UpIUEhKiP//886LbzcnJ0fXXX6/FixcrLCxMP/zwgwYOHKgKFSqoe/fuF11ekrKyspSVleV4nZ6eLkny87PLbHb9JcpwH4vFLovFLLvdLkluuQQdnpHbW3rsneiv96PH3o3+ej9v6nFx9uGaD6OGYWjVqlX6+uuv9cgjjzimX3/99ZLOh0ZJuvPOO1WjRg1J0t69e9W6devL2u4999zj9Prdd99VeHi4du7cqVq1aikpKUnHjx/X5s2bVbZsWUlSXFycY3xQUJD8/PwUERFRrO2azWZNmDDB8To2Nlbr16/Xhx9+WOQwOmnSJKd15OrTZ58CAo4Uqx5cDfpoz549kqQVK1Z4uBa4Gj32bvTX+9Fj70Z/vZ839DgjI6PIY6/ZMPr5558rKChINptNOTk5SkhIUGJiojZv3ixJWrt2rQICArRhwwa98MILmj17tmNZwzAue/t79+7VuHHjtHHjRv3111/KycmRJB06dEi1atVScnKy6tev7wiiJen111/Xu+++q0OHDikzM1PZ2dnF+ozrmDFjNGLECMfr9PR0RUVFad68OAUEFH09uPJlZKQqLW205syZqD179qh9+/ZFOvOPq4/NZtOKFSvosZeiv96PHns3+uv9vKnHuVdNFsU1G0bbtGmjWbNmyWKxKDIyUn5+zociNjZWoaGhql69uo4dO6YePXrou+++kyRVq1bN8fnRS3XHHXcoOjpab7/9tiIjI5WTk6NatWopOztb0vnLhovLx+f8R4AvDMv/Pk2+aNEijRw5UtOmTVPTpk1VunRpvfTSS9q4cWORt2O1WmW1WvNMP3fOVzbb1f3mgbPsbF9lZ9vk6+srqeiXoePqRY+9G/31fvTYu9Ff7+cNPS5O/dfsDYwCAwMVFxenSpUq5Qmi//bwww/rl19+0SeffCJJSkhI0MqVK7Vt27Y8Y202m+PS3oKcOHFCu3fv1jPPPKO2bdsqPj5e//zzj9OYOnXqKDk5WX///Xe+67BYLI7P8eUKDw+XJB09etQx7cKbGUnSunXr1KxZMw0ZMkT169dXXFxcnhsnAQAAAICrXbNhtDgCAgL04IMPavz48TIMQ8OHD1fz5s3Vtm1bvf7669q+fbt+/fVXffjhh7rpppu0d+/eQtdXpkwZhYWF6a233tK+ffv0zTffOF32Kkk9e/ZURESEunbtqnXr1unXX3/VRx99pPXr10uSYmJilJqaquTkZP3111/KysqSv7+/brrpJk2ePFkpKSn69ttv9cwzzzitt2rVqvrxxx/19ddfa8+ePRo7dqzj0mQAAAAAcBfCaBENHTpUKSkpWrx4saxWq1asWKFRo0bpzTff1E033aQbb7xRM2fO1LBhw1SrVq1C1+Xj46NFixZpy5YtqlWrlh577DG99NJLTmMsFouWL1+u8uXLq1OnTqpdu7YmT57suFzynnvuUceOHdWmTRuFh4dr4cKFks7fCOncuXNq2LChhg8frueff95pvYMGDdLdd9+tHj16qEmTJjpx4oSGDBlSgkcKAAAAAC7OZJTE3XhwTUtPT1dISIjatt0if/8Gni4HJejMmf1KSxuuRYumKSUlRZ06dbrqP8eA/NlsNn355Zf02EvRX+9Hj70b/fV+3tTj3Gxw8uRJBQcHFzqWM6MAAAAAALcjjLrAoUOHFBQUVODXoUOHPF0iAAAAAHjUNftoF1eKjIzMcxfbf88HAAAAgGsZYdQF/Pz8FBcX5+kyAAAAAOCKxWW6AAAAAAC3I4wCAAAAANyOMAoAAAAAcDvCKAAAAADA7QijAAAAAAC34266KDGZmUeUkxPi6TJQgjIzD3u6BAAAAHgpwihKTFbWm8rM9HQVKGlhYVaVLl3a02UAAADAyxBGUWLeemuCQkI4M+ptgoODFRoa6ukyAAAA4GUIoygx0dHRCgsL83QZcAGbzebpEgAAAOBluIERAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDt/DxdAK5+hmFIkk6dOiWz2ezhauAKNptNGRkZSk9Pp8deih57N/rr/eixd6O/3s+bepyeni7p/2eEwhBGcdlOnDghSYqNjfVwJQAAAACuBKdOnVJISEihYwijuGxly5aVJB06dOii33C4OqWnpysqKkqHDx9WcHCwp8uBC9Bj70Z/vR899m701/t5U48Nw9CpU6cUGRl50bGEUVw2H5/zHz0OCQm56t88KFxwcDA99nL02LvRX+9Hj70b/fV+3tLjop6g4gZGAAAAAAC3I4wCAAAAANyOMIrLZrVaNX78eFmtVk+XAhehx96PHns3+uv96LF3o7/e71rtsckoyj13AQAAAAAoQZwZBQAAAAC4HWEUAAAAAOB2hFEAAAAAgNsRRgEAAAAAbkcYRb5ef/11xcTEqFSpUmrSpIk2bdpU6PjFixerRo0aKlWqlGrXrq0vv/zSab5hGBo3bpwqVKggf39/tWvXTnv37nXlLuAiSrrHffv2lclkcvrq2LGjK3cBhShOf3fs2KF77rlHMTExMplMmjFjxmWvE65X0j1OTEzM8x6uUaOGC/cAhSlOf99++221bNlSZcqUUZkyZdSuXbs84/k9fOUp6R7ze/jKU5wef/zxx2rUqJFCQ0MVGBioevXq6b333nMa45XvYwP4l0WLFhkWi8V49913jR07dhgPPvigERoaavz555/5jl+3bp3h6+trvPjii8bOnTuNZ555xjCbzcbPP//sGDN58mQjJCTE+PTTT43t27cbd955pxEbG2tkZma6a7dwAVf0uE+fPkbHjh2No0ePOr7+/vtvd+0SLlDc/m7atMkYOXKksXDhQiMiIsKYPn36Za8TruWKHo8fP96oWbOm03v4+PHjLt4T5Ke4/U1ISDBef/11Y9u2bUZKSorRt29fIyQkxPjtt98cY/g9fGVxRY/5PXxlKW6PV69ebXz88cfGzp07jX379hkzZswwfH19jWXLljnGeOP7mDCKPBo3bmw8/PDDjtd2u92IjIw0Jk2alO/47t27G7fffrvTtCZNmhiDBg0yDMMwcnJyjIiICOOll15yzE9LSzOsVquxcOFCF+wBLqake2wY538JdunSxSX1oniK298LRUdH5xtULmedKHmu6PH48eONunXrlmCVuFSX+347d+6cUbp0aWPevHmGYfB7+EpU0j02DH4PX2lK4vdm/fr1jWeeecYwDO99H3OZLpxkZ2dry5YtateunWOaj4+P2rVrp/Xr1+e7zPr1653GS1KHDh0c41NTU/XHH384jQkJCVGTJk0KXCdcxxU9zrVmzRqVL19e1atX1+DBg3XixImS3wEU6lL664l14tK5sh979+5VZGSkKleurF69eunQoUOXWy6KqST6m5GRIZvNprJly0ri9/CVxhU9zsXv4SvD5fbYMAytWrVKu3fv1s033yzJe9/HhFE4+euvv2S323Xdddc5Tb/uuuv0xx9/5LvMH3/8Uej43P8vzjrhOq7osSR17NhR8+fP16pVqzRlyhR9++23uu2222S320t+J1CgS+mvJ9aJS+eqfjRp0kRz587VsmXLNGvWLKWmpqply5Y6derU5ZaMYiiJ/j755JOKjIx0/NHK7+Eriyt6LPF7+EpyqT0+efKkgoKCZLFYdPvtt+vVV19V+/btJXnv+9jP0wUA8A733Xef479r166tOnXqqEqVKlqzZo3atm3rwcoAFMVtt93m+O86deqoSZMmio6O1ocffqj+/ft7sDIUx+TJk7Vo0SKtWbNGpUqV8nQ5cIGCeszv4atf6dKllZycrNOnT2vVqlUaMWKEKleurNatW3u6NJfhzCiclCtXTr6+vvrzzz+dpv/555+KiIjId5mIiIhCx+f+f3HWCddxRY/zU7lyZZUrV0779u27/KJRZJfSX0+sE5fOXf0IDQ1VtWrVeA+72eX0d+rUqZo8ebKWL1+uOnXqOKbze/jK4ooe54ffw55zqT328fFRXFyc6tWrp8cff1zdunXTpEmTJHnv+5gwCicWi0UNGzbUqlWrHNNycnK0atUqNW3aNN9lmjZt6jReklasWOEYHxsbq4iICKcx6enp2rhxY4HrhOu4osf5+e2333TixAlVqFChZApHkVxKfz2xTlw6d/Xj9OnT2r9/P+9hN7vU/r744ot67rnntGzZMjVq1MhpHr+Hryyu6HF++D3sOSX1czonJ0dZWVmSvPh97Ok7KOHKs2jRIsNqtRpz5841du7caQwcONAIDQ01/vjjD8MwDOP+++83Ro8e7Ri/bt06w8/Pz5g6daqRkpJijB8/Pt9Hu4SGhhqfffaZ8dNPPxldunS56m9FfTUr6R6fOnXKGDlypLF+/XojNTXVWLlypdGgQQOjatWqxtmzZz2yj9ey4vY3KyvL2LZtm7Ft2zajQoUKxsiRI41t27YZe/fuLfI64V6u6PHjjz9urFmzxkhNTTXWrVtntGvXzihXrpxx7Ngxt+/fta64/Z08ebJhsViMJUuWOD3W49SpU05j+D185SjpHvN7+MpT3B6/8MILxvLly439+/cbO3fuNKZOnWr4+fkZb7/9tmOMN76PCaPI16uvvmpUqlTJsFgsRuPGjY0NGzY45rVq1cro06eP0/gPP/zQqFatmmGxWIyaNWsaX3zxhdP8nJwcY+zYscZ1111nWK1Wo23btsbu3bvdsSsoQEn2OCMjw7j11luN8PBww2w2G9HR0caDDz5IUPGg4vQ3NTXVkJTnq1WrVkVeJ9yvpHvco0cPo0KFCobFYjEqVqxo9OjRw9i3b58b9wgXKk5/o6Oj8+3v+PHjHWP4PXzlKcke83v4ylScHj/99NNGXFycUapUKaNMmTJG06ZNjUWLFjmtzxvfxybDMAz3nosFAAAAAFzr+MwoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAAAAAMDtCKMAAAAAALcjjAIAAAAA3I4wCgAAAABwO8IoAACQJJ04cULly5fXgQMHPF3KZWvdurWGDx/ueB0TE6MZM2a4bHsHDhyQyWRScnKyy7ZRHKNHj9Yjjzzi6TIAoFCEUQDAValv377q2rWrp8so0JUWTopi4sSJ6tKli2JiYiT9/30oX768Tp065TS2Xr16SkxMdH+Rl2jz5s0aOHCgR2soqUB89OhRJSQkqFq1avLx8XEK3blGjhypefPm6ddff73s7QGAqxBGAQAoYdnZ2Z4uodgyMjL03//+V/37988z79SpU5o6dWqJbs9utysnJ6dE11mY8PBwBQQEuG17rpSVlaXw8HA988wzqlu3br5jypUrpw4dOmjWrFlurg4Aio4wCgDwCq1bt9Yjjzyi4cOHq0yZMrruuuv09ttv68yZM+rXr59Kly6tuLg4ffXVV45l1qxZI5PJpC+++EJ16tRRqVKldNNNN+mXX35xWvdHH32kmjVrymq1KiYmRtOmTXOaHxMTo+eee069e/dWcHCwBg4cqNjYWElS/fr1ZTKZ1Lp1a0nnz9C1b99e5cqVU0hIiFq1aqWtW7c6rc9kMumdd97RXXfdpYCAAFWtWlVLly51GrNjxw517txZwcHBKl26tFq2bKn9+/c75r/zzjuKj49XqVKlVKNGDb3xxhuFHr8vv/xSVqtVN910U555jzzyiF5++WUdO3aswOX/+ecf9e7dW2XKlFFAQIBuu+027d271zF/7ty5Cg0N1dKlS3XDDTfIarXq0KFDiomJ0fPPP6/evXsrKChI0dHRWrp0qY4fP64uXbooKChIderU0Y8//uhY14kTJ9SzZ09VrFhRAQEBql27thYuXFjo/l14VnLu3LkymUx5vi4803ux47dp0ybVr19fpUqVUqNGjbRt27ZCt9+6dWsdPHhQjz32mGN7uS72/ZXfvrzyyivq3bu3QkJCChx3xx13aNGiRYWuCwA8iTAKAPAa8+bNU7ly5bRp0yY98sgjGjx4sO699141a9ZMW7du1a233qr7779fGRkZTss98cQTmjZtmjZv3qzw8HDdcccdstlskqQtW7aoe/fuuu+++/Tzzz8rMTFRY8eO1dy5c53WMXXqVNWtW1fbtm3T2LFjtWnTJknSypUrdfToUX388ceSzp9l7NOnj77//ntt2LBBVatWVadOnfJcBjthwgR1795dP/30kzp16qRevXrp77//liT9/vvvuvnmm2W1WvXNN99oy5YteuCBB3Tu3DlJ0oIFCzRu3DhNnDhRKSkpeuGFFzR27FjNmzevwGO3du1aNWzYMN95PXv2VFxcnJ599tkCl+/bt69+/PFHLV26VOvXr5dhGOrUqZPjOErnz75OmTJF77zzjnbs2KHy5ctLkqZPn67mzZtr27Ztuv3223X//ferd+/e+s9//qOtW7eqSpUq6t27twzDkCSdPXtWDRs21BdffKFffvlFAwcO1P333+845hfTo0cPHT161PH1/9q725Amuz8O4F/vpjVmFs6y7MFCNGbNmIaxVFYvQkhp0ZvCBxRr9CayMKFIK2aRFVRKWGaxxCjzRW8kNUKzh6WC0kpzOO2BSlRSxJgo5Tj3i/67uJdP0/63dtf3A4NdZ9d1nq7D4LdzrrM7d+5AJpMhKirKrf6z2+2Ij49HaGgompqacPLkSRw+fHjCMu/du4fly5fDaDRK5QLuj6/piIyMxKdPn36LZ4CJ6DcliIiI/oNSUlKEXq+XjnU6nYiOjpaOR0ZGhEKhEMnJyVJaV1eXACDq6uqEEEI8evRIABClpaXSOX19fUIul4u7d+8KIYRISEgQW7dudSk7MzNThIaGSseBgYFix44dLue8e/dOABAvXryYsB0Oh0PMnz9flJeXS2kARFZWlnRst9sFAFFZWSmEEOLo0aNi9erV4uvXr2PmGRQUJG7fvu2SlpOTI7Ra7bj10Ov1Ii0tbdw2VFVVCU9PT9HR0SGEEGL9+vXixIkTQgghbDabACDMZrN0bW9vr5DL5aKsrEwIIYTJZBIAhMVicSkjMDBQJCUlScfOe5SdnS2l1dXVCQCiq6tr3PrHxcWJjIwM6Vin04n09HSXci5evDjquo6ODuHr6yvOnTsnpU3Wf4WFhUKpVIqhoSHp8ytXrkx6v8eqgzvjayI/tvOfBgYGBABRW1vrVl5ERDONM6NERPTbCAsLk97PmTMHSqUSarVaSvP39weAUctNtVqt9N7X1xdr1qyB1WoFAFitVmnGzCkqKgrt7e1wOBxS2oYNG9yqY09PDwwGA4KDg7FgwQL4+PjAbrfjw4cP47ZFoVDAx8dHqrfFYkFMTAw8PT1H5T84OIg3b95gz5498Pb2ll6nTp1yWcb7o6GhIcybN2/cz2NjYxEdHY3s7OxRn1mtVshkMmzcuFFKUyqVLv0IAF5eXi7tGqutzns00X1zOBzIycmBWq2Gr68vvL298eDBg1F9OJmBgQHEx8cjLi4OmZmZANzrP6vVKi3rdvrnGJoKd8fXdMjlcgAYtRKAiOhXIZvtChAREf2//BiceXh4uKQ5n9P7NzbOUSgUbp2XkpKCvr4+5OXlITAwEHPnzoVWqx216dFYbXHW2xlkjMVutwMAioqKXIJD4HuAPh4/Pz/09/dPWPfc3FxotVopcJsquVzu8qyk01j3aKL7dv78eeTl5eHSpUtQq9VQKBQ4ePDglDaOcjgc2LVrF3x8fHDt2jUpfbr99ytyLutetGjRLNeEiGhsDEaJiOiPV19fj5UrVwL4vhGPzWaDSqUCAKhUKpjNZpfzzWYzQkJCJgxOvLy8AGDU7JbZbEZBQQG2bdsGAPj48SN6e3unVN+wsDAUFxfj27dvo4JWf39/BAQE4O3bt0hMTHQ7T41Gg1u3bk14TmRkJHbu3IkjR464pKtUKoyMjKChoQGbNm0C8H2Toba2NoSGhrpdB3eZzWbo9XokJSUB+B6k2my2KZV16NAhNDc3o7Gx0WWG053+U6lUKCkpwfDwsHRtfX39pGV6eXmNGg/THV/uaGlpgaenJ9auXftT+RAR/Vu4TJeIiP54RqMR1dXVaGlpQWpqKvz8/KT/MM3IyEB1dTVycnJgs9lQXFyMy5cvT7phzeLFiyGXy1FVVYWenh4MDAwAAIKDg1FSUgKr1YqGhgYkJiZOONM5lv379+PLly/YvXs3Ghsb0d7ejpKSErS1tQH4vvnRmTNnkJ+fD5vNhubmZphMJly4cGHcPGNjY/H69etJZ0dPnz6NmpoaqSxnm/R6PQwGA549e4aXL18iKSkJy5Ytg16vn1Lb3BEcHIyHDx/i+fPnsFqt2LdvH3p6ety+3mQyoaCgAFevXoWHhwe6u7vR3d0tzYpO1n8JCQnw8PCAwWBAa2srKioq3Prrm1WrVuHJkyfo7OyUfoCY7viyWCywWCyw2+34/PkzLBYLWltbXc55+vQpYmJipjy+iIhmCoNRIiL64+Xm5iI9PR0RERHo7u5GeXm5NLMZHh6OsrIylJaWYt26dTh+/DiMRiNSU1MnzFMmkyE/Px+FhYUICAiQgrIbN26gv78f4eHhSE5OxoEDB6RdZd2lVCpRU1MDu90OnU6HiIgIFBUVSbOke/fuxfXr12EymaBWq6HT6XDz5k3p72bGolarpbZOJCQkBGlpaRgeHnZJN5lMiIiIQHx8PLRaLYQQqKioGPO51p+VlZWF8PBwxMbGYvPmzViyZIn044E7Hj9+DIfDge3bt2Pp0qXSyxlQTtZ/3t7eKC8vR3NzMzQaDY4dO4azZ89OWq7RaMT79+8RFBQkLZ2d7vjSaDTQaDRoamrC7du3odFopNl2p9LSUhgMBrf7hYhopnkI8b990omIiP4wtbW12LJlC/r7+7Fw4cLZrs6su3//PjIzM9HS0oK//uLv1f9llZWVyMjIwKtXryCT8aksIvo18duJiIiIAABxcXFob29HZ2cnVqxYMdvVoZ8wODgIk8nEQJSIfmmcGSUioj8WZ0aJiIhmD4NRIiIiIiIimnF8IISIiIiIiIhmHINRIiIiIiIimnEMRomIiIiIiGjGMRglIiIiIiKiGcdglIiIiIiIiGYcg1EiIiIiIiKacQxGiYiIiIiIaMYxGCUiIiIiIqIZ9zesLqRGN27FpgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def select_top_features(feature_importances, top_n=10):\n",
    "    \"\"\"\n",
    "    Select the top N features based on importance.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_importances (pd.DataFrame): A dataframe containing feature names and their importance.\n",
    "    - top_n (int): Number of top features to select.\n",
    "    \n",
    "    Returns:\n",
    "    - selected_features (pd.DataFrame): A dataframe with the selected features and their importance.\n",
    "    \"\"\"\n",
    "    # Select the top N features\n",
    "    selected_features = feature_importances.iloc[:top_n].copy()\n",
    "\n",
    "    # Normalize the selected features' importance so that they sum to 1\n",
    "    selected_features['Importance'] /= selected_features['Importance'].sum()\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "selected_features_call_nn = select_top_features(feature_importances_call_nn, top_n=10)\n",
    "selected_features_put_nn = select_top_features(feature_importances_put_nn, top_n=10)\n",
    "\n",
    "# Plot Feature Importance for the top selected features\n",
    "def plot_feature_importance(feature_importances, title):\n",
    "    \"\"\"\n",
    "    Plot the feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_importances (pd.DataFrame): Feature importance dataframe.\n",
    "    - title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Importance (Normalized to 1)')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # Reverse order for better readability\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the selected features for Call and Put options\n",
    "plot_feature_importance(selected_features_call_nn, \"Selected Features for Call Options (NN Model)\")\n",
    "plot_feature_importance(selected_features_put_nn, \"Selected Features for Put Options (NN Model)\")\n",
    "\n",
    "# Average Feature Importance for Calls and Puts\n",
    "def average_feature_importance(feature_importance_call, feature_importance_put):\n",
    "    \"\"\"\n",
    "    Calculate the average feature importance for Calls and Puts and plot the average features.\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_importance_call (pd.DataFrame): Feature importance for Call options.\n",
    "    - feature_importance_put (pd.DataFrame): Feature importance for Put options.\n",
    "    \"\"\"\n",
    "    # Merge call and put feature importance\n",
    "    combined_importance = pd.merge(feature_importance_call[['Feature', 'Importance']],\n",
    "                                   feature_importance_put[['Feature', 'Importance']],\n",
    "                                   on='Feature', how='outer', suffixes=('_call', '_put')).fillna(0)\n",
    "\n",
    "    # Calculate average importance\n",
    "    combined_importance['Average'] = (combined_importance['Importance_call'] + combined_importance['Importance_put']) / 2\n",
    "\n",
    "    # Normalize the average importance so they sum to 1\n",
    "    combined_importance['Average'] /= combined_importance['Average'].sum()\n",
    "\n",
    "    # Sort by average importance and plot\n",
    "    combined_importance = combined_importance.sort_values(by='Average', ascending=False).head(10)\n",
    "\n",
    "    # Plot the average feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(combined_importance['Feature'], combined_importance['Average'], color='Blue', alpha=0.55, edgecolor='black')\n",
    "    # plt.xlabel('Average Importance (Normalized to 1)')\n",
    "    plt.title(\"NN (Put & Calls)\")\n",
    "    plt.gca().invert_yaxis()  # Reverse order for better readability\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return combined_importance\n",
    "\n",
    "# Plot the average feature importance for Calls and Puts\n",
    "avg_featimport = average_feature_importance(selected_features_call_nn, selected_features_put_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance_call</th>\n",
       "      <th>Importance_put</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.306055</td>\n",
       "      <td>0.277860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.128674</td>\n",
       "      <td>0.123391</td>\n",
       "      <td>0.126032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.087864</td>\n",
       "      <td>0.100107</td>\n",
       "      <td>0.093985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.071578</td>\n",
       "      <td>0.102666</td>\n",
       "      <td>0.087122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.070516</td>\n",
       "      <td>0.072661</td>\n",
       "      <td>0.071589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T</td>\n",
       "      <td>0.069011</td>\n",
       "      <td>0.071345</td>\n",
       "      <td>0.070178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.077270</td>\n",
       "      <td>0.054480</td>\n",
       "      <td>0.065875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.097286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.077726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIDLO</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature  Importance_call  Importance_put   Average\n",
       "10   prev_day_iv         0.249665        0.306055  0.277860\n",
       "2        FF_rate         0.128674        0.123391  0.126032\n",
       "11    reces_indi         0.087864        0.100107  0.093985\n",
       "9   prev2_day_iv         0.071578        0.102666  0.087122\n",
       "7     gold_price         0.070516        0.072661  0.071589\n",
       "5              T         0.069011        0.071345  0.070178\n",
       "8    hi-lo_stock         0.077270        0.054480  0.065875\n",
       "3       OPEN_vix         0.097286        0.000000  0.048643\n",
       "12    spread_vix         0.077726        0.000000  0.038863\n",
       "0          BIDLO         0.070411        0.000000  0.035205"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_featimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "      <th>Importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.249665</td>\n",
       "      <td>0.005350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.128674</td>\n",
       "      <td>0.003608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.097286</td>\n",
       "      <td>0.002761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.087864</td>\n",
       "      <td>0.002408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.077726</td>\n",
       "      <td>0.002712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.077270</td>\n",
       "      <td>0.002896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.071578</td>\n",
       "      <td>0.001803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.070516</td>\n",
       "      <td>0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIDLO</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.002673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T</td>\n",
       "      <td>0.069011</td>\n",
       "      <td>0.003905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  Importance  Importance_std\n",
       "0   prev_day_iv    0.249665        0.005350\n",
       "1       FF_rate    0.128674        0.003608\n",
       "2      OPEN_vix    0.097286        0.002761\n",
       "3    reces_indi    0.087864        0.002408\n",
       "4    spread_vix    0.077726        0.002712\n",
       "5   hi-lo_stock    0.077270        0.002896\n",
       "6  prev2_day_iv    0.071578        0.001803\n",
       "7    gold_price    0.070516        0.002149\n",
       "8         BIDLO    0.070411        0.002673\n",
       "9             T    0.069011        0.003905"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features_call_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top features for Call and Put options\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv', 'BIDLO', 'OPEN_vix'] used first\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv','gold_price', 'hi-lo_stock']\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '5_day_rolling_return_stock']\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv', 'cumulative_return', 'BIDLO'] used first\n",
    "\n",
    "\n",
    "# top_features_c = ['cp_flag', 'Tiker', 'date', 'impl_volatilicker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'prev2_day_iv', 'BIDLO', 'OPEN_vix','hi-lo_stock','FF_rate', 'gold_price', 'reces_indi','spread_vix' ]\n",
    "# top_features_p = ['cp_flag', 'Ticty', 'prev_day_iv', 'T', 'prev2_day_iv', 'cumulative_return', 'gold_price', 'reces_indi','FF_rate','hi-lo_stock', 'PRC_actual' , 'CLOSE_vix' ]\n",
    "\n",
    "\n",
    "# # Prepare train data for Call and Put options\n",
    "# data_train_c = data_train[data_train['cp_flag'] == 'C'][top_features_c]\n",
    "# data_train_p = data_train[data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare validation data for Call and Put options\n",
    "# data_validate_c = data_val[data_val['cp_flag'] == 'C'][top_features_c]\n",
    "# data_validate_p = data_val[data_val['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare the total train data for Call and Put options\n",
    "# data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C'][top_features_c]\n",
    "# data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C'][top_features_c]\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare the total train data for Call and Put options\n",
    "# data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C']\n",
    "# data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P']\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>T</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>spread_option</th>\n",
       "      <th>volume_option</th>\n",
       "      <th>...</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>2Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>C</td>\n",
       "      <td>0.433412</td>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.661302</td>\n",
       "      <td>-0.597376</td>\n",
       "      <td>-0.589313</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>2.446542</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-1.050440</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>C</td>\n",
       "      <td>0.428775</td>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.479080</td>\n",
       "      <td>-0.634582</td>\n",
       "      <td>-0.601956</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>7.399290</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-1.050440</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>C</td>\n",
       "      <td>0.426358</td>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.298260</td>\n",
       "      <td>-0.644805</td>\n",
       "      <td>-0.606978</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>2.519860</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-1.050440</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>C</td>\n",
       "      <td>0.422649</td>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.118841</td>\n",
       "      <td>-0.650397</td>\n",
       "      <td>-0.611027</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>4.486660</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-1.050440</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>C</td>\n",
       "      <td>0.421479</td>\n",
       "      <td>0.602872</td>\n",
       "      <td>-0.940824</td>\n",
       "      <td>-0.653209</td>\n",
       "      <td>-0.614172</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>4.257797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-1.050440</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>C</td>\n",
       "      <td>0.742669</td>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.185566</td>\n",
       "      <td>0.670290</td>\n",
       "      <td>0.487321</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.573211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.297669</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>C</td>\n",
       "      <td>0.764381</td>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.328541</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>0.537987</td>\n",
       "      <td>-0.465828</td>\n",
       "      <td>-0.119047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.297669</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>C</td>\n",
       "      <td>0.786317</td>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.471515</td>\n",
       "      <td>0.768025</td>\n",
       "      <td>0.589376</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>0.106170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.297669</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>C</td>\n",
       "      <td>0.798978</td>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.613087</td>\n",
       "      <td>0.807792</td>\n",
       "      <td>0.643192</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.297669</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>C</td>\n",
       "      <td>0.822016</td>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.753258</td>\n",
       "      <td>0.856795</td>\n",
       "      <td>0.694057</td>\n",
       "      <td>-0.487390</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.297669</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36352 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker       date cp_flag  impl_volatility         T  moneyness  \\\n",
       "0       AAPL 2021-01-04       C         0.433412  0.602872  -1.661302   \n",
       "1       AAPL 2021-01-04       C         0.428775  0.602872  -1.479080   \n",
       "2       AAPL 2021-01-04       C         0.426358  0.602872  -1.298260   \n",
       "3       AAPL 2021-01-04       C         0.422649  0.602872  -1.118841   \n",
       "4       AAPL 2021-01-04       C         0.421479  0.602872  -0.940824   \n",
       "...      ...        ...     ...              ...       ...        ...   \n",
       "36347   TSLA 2021-12-30       C         0.742669 -1.582517   1.185566   \n",
       "36348   TSLA 2021-12-30       C         0.764381 -1.582517   1.328541   \n",
       "36349   TSLA 2021-12-30       C         0.786317 -1.582517   1.471515   \n",
       "36350   TSLA 2021-12-30       C         0.798978 -1.582517   1.613087   \n",
       "36351   TSLA 2021-12-30       C         0.822016 -1.582517   1.753258   \n",
       "\n",
       "       prev2_day_iv  prev_day_iv  spread_option  volume_option  ...   1Y_bond  \\\n",
       "0         -0.597376    -0.589313      -0.498171       2.446542  ... -1.052185   \n",
       "1         -0.634582    -0.601956      -0.498171       7.399290  ... -1.052185   \n",
       "2         -0.644805    -0.606978      -0.498171       2.519860  ... -1.052185   \n",
       "3         -0.650397    -0.611027      -0.498171       4.486660  ... -1.052185   \n",
       "4         -0.653209    -0.614172      -0.508952       4.257797  ... -1.052185   \n",
       "...             ...          ...            ...            ...  ...       ...   \n",
       "36347      0.670290     0.487321      -0.508952       0.573211  ... -0.648240   \n",
       "36348      0.716221     0.537987      -0.465828      -0.119047  ... -0.648240   \n",
       "36349      0.768025     0.589376      -0.476609       0.106170  ... -0.648240   \n",
       "36350      0.807792     0.643192      -0.476609       0.032853  ... -0.648240   \n",
       "36351      0.856795     0.694057      -0.487390       0.152752  ... -0.648240   \n",
       "\n",
       "        2Y_bond  CLOSE_vix   FF_rate  HIGH_vix   LOW_vix  OPEN_vix  \\\n",
       "0     -1.050440  -0.215840 -1.007385 -0.269391 -0.167645 -0.242053   \n",
       "1     -1.050440  -0.215840 -1.007385 -0.269391 -0.167645 -0.242053   \n",
       "2     -1.050440  -0.215840 -1.007385 -0.269391 -0.167645 -0.242053   \n",
       "3     -1.050440  -0.215840 -1.007385 -0.269391 -0.167645 -0.242053   \n",
       "4     -1.050440  -0.215840 -1.007385 -0.269391 -0.167645 -0.242053   \n",
       "...         ...        ...       ...       ...       ...       ...   \n",
       "36347 -0.297669  -0.709054 -1.017284 -0.681385 -0.715059 -0.658102   \n",
       "36348 -0.297669  -0.709054 -1.017284 -0.681385 -0.715059 -0.658102   \n",
       "36349 -0.297669  -0.709054 -1.017284 -0.681385 -0.715059 -0.658102   \n",
       "36350 -0.297669  -0.709054 -1.017284 -0.681385 -0.715059 -0.658102   \n",
       "36351 -0.297669  -0.709054 -1.017284 -0.681385 -0.715059 -0.658102   \n",
       "\n",
       "       gold_price  reces_indi  spread_vix  \n",
       "0        1.256206    0.169346   -0.481824  \n",
       "1        1.256206    0.169346   -0.481824  \n",
       "2        1.256206    0.169346   -0.481824  \n",
       "3        1.256206    0.169346   -0.481824  \n",
       "4        1.256206    0.169346   -0.481824  \n",
       "...           ...         ...         ...  \n",
       "36347    0.776101   -0.945233   -0.460457  \n",
       "36348    0.776101   -0.945233   -0.460457  \n",
       "36349    0.776101   -0.945233   -0.460457  \n",
       "36350    0.776101   -0.945233   -0.460457  \n",
       "36351    0.776101   -0.945233   -0.460457  \n",
       "\n",
       "[36352 rows x 30 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Prepare the total train data for Call and Put options\n",
    "# data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C']\n",
    "# data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P']\n",
    "\n",
    "# Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "data_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, validation, and test data for Call options\n",
    "train_x_c = data_train_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "train_y_c = data_train_c['impl_volatility']\n",
    "\n",
    "# validate_x_c = data_validate_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "# validate_y_c = data_validate_c['impl_volatility']\n",
    "\n",
    "combined_x_c = data_train_tot_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "combined_y_c = data_train_tot_c['impl_volatility']\n",
    "\n",
    "test_x_c = data_test_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "test_y_c = data_test_c['impl_volatility']\n",
    "\n",
    "# Prepare train, validation, and test data for Put options\n",
    "train_x_p = data_train_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "train_y_p = data_train_p['impl_volatility']\n",
    "\n",
    "# validate_x_p = data_validate_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "# validate_y_p = data_validate_p['impl_volatility']\n",
    "\n",
    "combined_x_p = data_train_tot_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "combined_y_p = data_train_tot_p['impl_volatility']\n",
    "\n",
    "test_x_p = data_test_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "test_y_p = data_test_p['impl_volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>spread_option</th>\n",
       "      <th>volume_option</th>\n",
       "      <th>5_day_rolling_return_stock</th>\n",
       "      <th>ASK</th>\n",
       "      <th>ASKHI</th>\n",
       "      <th>BID</th>\n",
       "      <th>...</th>\n",
       "      <th>10Y_RIR</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.661302</td>\n",
       "      <td>-0.597376</td>\n",
       "      <td>-0.589313</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>2.446542</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-1.046156</td>\n",
       "      <td>-1.047221</td>\n",
       "      <td>-1.046290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641329</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.479080</td>\n",
       "      <td>-0.634582</td>\n",
       "      <td>-0.601956</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>7.399290</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-1.046156</td>\n",
       "      <td>-1.047221</td>\n",
       "      <td>-1.046290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641329</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.298260</td>\n",
       "      <td>-0.644805</td>\n",
       "      <td>-0.606978</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>2.519860</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-1.046156</td>\n",
       "      <td>-1.047221</td>\n",
       "      <td>-1.046290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641329</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.602872</td>\n",
       "      <td>-1.118841</td>\n",
       "      <td>-0.650397</td>\n",
       "      <td>-0.611027</td>\n",
       "      <td>-0.498171</td>\n",
       "      <td>4.486660</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-1.046156</td>\n",
       "      <td>-1.047221</td>\n",
       "      <td>-1.046290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641329</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602872</td>\n",
       "      <td>-0.940824</td>\n",
       "      <td>-0.653209</td>\n",
       "      <td>-0.614172</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>4.257797</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-1.046156</td>\n",
       "      <td>-1.047221</td>\n",
       "      <td>-1.046290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641329</td>\n",
       "      <td>-1.052185</td>\n",
       "      <td>-0.215840</td>\n",
       "      <td>-1.007385</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>-0.167645</td>\n",
       "      <td>-0.242053</td>\n",
       "      <td>1.256206</td>\n",
       "      <td>0.169346</td>\n",
       "      <td>-0.481824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.185566</td>\n",
       "      <td>0.670290</td>\n",
       "      <td>0.487321</td>\n",
       "      <td>-0.508952</td>\n",
       "      <td>0.573211</td>\n",
       "      <td>2.507680</td>\n",
       "      <td>-0.120726</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199799</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.328541</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>0.537987</td>\n",
       "      <td>-0.465828</td>\n",
       "      <td>-0.119047</td>\n",
       "      <td>2.507680</td>\n",
       "      <td>-0.120726</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199799</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.471515</td>\n",
       "      <td>0.768025</td>\n",
       "      <td>0.589376</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>0.106170</td>\n",
       "      <td>2.507680</td>\n",
       "      <td>-0.120726</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199799</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.613087</td>\n",
       "      <td>0.807792</td>\n",
       "      <td>0.643192</td>\n",
       "      <td>-0.476609</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>2.507680</td>\n",
       "      <td>-0.120726</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199799</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>-1.582517</td>\n",
       "      <td>1.753258</td>\n",
       "      <td>0.856795</td>\n",
       "      <td>0.694057</td>\n",
       "      <td>-0.487390</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>2.507680</td>\n",
       "      <td>-0.120726</td>\n",
       "      <td>-0.106940</td>\n",
       "      <td>-0.120734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199799</td>\n",
       "      <td>-0.648240</td>\n",
       "      <td>-0.709054</td>\n",
       "      <td>-1.017284</td>\n",
       "      <td>-0.681385</td>\n",
       "      <td>-0.715059</td>\n",
       "      <td>-0.658102</td>\n",
       "      <td>0.776101</td>\n",
       "      <td>-0.945233</td>\n",
       "      <td>-0.460457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36352 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              T  moneyness  prev2_day_iv  prev_day_iv  spread_option  \\\n",
       "0      0.602872  -1.661302     -0.597376    -0.589313      -0.498171   \n",
       "1      0.602872  -1.479080     -0.634582    -0.601956      -0.498171   \n",
       "2      0.602872  -1.298260     -0.644805    -0.606978      -0.498171   \n",
       "3      0.602872  -1.118841     -0.650397    -0.611027      -0.498171   \n",
       "4      0.602872  -0.940824     -0.653209    -0.614172      -0.508952   \n",
       "...         ...        ...           ...          ...            ...   \n",
       "36347 -1.582517   1.185566      0.670290     0.487321      -0.508952   \n",
       "36348 -1.582517   1.328541      0.716221     0.537987      -0.465828   \n",
       "36349 -1.582517   1.471515      0.768025     0.589376      -0.476609   \n",
       "36350 -1.582517   1.613087      0.807792     0.643192      -0.476609   \n",
       "36351 -1.582517   1.753258      0.856795     0.694057      -0.487390   \n",
       "\n",
       "       volume_option  5_day_rolling_return_stock       ASK     ASKHI  \\\n",
       "0           2.446542                    0.011508 -1.046156 -1.047221   \n",
       "1           7.399290                    0.011508 -1.046156 -1.047221   \n",
       "2           2.519860                    0.011508 -1.046156 -1.047221   \n",
       "3           4.486660                    0.011508 -1.046156 -1.047221   \n",
       "4           4.257797                    0.011508 -1.046156 -1.047221   \n",
       "...              ...                         ...       ...       ...   \n",
       "36347       0.573211                    2.507680 -0.120726 -0.106940   \n",
       "36348      -0.119047                    2.507680 -0.120726 -0.106940   \n",
       "36349       0.106170                    2.507680 -0.120726 -0.106940   \n",
       "36350       0.032853                    2.507680 -0.120726 -0.106940   \n",
       "36351       0.152752                    2.507680 -0.120726 -0.106940   \n",
       "\n",
       "            BID  ...   10Y_RIR   1Y_bond  CLOSE_vix   FF_rate  HIGH_vix  \\\n",
       "0     -1.046290  ... -0.641329 -1.052185  -0.215840 -1.007385 -0.269391   \n",
       "1     -1.046290  ... -0.641329 -1.052185  -0.215840 -1.007385 -0.269391   \n",
       "2     -1.046290  ... -0.641329 -1.052185  -0.215840 -1.007385 -0.269391   \n",
       "3     -1.046290  ... -0.641329 -1.052185  -0.215840 -1.007385 -0.269391   \n",
       "4     -1.046290  ... -0.641329 -1.052185  -0.215840 -1.007385 -0.269391   \n",
       "...         ...  ...       ...       ...        ...       ...       ...   \n",
       "36347 -0.120734  ...  0.199799 -0.648240  -0.709054 -1.017284 -0.681385   \n",
       "36348 -0.120734  ...  0.199799 -0.648240  -0.709054 -1.017284 -0.681385   \n",
       "36349 -0.120734  ...  0.199799 -0.648240  -0.709054 -1.017284 -0.681385   \n",
       "36350 -0.120734  ...  0.199799 -0.648240  -0.709054 -1.017284 -0.681385   \n",
       "36351 -0.120734  ...  0.199799 -0.648240  -0.709054 -1.017284 -0.681385   \n",
       "\n",
       "        LOW_vix  OPEN_vix  gold_price  reces_indi  spread_vix  \n",
       "0     -0.167645 -0.242053    1.256206    0.169346   -0.481824  \n",
       "1     -0.167645 -0.242053    1.256206    0.169346   -0.481824  \n",
       "2     -0.167645 -0.242053    1.256206    0.169346   -0.481824  \n",
       "3     -0.167645 -0.242053    1.256206    0.169346   -0.481824  \n",
       "4     -0.167645 -0.242053    1.256206    0.169346   -0.481824  \n",
       "...         ...       ...         ...         ...         ...  \n",
       "36347 -0.715059 -0.658102    0.776101   -0.945233   -0.460457  \n",
       "36348 -0.715059 -0.658102    0.776101   -0.945233   -0.460457  \n",
       "36349 -0.715059 -0.658102    0.776101   -0.945233   -0.460457  \n",
       "36350 -0.715059 -0.658102    0.776101   -0.945233   -0.460457  \n",
       "36351 -0.715059 -0.658102    0.776101   -0.945233   -0.460457  \n",
       "\n",
       "[36352 rows x 30 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from scikeras.wrappers import KerasRegressor  # Use SciKeras instead\n",
    "# from keras.optimizers import RMSprop\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(neurons=32, layers=1, dropout_rate=0.0, bias=0.01, activity=0.01):\n",
    "#     model = Sequential()\n",
    "#     # Input layer (first hidden layer)\n",
    "#     model.add(Dense(neurons, activation='relu', input_dim=train_x.shape[1],\n",
    "#                     bias_regularizer=regularizers.L2(bias),\n",
    "#                     activity_regularizer=regularizers.L2(activity)))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu',\n",
    "#                         bias_regularizer=regularizers.L2(bias),\n",
    "#                         activity_regularizer=regularizers.L2(activity)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Function to perform hyperparameter tuning, retrain the model, and test\n",
    "# def train_and_evaluate(train_x, train_y, validate_x, validate_y, combined_x, combined_y, test_x, test_y):\n",
    "#     # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "#     grid_search = GridSearchCV(estimator=model,\n",
    "#                                param_grid=param_grid,\n",
    "#                                scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "#                                cv=3,  # 3-fold cross-validation\n",
    "#                                verbose=1)  # Verbose for tracking progress\n",
    "\n",
    "#     # Hyperparameter tuning on training and validation sets\n",
    "#     print(\"Running hyperparameter tuning...\")\n",
    "#     grid_search.fit(train_x, train_y, validation_data=(validate_x, validate_y))\n",
    "\n",
    "#     # Get the best estimator and parameters\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "\n",
    "#     print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "#     # Retrain on the combined training and validation set\n",
    "#     best_model.fit(combined_x, combined_y)\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predictions = best_model.predict(test_x)\n",
    "\n",
    "#     # Calculate R² and RMSE for the best model\n",
    "#     r2 = r2_score(test_y, predictions)\n",
    "#     rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f\"R²: {r2:.4f}\")\n",
    "#     print(f\"RMSE: {rmse:.4f}\")\n",
    "#     return best_model\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# print(\"\\nEvaluating Call options...\")\n",
    "# best_model_call = train_and_evaluate(train_x_c, train_y_c, validate_x_c, validate_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# print(\"\\nEvaluating Put options...\")\n",
    "# best_model_put = train_and_evaluate(train_x_p, train_y_p, validate_x_p, validate_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout, Input\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0):\n",
    "#     model = Sequential()\n",
    "#     # Input layer using Input instead of input_dim argument\n",
    "#     model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "#     # First hidden layer\n",
    "#     model.add(Dense(neurons, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu'))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# def create_keras_regressor(input_dim):\n",
    "#     return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [64]}  # Put\n",
    "# param_grid_call = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [32]}  # Call\n",
    "\n",
    "# # Function to perform hyperparameter tuning, retrain the model, and test\n",
    "# def train_and_evaluate(train_x, train_y, combined_x, combined_y, test_x, test_y, param_grid, option_type):\n",
    "#     # Get input dimension from the training data\n",
    "#     input_dim = combined_x.shape[1]\n",
    "    \n",
    "#     # Create KerasRegressor with the correct input dimension\n",
    "#     model = create_keras_regressor(input_dim)\n",
    "\n",
    "#     # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "#     grid_search = GridSearchCV(estimator=model,\n",
    "#                                param_grid=param_grid,\n",
    "#                                scoring='neg_mean_squared_error',\n",
    "#                                verbose=3,\n",
    "#                                cv=5, \n",
    "#                                n_jobs=-1)\n",
    "\n",
    "#     # Hyperparameter tuning using validation data\n",
    "#     print(f\"Running hyperparameter tuning with validation data for {option_type}...\")\n",
    "#     grid_search.fit(combined_x, combined_y, verbose=1)\n",
    "\n",
    "#     # Get the best estimator and parameters\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "\n",
    "#     print(f\"Best Parameters for {option_type}: {best_params}\")\n",
    "\n",
    "#     # In-sample evaluation on the combined training and validation set\n",
    "#     predictions_combined = best_model.predict(combined_x)\n",
    "#     r2_combined = r2_score(combined_y, predictions_combined)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(combined_y, predictions_combined))\n",
    "#     print(f\"In-sample R² ({option_type}): {r2_combined:.4f}\")\n",
    "#     print(f\"In-sample RMSE ({option_type}): {rmse_combined:.4f}\")\n",
    "\n",
    "#     # Out-of-sample evaluation on the test set\n",
    "#     predictions_test = best_model.predict(test_x)\n",
    "#     r2_test = r2_score(test_y, predictions_test)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(test_y, predictions_test))\n",
    "#     print(f\"Out-of-sample R² ({option_type}): {r2_test:.4f}\")\n",
    "#     print(f\"Out-of-sample RMSE ({option_type}): {rmse_test:.4f}\")\n",
    "\n",
    "#     return best_model, predictions_test\n",
    "\n",
    "# # Extract feature importances based on model weights\n",
    "# def extract_feature_importances(model, X_train, option_type):\n",
    "#     \"\"\"\n",
    "#     Extract feature importance from the weights of the first hidden layer.\n",
    "#     This is a rough approximation based on the magnitude of weights.\n",
    "#     \"\"\"\n",
    "#     # Get the weights of the first hidden layer\n",
    "#     first_layer_weights = model.model_.layers[0].get_weights()[0]\n",
    "\n",
    "#     # Compute the importance as the sum of absolute weights across the neurons\n",
    "#     feature_importances = np.sum(np.abs(first_layer_weights), axis=1)\n",
    "\n",
    "#     # Normalize the importances so they sum to 1\n",
    "#     feature_importances = feature_importances / np.sum(feature_importances)\n",
    "\n",
    "#     # Create a pandas Series for easy handling\n",
    "#     feature_importances = pd.Series(feature_importances, index=X_train.columns)\n",
    "    \n",
    "#     print(f\"Feature importances for {option_type}:\\n\", feature_importances)\n",
    "    \n",
    "#     return feature_importances\n",
    "\n",
    "# # Plot Feature Importance for Keras Neural Network\n",
    "# def plot_feature_importance(feature_importances, option_type, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Create a bar plot showing the top N features based on Keras model weights.\n",
    "#     \"\"\"\n",
    "#     # Select the top N features\n",
    "#     top_features = feature_importances.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "#     # Plot the top N feature importances\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = top_features.sort_values(ascending=True).plot(kind='barh', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "#     plt.grid(True)  # Add grid\n",
    "#     ax.set_title(f'Top {top_n} Feature Importance ({option_type})')\n",
    "#     plt.show()\n",
    "\n",
    "# # Combine feature importances from Call and Put options and plot the top 5\n",
    "# def plot_combined_feature_importance(feature_importances_call, feature_importances_put, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Create a bar plot showing the top N average feature importance of both Call and Put options.\n",
    "#     \"\"\"\n",
    "#     # Combine the feature importances from Call and Put options\n",
    "#     combined_importance = pd.concat([feature_importances_call, feature_importances_put], axis=1, keys=[\"Call\", \"Put\"]).fillna(0)\n",
    "\n",
    "#     # Calculate the average importance\n",
    "#     combined_importance['Average'] = combined_importance.mean(axis=1)\n",
    "\n",
    "#     # Select the top N features based on the average importance\n",
    "#     top_features = combined_importance['Average'].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "#     # Plot the top N combined feature importance\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = top_features.sort_values(ascending=True).plot(kind='barh', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "#     plt.grid(True)  # Add grid\n",
    "#     ax.set_title('NN (Put and Call)')\n",
    "#     plt.show()\n",
    "\n",
    "# # # Example usage\n",
    "# # print(\"\\nEvaluating Call options...\")\n",
    "# # best_model_call, predictions_call = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c, param_grid_call, 'Call')\n",
    "\n",
    "# # print(\"\\nEvaluating Put options...\")\n",
    "# # best_model_put, predictions_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p, param_grid, 'Put')\n",
    "\n",
    "# # # Extract and plot feature importance for Call options\n",
    "# # feature_importances_call = extract_feature_importances(best_model_call, combined_x_c, 'Call')\n",
    "# # plot_feature_importance(feature_importances_call, 'Call')\n",
    "\n",
    "# # # Extract and plot feature importance for Put options\n",
    "# # feature_importances_put = extract_feature_importances(best_model_put, combined_x_p, 'Put')\n",
    "# # plot_feature_importance(feature_importances_put, 'Put')\n",
    "\n",
    "# # # Plot combined feature importance for both Call and Put options\n",
    "# # plot_combined_feature_importance(feature_importances_call, feature_importances_put, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to calculate permutation importance on the training set\n",
    "# def permutation_importance(model, X_train, y_train, metric=mean_squared_error, option_type=\"Call\"):\n",
    "#     \"\"\"\n",
    "#     Calculate permutation feature importance by shuffling one feature at a time and measuring the impact on the model's performance.\n",
    "    \n",
    "#     Parameters:\n",
    "#     model: The trained neural network model.\n",
    "#     X_train (pd.DataFrame): Training feature set.\n",
    "#     y_train (pd.Series): Training target variable.\n",
    "#     metric: The metric to measure performance, defaults to mean_squared_error.\n",
    "#     option_type (str): To indicate if it is Call or Put.\n",
    "    \n",
    "#     Returns:\n",
    "#     pd.Series: Normalized feature importances (how much each feature influences model performance, scaled to sum to 1).\n",
    "#     \"\"\"\n",
    "#     # Baseline performance with no shuffling\n",
    "#     baseline_performance = metric(y_train, model.predict(X_train))\n",
    "\n",
    "#     # Initialize an empty dictionary to store the performance drop for each feature\n",
    "#     performance_drop = {}\n",
    "\n",
    "#     # Loop over each feature in the training set\n",
    "#     for feature in X_train.columns:\n",
    "#         # Shuffle the feature\n",
    "#         X_train_shuffled = X_train.copy()\n",
    "#         X_train_shuffled[feature] = np.random.permutation(X_train_shuffled[feature])\n",
    "\n",
    "#         # Measure the model performance on the shuffled training set\n",
    "#         shuffled_performance = metric(y_train, model.predict(X_train_shuffled))\n",
    "\n",
    "#         # The importance of the feature is the drop in performance\n",
    "#         performance_drop[feature] = shuffled_performance - baseline_performance\n",
    "\n",
    "#     # Convert to pandas Series for easier handling and sorting\n",
    "#     feature_importance = pd.Series(performance_drop).abs().sort_values(ascending=False)\n",
    "\n",
    "#     # Normalize the feature importance to sum to 1\n",
    "#     feature_importance = feature_importance / feature_importance.sum()\n",
    "\n",
    "#     print(f\"Normalized Permutation Feature Importance for {option_type}:\\n\", feature_importance)\n",
    "\n",
    "#     return feature_importance\n",
    "\n",
    "# # Plot Feature Importance for Keras Neural Network\n",
    "# def plot_feature_importance(feature_importances, option_type, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Create a bar plot showing the top N features based on permutation importance.\n",
    "#     \"\"\"\n",
    "#     # Select the top N features\n",
    "#     top_features = feature_importances.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "#     # Plot the top N feature importances\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = top_features.sort_values(ascending=True).plot(kind='barh', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "#     plt.grid(True)  # Add grid\n",
    "#     ax.set_title(f'Top {top_n} Feature Importance ({option_type})')\n",
    "#     plt.show()\n",
    "\n",
    "# # Combine feature importances from Call and Put options and plot the top 5\n",
    "# def plot_combined_feature_importance(feature_importances_call, feature_importances_put, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Create a bar plot showing the top N average feature importance of both Call and Put options.\n",
    "#     \"\"\"\n",
    "#     # Combine the feature importances from Call and Put options\n",
    "#     combined_importance = pd.concat([feature_importances_call, feature_importances_put], axis=1, keys=[\"Call\", \"Put\"]).fillna(0)\n",
    "\n",
    "#     # Calculate the average importance\n",
    "#     combined_importance['Average'] = combined_importance.mean(axis=1)\n",
    "\n",
    "#     # Select the top N features based on the average importance\n",
    "#     top_features = combined_importance['Average'].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "#     # Plot the top N combined feature importance\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     ax = top_features.sort_values(ascending=True).plot(kind='barh', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "#     plt.grid(True)  # Add grid\n",
    "#     ax.set_title('Combined Feature Importance (Put and Call)')\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# print(\"\\nEvaluating Call options...\")\n",
    "# best_model_call, predictions_call = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c, param_grid_call, 'Call')\n",
    "\n",
    "# print(\"\\nEvaluating Put options...\")\n",
    "# best_model_put, predictions_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p, param_grid, 'Put')\n",
    "\n",
    "# # Calculate and plot permutation feature importance for Call options\n",
    "# feature_importances_call = permutation_importance(best_model_call, combined_x_c, combined_y_c, option_type='Call')\n",
    "# plot_feature_importance(feature_importances_call, 'Call')\n",
    "\n",
    "# # Calculate and plot permutation feature importance for Put options\n",
    "# feature_importances_put = permutation_importance(best_model_put, combined_x_p, combined_y_p, option_type='Put')\n",
    "# plot_feature_importance(feature_importances_put, 'Put')\n",
    "\n",
    "# # Plot combined feature importance for both Call and Put options\n",
    "# plot_combined_feature_importance(feature_importances_call, feature_importances_put, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>T</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>BIDLO</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>hi-lo_stock</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>cumulative_return</th>\n",
       "      <th>spread_vix</th>\n",
       "      <th>vol_stock</th>\n",
       "      <th>5_day_rolling_return_stock</th>\n",
       "      <th>spread_stock</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>RET</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>10Y_RIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.542073</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.699846</td>\n",
       "      <td>-0.133918</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.125902</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>1.562822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.759052</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.699846</td>\n",
       "      <td>-0.133918</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.125902</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-1.062773</td>\n",
       "      <td>1.562822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.716917</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.699846</td>\n",
       "      <td>-0.133918</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.125902</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-0.482466</td>\n",
       "      <td>1.562822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.693295</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.699846</td>\n",
       "      <td>-0.133918</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.125902</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>0.088029</td>\n",
       "      <td>1.562822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.674080</td>\n",
       "      <td>-0.854054</td>\n",
       "      <td>-1.519873</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-0.484127</td>\n",
       "      <td>1.124610</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.699846</td>\n",
       "      <td>-0.133918</td>\n",
       "      <td>0.737871</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.125902</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>0.650114</td>\n",
       "      <td>1.562822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72342</th>\n",
       "      <td>0.024429</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>-0.090463</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>6.899552</td>\n",
       "      <td>-0.552678</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.047492</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>1.060814</td>\n",
       "      <td>-0.800536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72343</th>\n",
       "      <td>0.024429</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>-0.090463</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>6.899552</td>\n",
       "      <td>-0.552678</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.047492</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>1.156130</td>\n",
       "      <td>-0.800536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72344</th>\n",
       "      <td>0.110670</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>-0.071801</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>-0.090463</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>6.899552</td>\n",
       "      <td>-0.552678</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.047492</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>1.345361</td>\n",
       "      <td>-0.800536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72345</th>\n",
       "      <td>0.158024</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>-0.037014</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>-0.090463</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>6.899552</td>\n",
       "      <td>-0.552678</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.047492</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>1.533190</td>\n",
       "      <td>-0.800536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72346</th>\n",
       "      <td>0.208890</td>\n",
       "      <td>1.331335</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.057404</td>\n",
       "      <td>-0.090463</td>\n",
       "      <td>-1.160795</td>\n",
       "      <td>1.438222</td>\n",
       "      <td>0.268029</td>\n",
       "      <td>6.899552</td>\n",
       "      <td>-0.552678</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.168781</td>\n",
       "      <td>-0.047492</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>0.020966</td>\n",
       "      <td>1.718215</td>\n",
       "      <td>-0.800536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72347 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prev_day_iv         T  prev2_day_iv     BIDLO  OPEN_vix  hi-lo_stock  \\\n",
       "0        -1.542073 -0.854054     -1.519873 -0.567166  0.674867    -0.484127   \n",
       "1        -0.759052 -0.854054     -1.519873 -0.567166  0.674867    -0.484127   \n",
       "2        -0.716917 -0.854054     -1.519873 -0.567166  0.674867    -0.484127   \n",
       "3        -0.693295 -0.854054     -1.519873 -0.567166  0.674867    -0.484127   \n",
       "4        -0.674080 -0.854054     -1.519873 -0.567166  0.674867    -0.484127   \n",
       "...            ...       ...           ...       ...       ...          ...   \n",
       "72342     0.024429  1.331335     -0.140819  0.072540 -0.057404    -0.090463   \n",
       "72343     0.024429  1.331335     -0.140819  0.072540 -0.057404    -0.090463   \n",
       "72344     0.110670  1.331335     -0.071801  0.072540 -0.057404    -0.090463   \n",
       "72345     0.158024  1.331335     -0.037014  0.072540 -0.057404    -0.090463   \n",
       "72346     0.208890  1.331335      0.001623  0.072540 -0.057404    -0.090463   \n",
       "\n",
       "        FF_rate  gold_price  reces_indi  cumulative_return  spread_vix  \\\n",
       "0      1.124610   -1.353864   -0.709827          -0.699846   -0.133918   \n",
       "1      1.124610   -1.353864   -0.709827          -0.699846   -0.133918   \n",
       "2      1.124610   -1.353864   -0.709827          -0.699846   -0.133918   \n",
       "3      1.124610   -1.353864   -0.709827          -0.699846   -0.133918   \n",
       "4      1.124610   -1.353864   -0.709827          -0.699846   -0.133918   \n",
       "...         ...         ...         ...                ...         ...   \n",
       "72342 -1.160795    1.438222    0.268029           6.899552   -0.552678   \n",
       "72343 -1.160795    1.438222    0.268029           6.899552   -0.552678   \n",
       "72344 -1.160795    1.438222    0.268029           6.899552   -0.552678   \n",
       "72345 -1.160795    1.438222    0.268029           6.899552   -0.552678   \n",
       "72346 -1.160795    1.438222    0.268029           6.899552   -0.552678   \n",
       "\n",
       "       vol_stock  5_day_rolling_return_stock  spread_stock   1Y_bond  \\\n",
       "0       0.737871                   -0.286276     -0.125902  1.371916   \n",
       "1       0.737871                   -0.286276     -0.125902  1.371916   \n",
       "2       0.737871                   -0.286276     -0.125902  1.371916   \n",
       "3       0.737871                   -0.286276     -0.125902  1.371916   \n",
       "4       0.737871                   -0.286276     -0.125902  1.371916   \n",
       "...          ...                         ...           ...       ...   \n",
       "72342   0.003482                    0.168781     -0.047492 -1.213802   \n",
       "72343   0.003482                    0.168781     -0.047492 -1.213802   \n",
       "72344   0.003482                    0.168781     -0.047492 -1.213802   \n",
       "72345   0.003482                    0.168781     -0.047492 -1.213802   \n",
       "72346   0.003482                    0.168781     -0.047492 -1.213802   \n",
       "\n",
       "       CLOSE_vix       RET  moneyness   10Y_RIR  \n",
       "0       0.476238 -0.080584  -1.651490  1.562822  \n",
       "1       0.476238 -0.080584  -1.062773  1.562822  \n",
       "2       0.476238 -0.080584  -0.482466  1.562822  \n",
       "3       0.476238 -0.080584   0.088029  1.562822  \n",
       "4       0.476238 -0.080584   0.650114  1.562822  \n",
       "...          ...       ...        ...       ...  \n",
       "72342  -0.034979  0.020966   1.060814 -0.800536  \n",
       "72343  -0.034979  0.020966   1.156130 -0.800536  \n",
       "72344  -0.034979  0.020966   1.345361 -0.800536  \n",
       "72345  -0.034979  0.020966   1.533190 -0.800536  \n",
       "72346  -0.034979  0.020966   1.718215 -0.800536  \n",
       "\n",
       "[72347 rows x 19 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_x_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Call options...\n",
      "Running hyperparameter tuning with validation data...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516us/step - loss: 1.0461 - mse: 1.0461\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 575us/step - loss: 0.8704 - mse: 0.8704\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 0.5006 - mse: 0.5006\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681us/step - loss: 0.1136 - mse: 0.1136\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626us/step - loss: 0.0616 - mse: 0.0616\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653us/step - loss: 0.0706 - mse: 0.0706\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581us/step - loss: 0.0684 - mse: 0.0684\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633us/step - loss: 0.0630 - mse: 0.0630\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551us/step - loss: 0.0300 - mse: 0.0300\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step - loss: 0.0576 - mse: 0.0576\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - loss: 0.0549 - mse: 0.0549\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - loss: 0.0238 - mse: 0.0238\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - loss: 0.0581 - mse: 0.0581\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 429us/step - loss: 0.0235 - mse: 0.0235\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 646us/step - loss: 0.0529 - mse: 0.0529\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655us/step - loss: 0.0575 - mse: 0.0575\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689us/step - loss: 0.0563 - mse: 0.0563\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642us/step - loss: 0.0222 - mse: 0.0222\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 598us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 591us/step - loss: 0.0548 - mse: 0.0548\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 573us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544us/step - loss: 0.0561 - mse: 0.0561\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 568us/step - loss: 0.0549 - mse: 0.0549\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 603us/step - loss: 0.0564 - mse: 0.0564\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 0.0551 - mse: 0.0551\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 643us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560us/step - loss: 0.0550 - mse: 0.0550\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 0.0540 - mse: 0.0540 \n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 469us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534us/step - loss: 0.0558 - mse: 0.0558\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 596us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 647us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - loss: 0.0213 - mse: 0.0213\n",
      "\u001b[1m 707/1521\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 498us/step - loss: 0.0567 - mse: 0.0567Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step - loss: 0.0558 - mse: 0.0558\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417us/step - loss: 0.0532 - mse: 0.0532\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882us/step - loss: 0.0568 - mse: 0.0568\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 999us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869us/step - loss: 0.0219 - mse: 0.0219\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 655us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 630us/step - loss: 0.0553 - mse: 0.0553\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 503us/step - loss: 0.0525 - mse: 0.0525\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - loss: 0.0542 - mse: 0.0542\n",
      "\u001b[1m 121/1521\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 417us/step - loss: 0.0201 - mse: 0.0201Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - loss: 0.0548 - mse: 0.0548\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - loss: 0.0554 - mse: 0.0554\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623us/step - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669us/step - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 483us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 590us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569us/step - loss: 0.0557 - mse: 0.0557\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - loss: 0.0203 - mse: 0.0203\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - loss: 0.0549 - mse: 0.0549\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611us/step - loss: 0.0560 - mse: 0.0560\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - loss: 0.0526 - mse: 0.0526\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 580us/step - loss: 0.0212 - mse: 0.0212\n",
      "\u001b[1m1407/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 593us/step - loss: 0.0559 - mse: 0.0559Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 633us/step - loss: 0.0558 - mse: 0.0558\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 0.0529 - mse: 0.0529\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 481us/step - loss: 0.0210 - mse: 0.0210\n",
      "\u001b[1m   1/1521\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - loss: 0.0494 - mse: 0.0494Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618us/step - loss: 0.0546 - mse: 0.0546\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594us/step - loss: 0.0551 - mse: 0.0551\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 683us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 565us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 619us/step - loss: 0.0550 - mse: 0.0550\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 607us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 769us/step - loss: 0.0551 - mse: 0.0551\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 682us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 510us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618us/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721us/step - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 542us/step - loss: 0.0556 - mse: 0.0556\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 616us/step - loss: 0.0532 - mse: 0.0532\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 585us/step - loss: 0.0203 - mse: 0.0203\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 620us/step - loss: 0.0553 - mse: 0.0553\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628us/step - loss: 0.0538 - mse: 0.0538\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567us/step - loss: 0.0206 - mse: 0.0206\n",
      "\u001b[1m1166/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 0.0550 - mse: 0.0550Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 576us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 621us/step - loss: 0.0525 - mse: 0.0525  \n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502us/step - loss: 0.0545 - mse: 0.0545\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440us/step - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - loss: 0.0561 - mse: 0.0561\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741us/step - loss: 0.0522 - mse: 0.0522\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/stepep - loss: 0.0545 - mse: 0.05\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step - loss: 0.0546 - mse: 0.0546\n",
      "Epoch 49/50\n",
      "\u001b[1m   1/1521\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 23ms/step - loss: 0.0416 - mse: 0.0416[CV 2/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.031 total time=  48.2s\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692us/step - loss: 0.0210 - mse: 0.0210\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/stepep - loss: 0.0536 - mse: 0.05\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 50/50\n",
      "[CV 3/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.112 total time=  48.8s\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477us/step - loss: 0.0544 - mse: 0.0544\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step\n",
      "[CV 1/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.017 total time=  49.9s\n",
      "Epoch 1/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 362us/step - loss: 0.4036 - mse: 0.4036\n",
      "Epoch 2/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 530us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 3/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453us/step - loss: 0.0476 - mse: 0.0476\n",
      "Epoch 4/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 5/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364us/step - loss: 0.0468 - mse: 0.0468\n",
      "Epoch 6/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 513us/step - loss: 0.0466 - mse: 0.0466\n",
      "Epoch 7/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 8/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 434us/step - loss: 0.0464 - mse: 0.0464\n",
      "Epoch 9/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 10/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656us/step - loss: 0.0456 - mse: 0.0456\n",
      "Epoch 11/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 12/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step - loss: 0.0463 - mse: 0.0463\n",
      "Epoch 13/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0464 - mse: 0.0464\n",
      "Epoch 14/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 15/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455us/step - loss: 0.0455 - mse: 0.0455\n",
      "Epoch 16/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440us/step - loss: 0.0465 - mse: 0.0465\n",
      "Epoch 17/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0460 - mse: 0.0460\n",
      "Epoch 18/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - loss: 0.0456 - mse: 0.0456\n",
      "Epoch 19/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 20/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 432us/step - loss: 0.0446 - mse: 0.0446\n",
      "Epoch 21/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 22/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 23/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 24/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458us/step - loss: 0.0449 - mse: 0.0449\n",
      "Epoch 25/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 26/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559us/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 27/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step - loss: 0.0456 - mse: 0.0456\n",
      "Epoch 28/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447us/step - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 29/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 30/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 535us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 31/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 32/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 33/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 34/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 35/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 36/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 37/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 38/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 39/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 40/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431us/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 41/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 42/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 533us/step - loss: 0.0460 - mse: 0.0460\n",
      "Epoch 43/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 44/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 45/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 46/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 47/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 48/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430us/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 49/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 50/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 502us/step - loss: 0.0453 - mse: 0.0453\n",
      "Best Parameters: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.5, 'model__layers': 1, 'model__neurons': 16}\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361us/step\n",
      "In-sample R²: 0.8124\n",
      "In-sample RMSE: 0.1706\n",
      "\u001b[1m1105/1105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step\n",
      "Out-of-sample R²: 0.6989\n",
      "Out-of-sample RMSE: 0.1354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    # Input layer using Input instead of input_dim argument\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Wrapping the model in KerasRegressor\n",
    "def create_keras_regressor(input_dim):\n",
    "    return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)  # Set verbose=1 for model fit\n",
    "\n",
    "# Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'model__neurons': [16, 32, 64],    # Number of neurons in each hidden layer\n",
    "#     'model__layers': [1,2],        # Number of hidden layers\n",
    "#     'model__dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32],                   # Batch size for training\n",
    "#     'epochs': [50],                       # Number of epochs\n",
    "# }\n",
    "\n",
    "param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.5], 'model__layers': [1], 'model__neurons': [16]} # Put\n",
    "# param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [64]} # Call\n",
    "# Function to perform hyperparameter tuning, retrain the model, and test\n",
    "def train_and_evaluate(train_x, train_y, combined_x, combined_y, test_x, test_y):\n",
    "    # Get input dimension from the training data\n",
    "    input_dim = combined_x.shape[1]\n",
    "    \n",
    "    # Create KerasRegressor with the correct input dimension\n",
    "    model = create_keras_regressor(input_dim)\n",
    "\n",
    "    # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "                               verbose=3,\n",
    "                               cv=3, \n",
    "                               n_jobs=-1)  # Verbose=3 for detailed progress tracking\n",
    "\n",
    "    # Hyperparameter tuning using validation data\n",
    "    print(\"Running hyperparameter tuning with validation data...\")\n",
    "    grid_search.fit(combined_x, combined_y,\n",
    "                    verbose=1)\n",
    "\n",
    "    # Get the best estimator and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # In-sample evaluation on the combined training and validation set\n",
    "    predictions_combined = best_model.predict(combined_x)\n",
    "\n",
    "    r2_combined = r2_score(combined_y, predictions_combined)\n",
    "    rmse_combined = np.sqrt(mean_squared_error(combined_y, predictions_combined))\n",
    "    \n",
    "    print(f\"In-sample R²: {r2_combined:.4f}\")\n",
    "    print(f\"In-sample RMSE: {rmse_combined:.4f}\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions_test = best_model.predict(test_x)\n",
    "\n",
    "    # Out-of-sample evaluation on the test set\n",
    "    r2_test = r2_score(test_y, predictions_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(test_y, predictions_test))\n",
    "\n",
    "    print(f\"Out-of-sample R²: {r2_test:.4f}\")\n",
    "    print(f\"Out-of-sample RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Call the function for Call options data\n",
    "print(\"\\nEvaluating Call options...\")\n",
    "# best_model_call = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# print(\"\\nEvaluating Put options...\")\n",
    "best_model_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x38dc22ca0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=31\n",
       "\tmodel__dropout_rate=0.2\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=16\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KerasRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x38dc22ca0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=31\n",
       "\tmodel__dropout_rate=0.2\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=16\n",
       ")</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function create_model at 0x38dc22ca0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=31\n",
       "\tmodel__dropout_rate=0.2\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=16\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Call options...\n",
      "Running hyperparameter tuning with validation data...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353us/step - loss: 0.5219 - mse: 0.5219 \n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.6702 - mse: 0.6702\n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 1.1912 - mse: 1.1912\n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0603 - mse: 0.0603\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step - loss: 0.0676 - mse: 0.0676\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0281 - mse: 0.0281\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446us/step - loss: 0.0577 - mse: 0.0577\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 465us/step - loss: 0.0243 - mse: 0.0243\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - loss: 0.0562 - mse: 0.0562\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442us/step - loss: 0.0239 - mse: 0.0239\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 431us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 6/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 6/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449us/step - loss: 0.0236 - mse: 0.0236\n",
      "Epoch 6/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0224 - mse: 0.0224\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0226 - mse: 0.0226\n",
      "\u001b[1m 149/1508\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 338us/step - loss: 0.0494 - mse: 0.0494Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0504 - mse: 0.0504\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0225 - mse: 0.0225\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0529 - mse: 0.0529\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.0225 - mse: 0.0225\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358us/step - loss: 0.0512 - mse: 0.0512\n",
      "\u001b[1m 154/1508\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 327us/step - loss: 0.0489 - mse: 0.0489Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0225 - mse: 0.0225\n",
      "Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0222 - mse: 0.0222\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0230 - mse: 0.0230\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0501 - mse: 0.0501\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0502 - mse: 0.0502\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350us/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - loss: 0.0533 - mse: 0.0533\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0526 - mse: 0.0526\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0495 - mse: 0.0495\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0222 - mse: 0.0222\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0533 - mse: 0.0533\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0500 - mse: 0.0500\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step - loss: 0.0525 - mse: 0.0525\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361us/step - loss: 0.0221 - mse: 0.0221\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.0509 - mse: 0.0509\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step - loss: 0.0489 - mse: 0.0489\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - loss: 0.0508 - mse: 0.0508\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - loss: 0.0510 - mse: 0.0510\n",
      "Epoch 27/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 27/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 27/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step - loss: 0.0516 - mse: 0.0516\n",
      "\u001b[1m 422/1508\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 357us/step - loss: 0.0207 - mse: 0.0207Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0496 - mse: 0.0496\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0512 - mse: 0.0512\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0219 - mse: 0.0219\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step - loss: 0.0496 - mse: 0.0496\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0512 - mse: 0.0512\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0513 - mse: 0.0513\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0489 - mse: 0.0489\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step - loss: 0.0493 - mse: 0.0493\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430us/step - loss: 0.0509 - mse: 0.0509\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459us/step - loss: 0.0491 - mse: 0.0491\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 505us/step - loss: 0.0216 - mse: 0.0216\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 482us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417us/step - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0500 - mse: 0.0500\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0483 - mse: 0.0483\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0492 - mse: 0.0492\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0513 - mse: 0.0513\n",
      "Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408us/step - loss: 0.0490 - mse: 0.0490\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0502 - mse: 0.0502\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0500 - mse: 0.0500\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0491 - mse: 0.0491\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332us/step - loss: 0.0523 - mse: 0.0523\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263us/stepep - loss: 0.0203 - mse: 0.024\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0205 - mse: 0.0205\n",
      "[CV 2/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.114 total time=  29.1s\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0525 - mse: 0.0525\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261us/step\n",
      "\u001b[1m734/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 205us/step[CV 3/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.185 total time=  29.4s\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step\n",
      "[CV 1/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.032 total time=  29.6s\n",
      "Epoch 1/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 455us/step - loss: 0.3798 - mse: 0.3798\n",
      "Epoch 2/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816us/step - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 3/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 4/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 5/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305us/step - loss: 0.0449 - mse: 0.0449\n",
      "Epoch 6/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291us/step - loss: 0.0441 - mse: 0.0441\n",
      "Epoch 7/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 8/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 9/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 10/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 11/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 12/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 313us/step - loss: 0.0427 - mse: 0.0427\n",
      "Epoch 13/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 14/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 15/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 16/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 17/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317us/step - loss: 0.0433 - mse: 0.0433\n",
      "Epoch 18/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 306us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 19/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 20/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 21/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 22/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 23/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 286us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 24/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 25/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 26/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 27/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 28/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 29/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 30/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 31/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 32/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 33/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 34/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 35/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 36/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 37/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 38/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 39/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 40/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 41/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 42/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 43/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 44/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 45/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 46/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 47/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 48/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 49/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 50/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292us/step - loss: 0.0430 - mse: 0.0430\n",
      "Best Parameters: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.5, 'model__layers': 1, 'model__neurons': 16}\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214us/step\n",
      "In-sample R²: 0.7921\n",
      "In-sample RMSE: 0.1674\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213us/step\n",
      "Out-of-sample R²: 0.6836\n",
      "Out-of-sample RMSE: 0.1342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    # Input layer using Input instead of input_dim argument\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Wrapping the model in KerasRegressor\n",
    "def create_keras_regressor(input_dim):\n",
    "    return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)  # Set verbose=1 for model fit\n",
    "\n",
    "# Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'model__neurons': [16, 32, 64],    # Number of neurons in each hidden layer\n",
    "#     'model__layers': [1,2],        # Number of hidden layers\n",
    "#     'model__dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32],                   # Batch size for training\n",
    "#     'epochs': [50],                       # Number of epochs\n",
    "# }\n",
    "\n",
    "param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.5], 'model__layers': [1], 'model__neurons': [16]} # Put\n",
    "# param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [64]} # Call\n",
    "# Function to perform hyperparameter tuning, retrain the model, and test\n",
    "def train_and_evaluate(train_x, train_y, combined_x, combined_y, test_x, test_y):\n",
    "    # Get input dimension from the training data\n",
    "    input_dim = combined_x.shape[1]\n",
    "    \n",
    "    # Create KerasRegressor with the correct input dimension\n",
    "    model = create_keras_regressor(input_dim)\n",
    "\n",
    "    # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "                               verbose=3,\n",
    "                               cv=3, \n",
    "                               n_jobs=-1)  # Verbose=3 for detailed progress tracking\n",
    "\n",
    "    # Hyperparameter tuning using validation data\n",
    "    print(\"Running hyperparameter tuning with validation data...\")\n",
    "    grid_search.fit(combined_x, combined_y,\n",
    "                    verbose=1)\n",
    "\n",
    "    # Get the best estimator and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # In-sample evaluation on the combined training and validation set\n",
    "    predictions_combined = best_model.predict(combined_x)\n",
    "\n",
    "    r2_combined = r2_score(combined_y, predictions_combined)\n",
    "    rmse_combined = np.sqrt(mean_squared_error(combined_y, predictions_combined))\n",
    "    \n",
    "    print(f\"In-sample R²: {r2_combined:.4f}\")\n",
    "    print(f\"In-sample RMSE: {rmse_combined:.4f}\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions_test = best_model.predict(test_x)\n",
    "\n",
    "    # Out-of-sample evaluation on the test set\n",
    "    r2_test = r2_score(test_y, predictions_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(test_y, predictions_test))\n",
    "\n",
    "    print(f\"Out-of-sample R²: {r2_test:.4f}\")\n",
    "    print(f\"Out-of-sample RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Call the function for Call options data\n",
    "print(\"\\nEvaluating Call options...\")\n",
    "best_model_call = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# print(\"\\nEvaluating Put options...\")\n",
    "# best_model_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Call options...\n",
      "Running hyperparameter tuning for Call Model...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.6407 - mse: 0.6407 \n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 1.6090 - mse: 1.6090\n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.9213 - mse: 0.9213\n",
      "Epoch 2/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0611 - mse: 0.0611\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366us/step - loss: 0.1225 - mse: 0.1225\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0720 - mse: 0.0720\n",
      "Epoch 3/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step - loss: 0.0556 - mse: 0.0556\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765us/step - loss: 0.0613 - mse: 0.0613\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779us/step - loss: 0.0597 - mse: 0.0597\n",
      "Epoch 4/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 0.0509 - mse: 0.0509\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0563 - mse: 0.0563\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 0.0314 - mse: 0.0314\n",
      "Epoch 5/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 363us/step - loss: 0.0556 - mse: 0.0556\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 6/50\n",
      "Epoch 6/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 0.0245 - mse: 0.0245\n",
      "Epoch 6/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - loss: 0.0557 - mse: 0.0557\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - loss: 0.0232 - mse: 0.0232\n",
      "Epoch 7/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - loss: 0.0548 - mse: 0.0548\n",
      "Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0513 - mse: 0.0513\n",
      "Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - loss: 0.0221 - mse: 0.0221\n",
      "Epoch 8/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442us/step - loss: 0.0508 - mse: 0.0508\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0224 - mse: 0.0224\n",
      "Epoch 9/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0221 - mse: 0.0221\n",
      "Epoch 10/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367us/step - loss: 0.0502 - mse: 0.0502\n",
      "Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0222 - mse: 0.0222\n",
      "Epoch 11/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 12/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0533 - mse: 0.0533\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 13/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0517 - mse: 0.0517\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 14/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.0510 - mse: 0.0510\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 15/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0515 - mse: 0.0515\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 16/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0516 - mse: 0.0516\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 17/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0515 - mse: 0.0515\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 18/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 19/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - loss: 0.0529 - mse: 0.0529\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 0.0512 - mse: 0.0512\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 20/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 21/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0504 - mse: 0.0504\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 22/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 23/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0506 - mse: 0.0506\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 24/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0510 - mse: 0.0510\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 25/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 26/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0518 - mse: 0.0518\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 27/50\n",
      "Epoch 27/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 27/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 28/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0495 - mse: 0.0495\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 29/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359us/step - loss: 0.0507 - mse: 0.0507\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 30/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366us/step - loss: 0.0491 - mse: 0.0491\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 31/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0521 - mse: 0.0521\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 32/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 33/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0206 - mse: 0.0206 \n",
      "Epoch 34/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0504 - mse: 0.0504\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 35/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0497 - mse: 0.0497\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353us/step - loss: 0.0201 - mse: 0.0201\n",
      "Epoch 36/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - loss: 0.0493 - mse: 0.0493\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 37/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0500 - mse: 0.0500\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 38/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 409us/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 39/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 584us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430us/step - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 40/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 41/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0508 - mse: 0.0508\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 42/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0502 - mse: 0.0502\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 43/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0500 - mse: 0.0500\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 44/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 336us/step - loss: 0.0511 - mse: 0.0511\n",
      "Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0203 - mse: 0.0203\n",
      "\u001b[1m 593/1508\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 339us/step - loss: 0.0518 - mse: 0.0518Epoch 45/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 345us/step - loss: 0.0509 - mse: 0.0509\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0203 - mse: 0.0203\n",
      "Epoch 46/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0532 - mse: 0.0532\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0497 - mse: 0.0497\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 47/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0512 - mse: 0.0512\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0495 - mse: 0.0495\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0200 - mse: 0.0200\n",
      "Epoch 48/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0502 - mse: 0.0502\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 49/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 50/50\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0505 - mse: 0.0505\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0504 - mse: 0.0504\n",
      "\u001b[1m1508/1508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0210 - mse: 0.0210\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260us/step\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250us/step\n",
      "\u001b[1m202/754\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 249us/step[CV 1/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.019 total time=  29.2s\n",
      "\u001b[1m445/754\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 225us/step[CV 2/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.075 total time=  29.3s\n",
      "\u001b[1m754/754\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step\n",
      "[CV 3/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.116 total time=  29.4s\n",
      "Epoch 1/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304us/step - loss: 0.3798 - mse: 0.3798\n",
      "Epoch 2/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step - loss: 0.0480 - mse: 0.0480\n",
      "Epoch 3/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 4/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 5/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step - loss: 0.0449 - mse: 0.0449\n",
      "Epoch 6/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0441 - mse: 0.0441\n",
      "Epoch 7/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 8/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 9/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 10/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 11/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 12/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0427 - mse: 0.0427\n",
      "Epoch 13/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 14/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 15/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 16/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 17/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0433 - mse: 0.0433\n",
      "Epoch 18/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 19/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 20/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 21/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 22/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 23/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 24/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 25/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 26/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 27/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294us/step - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 28/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 29/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 30/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 31/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 32/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 33/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 34/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step - loss: 0.0435 - mse: 0.0435\n",
      "Epoch 35/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 36/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 37/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 38/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 39/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 40/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 41/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 42/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 43/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 44/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 45/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 324us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 46/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 47/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 48/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 49/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326us/step - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 50/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.0430 - mse: 0.0430\n",
      "Best Parameters for Call Model: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.5, 'model__layers': 1, 'model__neurons': 16}\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 362us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18772/1964460253.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_std_error = ols_model.bse[0]  # Newey-West standard error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Model Newey-West Standard Error: 0.0010\n",
      "\n",
      "Evaluating Put options...\n",
      "Running hyperparameter tuning for Put Model...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.6656 - mse: 0.6656\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0620 - mse: 0.0620\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577us/step - loss: 0.0570 - mse: 0.0570\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414us/step - loss: 0.0554 - mse: 0.0554\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step - loss: 0.0573 - mse: 0.0573\n",
      "Epoch 6/50\n",
      "\u001b[1m 336/1521\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 300us/step - loss: 0.0541 - mse: 0.0541Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428us/step - loss: 0.0535 - mse: 0.0535  \n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497us/step - loss: 0.8271 - mse: 0.8271\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 499us/step - loss: 1.4350 - mse: 1.4350\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 680us/step - loss: 0.0550 - mse: 0.0550\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628us/step - loss: 0.0686 - mse: 0.0686\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 631us/step - loss: 0.1295 - mse: 0.1295\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426us/step - loss: 0.0330 - mse: 0.0330\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423us/step - loss: 0.0706 - mse: 0.0706\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 424us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - loss: 0.0255 - mse: 0.0255\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 0.0621 - mse: 0.0621\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 474us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725us/step - loss: 0.0238 - mse: 0.0238\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - loss: 0.0602 - mse: 0.0602\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 861us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 998us/step - loss: 0.0580 - mse: 0.0580\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0231 - mse: 0.0231\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 963us/step - loss: 0.0540 - mse: 0.0540\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605us/step - loss: 0.0576 - mse: 0.0576\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623us/step - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543us/step - loss: 0.0537 - mse: 0.0537\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step - loss: 0.0585 - mse: 0.0585\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 907us/step - loss: 0.0221 - mse: 0.0221\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445us/step - loss: 0.0575 - mse: 0.0575\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0216 - mse: 0.0216\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - loss: 0.0574 - mse: 0.0574\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 501us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - loss: 0.0528 - mse: 0.0528\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step - loss: 0.0561 - mse: 0.0561\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845us/step - loss: 0.0533 - mse: 0.0533\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 838us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 477us/step - loss: 0.0569 - mse: 0.0569\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 498us/step - loss: 0.0531 - mse: 0.0531\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504us/step - loss: 0.0225 - mse: 0.0225\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - loss: 0.0566 - mse: 0.0566\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0572 - mse: 0.0572\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 498us/step - loss: 0.0536 - mse: 0.0536\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 525us/step - loss: 0.0543 - mse: 0.0543\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 432us/step - loss: 0.0513 - mse: 0.0513\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636us/step - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 642us/step - loss: 0.0510 - mse: 0.0510\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step - loss: 0.0558 - mse: 0.0558\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 421us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410us/step - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0544 - mse: 0.0544\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 541us/step - loss: 0.0565 - mse: 0.0565\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 468us/step - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364us/step - loss: 0.0547 - mse: 0.0547\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0551 - mse: 0.0551\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358us/step - loss: 0.0546 - mse: 0.0546\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 362us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519us/step - loss: 0.0564 - mse: 0.0564\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 549us/step - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - loss: 0.0542 - mse: 0.0542\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367us/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359us/step - loss: 0.0525 - mse: 0.0525\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - loss: 0.0549 - mse: 0.0549\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 528us/step - loss: 0.0518 - mse: 0.0518\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0547 - mse: 0.0547\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0213 - mse: 0.0213\n",
      "\u001b[1m1228/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 409us/step - loss: 0.0535 - mse: 0.0535Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560us/step - loss: 0.0550 - mse: 0.0550\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 610us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - loss: 0.0541 - mse: 0.0541\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 0.0519 - mse: 0.0519\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - loss: 0.0551 - mse: 0.0551\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612us/step - loss: 0.0216 - mse: 0.0216\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 691us/step - loss: 0.0530 - mse: 0.0530\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440us/step - loss: 0.0538 - mse: 0.0538\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 433us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0535 - mse: 0.0535\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415us/step - loss: 0.0543 - mse: 0.0543\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 419us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 420us/step - loss: 0.0522 - mse: 0.0522\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0527 - mse: 0.0527\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0552 - mse: 0.0552\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step - loss: 0.0216 - mse: 0.0216\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 367us/step - loss: 0.0525 - mse: 0.0525\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0554 - mse: 0.0554\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0523 - mse: 0.0523\n",
      "\u001b[1m 694/1521\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 362us/step - loss: 0.0531 - mse: 0.0531Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 361us/step - loss: 0.0542 - mse: 0.0542\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0549 - mse: 0.0549\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464us/step - loss: 0.0534 - mse: 0.0534\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414us/step - loss: 0.0546 - mse: 0.0546\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411us/step - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 342us/step - loss: 0.0499 - mse: 0.0499\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 430us/step - loss: 0.0562 - mse: 0.0562\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step - loss: 0.0523 - mse: 0.0523\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - loss: 0.0564 - mse: 0.0564\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0576 - mse: 0.0576\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358us/step - loss: 0.0515 - mse: 0.0515\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0547 - mse: 0.0547\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 362us/step - loss: 0.0214 - mse: 0.0214\n",
      "\u001b[1m 267/1521\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.0572 - mse: 0.0572Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352us/step - loss: 0.0507 - mse: 0.0507\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271us/stepep - loss: 0.0213 - mse: 0.026\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step - loss: 0.0564 - mse: 0.0564\n",
      "Epoch 46/50\n",
      "\u001b[1m1283/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 0.0212 - mse: 0.0212[CV 2/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.059 total time=  37.8s\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - loss: 0.0217 - mse: 0.0217\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - loss: 0.0539 - mse: 0.0539\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.0567 - mse: 0.0567\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303us/step - loss: 0.0211 - mse: 0.0211\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529us/step - loss: 0.0555 - mse: 0.0555\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 540us/step - loss: 0.0214 - mse: 0.0214\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - loss: 0.0555 - mse: 0.0555\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - loss: 0.0221 - mse: 0.0221\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236us/step\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232us/step\n",
      "[CV 1/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.017 total time=  37.0s\n",
      "[CV 3/3] END batch_size=32, epochs=50, model__dropout_rate=0.5, model__layers=1, model__neurons=16;, score=-0.090 total time=  37.1s\n",
      "Epoch 1/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309us/step - loss: 0.6023 - mse: 0.6023\n",
      "Epoch 2/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 3/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0473 - mse: 0.0473\n",
      "Epoch 4/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step - loss: 0.0466 - mse: 0.0466\n",
      "Epoch 5/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282us/step - loss: 0.0455 - mse: 0.0455\n",
      "Epoch 6/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step - loss: 0.0455 - mse: 0.0455\n",
      "Epoch 7/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 338us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 8/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 9/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 10/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 492us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 11/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 12/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277us/step - loss: 0.0449 - mse: 0.0449\n",
      "Epoch 13/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 14/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0448 - mse: 0.0448\n",
      "Epoch 15/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296us/step - loss: 0.0446 - mse: 0.0446\n",
      "Epoch 16/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 17/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 18/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 19/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 20/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279us/step - loss: 0.0447 - mse: 0.0447\n",
      "Epoch 21/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 22/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 23/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 24/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 25/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 26/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320us/step - loss: 0.0447 - mse: 0.0447\n",
      "Epoch 27/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 28/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 29/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322us/step - loss: 0.0436 - mse: 0.0436\n",
      "Epoch 30/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312us/step - loss: 0.0440 - mse: 0.0440\n",
      "Epoch 31/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 32/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step - loss: 0.0437 - mse: 0.0437\n",
      "Epoch 33/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 34/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 35/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 410us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 36/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 37/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 38/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0444 - mse: 0.0444\n",
      "Epoch 39/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 346us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 40/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0440 - mse: 0.0440\n",
      "Epoch 41/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 278us/step - loss: 0.0441 - mse: 0.0441\n",
      "Epoch 42/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 43/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 44/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 45/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264us/step - loss: 0.0433 - mse: 0.0433\n",
      "Epoch 46/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0446 - mse: 0.0446\n",
      "Epoch 47/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359us/step - loss: 0.0442 - mse: 0.0442\n",
      "Epoch 48/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309us/step - loss: 0.0438 - mse: 0.0438\n",
      "Epoch 49/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step - loss: 0.0434 - mse: 0.0434\n",
      "Epoch 50/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0439 - mse: 0.0439\n",
      "Best Parameters for Put Model: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.5, 'model__layers': 1, 'model__neurons': 16}\n",
      "\u001b[1m1105/1105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219us/step\n",
      "Put Model Newey-West Standard Error: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18772/1964460253.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_std_error = ols_model.bse[0]  # Newey-West standard error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test errors and Newey-West statistics saved to /Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_nn.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "\n",
    "# Function to calculate Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West standard error for the series of prediction errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - errors: Array-like of prediction errors\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - Newey-West standard error of the prediction errors\n",
    "    \"\"\"\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    nw_std_error = ols_model.bse[0]  # Newey-West standard error\n",
    "    return nw_std_error\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))  # Input layer\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Regression output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Wrapping the model in KerasRegressor\n",
    "def create_keras_regressor(input_dim):\n",
    "    return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)\n",
    "\n",
    "# Function to train the model and calculate Newey-West errors\n",
    "def train_and_evaluate(train_x, train_y, combined_x, combined_y, test_x, test_y, model_name):\n",
    "    # Get input dimension from the training data\n",
    "    input_dim = combined_x.shape[1]\n",
    "    \n",
    "    # Create KerasRegressor\n",
    "    model = create_keras_regressor(input_dim)\n",
    "\n",
    "    # Initialize GridSearchCV with the model and parameter grid\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "                               verbose=3,\n",
    "                               cv=3, \n",
    "                               n_jobs=-1)\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    print(f\"Running hyperparameter tuning for {model_name}...\")\n",
    "    grid_search.fit(combined_x, combined_y, verbose=1)\n",
    "\n",
    "    # Get the best estimator and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best Parameters for {model_name}: {best_params}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions_test = best_model.predict(test_x)\n",
    "\n",
    "    # Calculate test errors and Newey-West standard error\n",
    "    errors_test = test_y - predictions_test\n",
    "    nw_std_error = newey_west_standard_error(errors_test, lag=1)\n",
    "\n",
    "    print(f\"{model_name} Newey-West Standard Error: {nw_std_error:.4f}\")\n",
    "\n",
    "    return errors_test, nw_std_error\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.5], 'model__layers': [1], 'model__neurons': [16]}\n",
    "\n",
    "# Call the function for Call options data\n",
    "print(\"\\nEvaluating Call options...\")\n",
    "calls_errors, calls_nw_std_error = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c, \"Call Model\")\n",
    "\n",
    "# Call the function for Put options data\n",
    "print(\"\\nEvaluating Put options...\")\n",
    "puts_errors, puts_nw_std_error = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p, \"Put Model\")\n",
    "\n",
    "# Pad the shorter error list with zeros to ensure both lists have the same length\n",
    "max_length = max(len(puts_errors), len(calls_errors))\n",
    "\n",
    "puts_errors_padded = np.pad(puts_errors, (0, max_length - len(puts_errors)), 'constant', constant_values=0)\n",
    "calls_errors_padded = np.pad(calls_errors, (0, max_length - len(calls_errors)), 'constant', constant_values=0)\n",
    "\n",
    "# Save the results to a DataFrame and CSV\n",
    "dm_test_data = pd.DataFrame({\n",
    "    'Put Errors': puts_errors_padded,\n",
    "    'Call Errors': calls_errors_padded,\n",
    "    'Put Newey-West Std Error': [puts_nw_std_error] * max_length,  # Constant value for all rows\n",
    "    'Call Newey-West Std Error': [calls_nw_std_error] * max_length  # Constant value for all rows\n",
    "})\n",
    "\n",
    "# Specify the file path for the CSV\n",
    "file_path = '/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_nn.csv'\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "dm_test_data.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Test errors and Newey-West statistics saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout, Input\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "# import tensorflow as tf\n",
    "# import random\n",
    "\n",
    "# # Set random seeds for reproducibility\n",
    "# def set_seed(seed=42):\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "# # Set the seed before training\n",
    "# set_seed(42)\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0):\n",
    "#     model = Sequential()\n",
    "#     # Input layer using Input instead of input_dim argument\n",
    "#     model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "#     # First hidden layer\n",
    "#     model.add(Dense(neurons, activation='relu'))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu'))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# def create_keras_regressor(input_dim):\n",
    "#     return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)  # Set verbose=1 for model fit\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'model__neurons': [16, 32, 64],    # Number of neurons in each hidden layer\n",
    "#     'model__layers': [1,2,3,4],        # Number of hidden layers\n",
    "#     'model__dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32],                   # Batch size for training\n",
    "#     'epochs': [50],                       # Number of epochs\n",
    "# }\n",
    "\n",
    "# # param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [32]} # Put\n",
    "# # param_grid = {'batch_size': [32], 'epochs': [50], 'model__dropout_rate': [0.2], 'model__layers': [2], 'model__neurons': [32]} # Call\n",
    "\n",
    "# # Function to perform hyperparameter tuning, retrain the model, and test\n",
    "# def train_and_evaluate(train_x, train_y, combined_x, combined_y, test_x, test_y):\n",
    "#     # Get input dimension from the training data\n",
    "#     input_dim = combined_x.shape[1]\n",
    "    \n",
    "#     # Create KerasRegressor with the correct input dimension\n",
    "#     model = create_keras_regressor(input_dim)\n",
    "\n",
    "#     # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "#     grid_search = GridSearchCV(estimator=model,\n",
    "#                                param_grid=param_grid,\n",
    "#                                scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "#                                verbose=3,\n",
    "#                                cv=5, \n",
    "#                                n_jobs=-1)  # Verbose=3 for detailed progress tracking\n",
    "\n",
    "#     # Hyperparameter tuning using validation data\n",
    "#     print(\"Running hyperparameter tuning with validation data...\")\n",
    "#     grid_search.fit(combined_x, combined_y,\n",
    "#                     verbose=1)\n",
    "\n",
    "#     # Get the best estimator and parameters\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "\n",
    "#     print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "#     # In-sample evaluation on the combined training and validation set\n",
    "#     predictions_combined = best_model.predict(combined_x)\n",
    "\n",
    "#     r2_combined = r2_score(combined_y, predictions_combined)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(combined_y, predictions_combined))\n",
    "    \n",
    "#     print(f\"In-sample R²: {r2_combined:.4f}\")\n",
    "#     print(f\"In-sample RMSE: {rmse_combined:.4f}\")\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predictions_test = best_model.predict(test_x)\n",
    "\n",
    "#     # Out-of-sample evaluation on the test set\n",
    "#     r2_test = r2_score(test_y, predictions_test)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(test_y, predictions_test))\n",
    "\n",
    "#     print(f\"Out-of-sample R²: {r2_test:.4f}\")\n",
    "#     print(f\"Out-of-sample RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# print(\"\\nEvaluating Call options...\")\n",
    "# best_model_call = train_and_evaluate(train_x_c, train_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# # # Call the function for Put options data\n",
    "# # print(\"\\nEvaluating Put options...\")\n",
    "# # best_model_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x17532a8e0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=10\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=1\n",
       "\tmodel__neurons=32\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KerasRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x17532a8e0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=10\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=1\n",
       "\tmodel__neurons=32\n",
       ")</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function create_model at 0x17532a8e0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=10\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=1\n",
       "\tmodel__neurons=32\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 514us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "results_c = permutation_importance(best_model_call, combined_x_c, combined_y_c, n_repeats=1, random_state=42) #NEW\n",
    "    # Get the feature importances and feature names\n",
    "importance = results_c.importances_mean #NEW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x332e80c20&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=30\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=32\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KerasRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_model at 0x332e80c20&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=30\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=32\n",
       ")</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function create_model at 0x332e80c20>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=32\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       "\tinput_dim=30\n",
       "\tmodel__dropout_rate=0.5\n",
       "\tmodel__layers=2\n",
       "\tmodel__neurons=32\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240us/step\n",
      "Mean Squared Error (MSE): 0.028032797160525903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.756283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.191636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CLOSE_vix</td>\n",
       "      <td>0.170249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.164074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>0.091177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.064454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LOW_vix</td>\n",
       "      <td>0.053092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.037955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.035771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASK</td>\n",
       "      <td>0.032649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.028910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HIGH_vix</td>\n",
       "      <td>0.023458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume_option</td>\n",
       "      <td>0.022929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PRC_actual</td>\n",
       "      <td>0.022223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.022017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.017977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.011096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spread_stock</td>\n",
       "      <td>0.010428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spread_option</td>\n",
       "      <td>0.009768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.002408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5_day_rolling_return_stock</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RET</td>\n",
       "      <td>0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>daily_return_indicator_stock</td>\n",
       "      <td>0.000870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moneyness</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vol_stock</td>\n",
       "      <td>-0.000127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature  importance\n",
       "3                    prev_day_iv    0.756283\n",
       "17                       2Y_bond    0.191636\n",
       "18                     CLOSE_vix    0.170249\n",
       "16                       1Y_bond    0.164074\n",
       "0                              T    0.091177\n",
       "2                   prev2_day_iv    0.064454\n",
       "21                       LOW_vix    0.053092\n",
       "10             cumulative_return    0.037955\n",
       "15                       10Y_RIR    0.035771\n",
       "7                            ASK    0.032649\n",
       "19                       FF_rate    0.028910\n",
       "20                      HIGH_vix    0.023458\n",
       "5                  volume_option    0.022929\n",
       "8                     PRC_actual    0.022223\n",
       "22                      OPEN_vix    0.022017\n",
       "23                    gold_price    0.017977\n",
       "25                    spread_vix    0.011096\n",
       "13                  spread_stock    0.010428\n",
       "4                  spread_option    0.009768\n",
       "12                   hi-lo_stock    0.002408\n",
       "6     5_day_rolling_return_stock    0.002233\n",
       "9                            RET    0.001572\n",
       "24                    reces_indi    0.001191\n",
       "11  daily_return_indicator_stock    0.000870\n",
       "1                      moneyness    0.000804\n",
       "14                     vol_stock   -0.000127"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions\n",
    "pred_y = best_model_call.predict(combined_x_c)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse_score = mean_squared_error(combined_y_c, pred_y)\n",
    "\n",
    "# Assuming 'importance' is a list or array of feature importances\n",
    "# Create a DataFrame with feature importance scores\n",
    "feature_importance_networks = pd.DataFrame({\n",
    "    'feature': combined_x_c.columns,  # Assuming feature names come from 'combined_x_p'\n",
    "    'importance': importance  # Assuming 'importance' is a list or array of the same length as the number of features\n",
    "})\n",
    "\n",
    "# Print the MSE score (separately from the feature importances)\n",
    "print(f\"Mean Squared Error (MSE): {mse_score}\")\n",
    "\n",
    "# Sort by 'importance' in descending order\n",
    "sorted_importance_c = feature_importance_networks.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display sorted feature importances\n",
    "(sorted_importance_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance_barplot(sorted_importance, top_n=5):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar plot showing the top N feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    sorted_importance (pd.DataFrame): Sorted feature importance DataFrame.\n",
    "    top_n (int): Number of top features to plot.\n",
    "    \"\"\"\n",
    "    # Select the top N features based on importance\n",
    "    top_features = sorted_importance.head(top_n)\n",
    "\n",
    "    # Sort the top N features by importance for plotting\n",
    "    top_features = top_features.sort_values(by='importance', ascending=True)\n",
    "\n",
    "    # Plot the top N feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = top_features.plot(kind='barh', x='feature', y='importance', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Remove the y-label\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Remove the legend\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    # Add grid and title\n",
    "    plt.grid(True)\n",
    "    ax.set_title('Top N Feature Importance (Put Options)')\n",
    "    \n",
    "    # plt.tight_layout()  # Optional: for better layout\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot the top 5 features from Put options data\n",
    "plot_feature_importance_barplot(sorted_importance_c, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 407us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 562us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 329us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 333us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 846us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 464us/step\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step\n",
      "Mean Squared Error (MSE): 0.02909987179759471\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.767611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.132207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>0.073404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.055579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.052455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.052418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HIGH_vix</td>\n",
       "      <td>0.044147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LOW_vix</td>\n",
       "      <td>0.041290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.033465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PRC_actual</td>\n",
       "      <td>0.031785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.027772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume_option</td>\n",
       "      <td>0.019116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASK</td>\n",
       "      <td>0.018770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.018363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.016358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.016285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CLOSE_vix</td>\n",
       "      <td>0.014797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spread_option</td>\n",
       "      <td>0.008814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spread_stock</td>\n",
       "      <td>0.008096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.005731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moneyness</td>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5_day_rolling_return_stock</td>\n",
       "      <td>0.002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RET</td>\n",
       "      <td>0.001929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.001798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>daily_return_indicator_stock</td>\n",
       "      <td>0.001103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vol_stock</td>\n",
       "      <td>-0.000118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature  importance\n",
       "3                    prev_day_iv    0.767611\n",
       "17                       2Y_bond    0.132207\n",
       "0                              T    0.073404\n",
       "2                   prev2_day_iv    0.055579\n",
       "15                       10Y_RIR    0.052455\n",
       "10             cumulative_return    0.052418\n",
       "20                      HIGH_vix    0.044147\n",
       "21                       LOW_vix    0.041290\n",
       "22                      OPEN_vix    0.033465\n",
       "8                     PRC_actual    0.031785\n",
       "16                       1Y_bond    0.027772\n",
       "5                  volume_option    0.019116\n",
       "7                            ASK    0.018770\n",
       "23                    gold_price    0.018363\n",
       "19                       FF_rate    0.016358\n",
       "25                    spread_vix    0.016285\n",
       "18                     CLOSE_vix    0.014797\n",
       "4                  spread_option    0.008814\n",
       "13                  spread_stock    0.008096\n",
       "24                    reces_indi    0.005731\n",
       "1                      moneyness    0.003681\n",
       "6     5_day_rolling_return_stock    0.002382\n",
       "9                            RET    0.001929\n",
       "12                   hi-lo_stock    0.001798\n",
       "11  daily_return_indicator_stock    0.001103\n",
       "14                     vol_stock   -0.000118"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results_p = permutation_importance(best_model_put, combined_x_p, combined_y_p, n_repeats=1, random_state=42) #NEW\n",
    "    # Get the feature importances and feature names\n",
    "importance_p = results_p.importances_mean #NEW\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "pred_y_p = best_model_put.predict(combined_x_p)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse_score = mean_squared_error(combined_y_p, pred_y_p)\n",
    "\n",
    "# Assuming 'importance' is a list or array of feature importances\n",
    "# Create a DataFrame with feature importance scores\n",
    "feature_importance_networks = pd.DataFrame({\n",
    "    'feature': combined_x_p.columns,  # Assuming feature names come from 'combined_x_p'\n",
    "    'importance': importance_p  # Assuming 'importance' is a list or array of the same length as the number of features\n",
    "})\n",
    "\n",
    "# Print the MSE score (separately from the feature importances)\n",
    "print(f\"Mean Squared Error (MSE): {mse_score}\")\n",
    "\n",
    "# Sort by 'importance' in descending order\n",
    "sorted_importance_p = feature_importance_networks.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Display sorted feature importances\n",
    "(sorted_importance_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance_barplot(sorted_importance, top_n=5):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar plot showing the top N feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    sorted_importance (pd.DataFrame): Sorted feature importance DataFrame.\n",
    "    top_n (int): Number of top features to plot.\n",
    "    \"\"\"\n",
    "    # Select the top N features based on importance\n",
    "    top_features = sorted_importance.head(top_n)\n",
    "\n",
    "    # Sort the top N features by importance for plotting\n",
    "    top_features = top_features.sort_values(by='importance', ascending=True)\n",
    "\n",
    "    # Plot the top N feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = top_features.plot(kind='barh', x='feature', y='importance', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Remove the y-label\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Remove the legend\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    # Add grid and title\n",
    "    plt.grid(True)\n",
    "    ax.set_title('Top N Feature Importance (Put Options)')\n",
    "    \n",
    "    # plt.tight_layout()  # Optional: for better layout\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot the top 5 features from Put options data\n",
    "plot_feature_importance_barplot(sorted_importance_p, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18772/1963729078.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_10_df['importance_normalized'] = top_10_df['importance'] / top_10_df['importance'].sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>importance_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.761074</td>\n",
       "      <td>0.600862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.079601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HIGH_vix</td>\n",
       "      <td>0.079193</td>\n",
       "      <td>0.062522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.065490</td>\n",
       "      <td>0.051704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.050018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.047051</td>\n",
       "      <td>0.037146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.035385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.039524</td>\n",
       "      <td>0.031204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.033716</td>\n",
       "      <td>0.026619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PRC_actual</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.024939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance  importance_normalized\n",
       "3         prev_day_iv    0.761074               0.600862\n",
       "18            2Y_bond    0.100826               0.079601\n",
       "21           HIGH_vix    0.079193               0.062522\n",
       "11  cumulative_return    0.065490               0.051704\n",
       "0                   T    0.063354               0.050018\n",
       "16            10Y_RIR    0.047051               0.037146\n",
       "2        prev2_day_iv    0.044820               0.035385\n",
       "23           OPEN_vix    0.039524               0.031204\n",
       "17            1Y_bond    0.033716               0.026619\n",
       "8          PRC_actual    0.031589               0.024939"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted_importance_p\n",
    "\n",
    "# Sort by importance\n",
    "df_sorted = sorted_importance_p.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 10 entries\n",
    "top_10_df = df_sorted.head(10)\n",
    "\n",
    "# Normalize the importance values to sum to 1 for the top 10 features\n",
    "top_10_df['importance_normalized'] = top_10_df['importance'] / top_10_df['importance'].sum()\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "top_10_df[['feature', 'importance', 'importance_normalized']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18772/3377667362.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_10_df['importance_normalized'] = top_10_df['importance'] / top_10_df['importance'].sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>importance_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.765249</td>\n",
       "      <td>0.485420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CLOSE_vix</td>\n",
       "      <td>0.287905</td>\n",
       "      <td>0.182627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.123037</td>\n",
       "      <td>0.078046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.094248</td>\n",
       "      <td>0.059784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LOW_vix</td>\n",
       "      <td>0.089782</td>\n",
       "      <td>0.056952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>0.076999</td>\n",
       "      <td>0.048843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.045990</td>\n",
       "      <td>0.029173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.043765</td>\n",
       "      <td>0.027762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>0.016455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.023552</td>\n",
       "      <td>0.014940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  importance  importance_normalized\n",
       "3         prev_day_iv    0.765249               0.485420\n",
       "19          CLOSE_vix    0.287905               0.182627\n",
       "18            2Y_bond    0.123037               0.078046\n",
       "17            1Y_bond    0.094248               0.059784\n",
       "22            LOW_vix    0.089782               0.056952\n",
       "0                   T    0.076999               0.048843\n",
       "11  cumulative_return    0.045990               0.029173\n",
       "2        prev2_day_iv    0.043765               0.027762\n",
       "16            10Y_RIR    0.025941               0.016455\n",
       "20            FF_rate    0.023552               0.014940"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by importance\n",
    "df_sorted = sorted_importance_c.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 10 entries\n",
    "top_10_df = df_sorted.head(10)\n",
    "\n",
    "# Normalize the importance values to sum to 1 for the top 10 features\n",
    "top_10_df['importance_normalized'] = top_10_df['importance'] / top_10_df['importance'].sum()\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "top_10_df[['feature', 'importance', 'importance_normalized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>importance_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.761074</td>\n",
       "      <td>0.600862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.100826</td>\n",
       "      <td>0.079601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HIGH_vix</td>\n",
       "      <td>0.079193</td>\n",
       "      <td>0.062522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.065490</td>\n",
       "      <td>0.051704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.050017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.047051</td>\n",
       "      <td>0.037146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.044820</td>\n",
       "      <td>0.035385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.039524</td>\n",
       "      <td>0.031204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.033716</td>\n",
       "      <td>0.026619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PRC_actual</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>0.024939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  importance  importance_normalized\n",
       "0        prev_day_iv    0.761074               0.600862\n",
       "1            2Y_bond    0.100826               0.079601\n",
       "2           HIGH_vix    0.079193               0.062522\n",
       "3  cumulative_return    0.065490               0.051704\n",
       "4                  T    0.063354               0.050017\n",
       "5            10Y_RIR    0.047051               0.037146\n",
       "6       prev2_day_iv    0.044820               0.035385\n",
       "7           OPEN_vix    0.039524               0.031204\n",
       "8            1Y_bond    0.033716               0.026619\n",
       "9         PRC_actual    0.031589               0.024939"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data based on your image\n",
    "data = {'feature': ['prev_day_iv', '2Y_bond', 'HIGH_vix', 'cumulative_return', 'T', \n",
    "                    '10Y_RIR', 'prev2_day_iv', 'OPEN_vix', '1Y_bond', 'PRC_actual'\n",
    "                    ],\n",
    "        'importance': [0.761074, 0.100826, 0.079193, 0.065490, 0.063354, 0.047051, 0.044820, \n",
    "                       0.039524, 0.033716, 0.031589]}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize the importance values to sum to 1\n",
    "df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Normalized Feature Importance\", dataframe=df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                     feature  importance\\n                 prev_day_iv    0.769679\\n                           T    0.077878\\n           cumulative_return    0.067786\\n                       BIDLO    0.050018\\n                prev2_day_iv    0.045465\\n                     10Y_RIR    0.043751\\n                         BID    0.041074\\n                  PRC_actual    0.040090\\n                     2Y_bond    0.034504\\n                     1Y_bond    0.031342\\n                    OPEN_vix    0.026933\\n                  gold_price    0.026808\\n                       ASKHI    0.025359\\n                   CLOSE_vix    0.025099\\n                     LOW_vix    0.022857\\n                  spread_vix    0.021125\\n                         ASK    0.019284\\n               volume_option    0.018687\\n                    HIGH_vix    0.015443\\n                        RETX    0.014120\\n                         PRC    0.013984\\n                         RET    0.013494\\n               spread_option    0.009703\\n                 hi-lo_stock    0.009206\\n                spread_stock    0.008067\\n  5_day_rolling_return_stock    0.007816\\n                  reces_indi    0.006974\\n                   vol_stock    0.004508\\n                   moneyness    0.003908\\n                     FF_rate    0.003355\\ndaily_return_indicator_stock    0.001834'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index and remove the old index by setting drop=True\n",
    "sorted_importance_p_cleaned = sorted_importance_p.reset_index(drop=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "\n",
    "# Print the DataFrame without showing the index\n",
    "sorted_importance_p= (sorted_importance_p.to_string(index=False))\n",
    "\n",
    "sorted_importance_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.936565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.497470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T</td>\n",
       "      <td>0.118749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.101188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.092912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIDLO</td>\n",
       "      <td>0.089504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.079932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.068652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.040914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.013355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature  importance\n",
       "0   prev_day_iv    0.936565\n",
       "7    gold_price    0.497470\n",
       "1             T    0.118749\n",
       "8    reces_indi    0.101188\n",
       "2  prev2_day_iv    0.092912\n",
       "3         BIDLO    0.089504\n",
       "4      OPEN_vix    0.079932\n",
       "6       FF_rate    0.068652\n",
       "5   hi-lo_stock    0.040914\n",
       "9    spread_vix    0.013355"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_importance_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_feature_importance_barplot(sorted_importance_c, sorted_importance_p, top_n=5):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar plot showing the top N normalized average feature importance from both Call and Put options.\n",
    "\n",
    "    Parameters:\n",
    "    sorted_importance_c (pd.DataFrame): Sorted feature importance DataFrame for Call options.\n",
    "    sorted_importance_p (pd.DataFrame): Sorted feature importance DataFrame for Put options.\n",
    "    top_n (int): Number of top features to plot.\n",
    "    \"\"\"\n",
    "    # Merge Call and Put DataFrames on feature name to get both importance values\n",
    "    combined_importance = pd.concat(\n",
    "        [sorted_importance_c.set_index('feature'), sorted_importance_p.set_index('feature')], \n",
    "        axis=1, keys=[\"importance_call\", \"importance_put\"]\n",
    "    ).fillna(0).reset_index()\n",
    "\n",
    "    # print(combined_importance)\n",
    "    \n",
    "    # Normalize the importance values to sum to 1 for both Call and Put\n",
    "    combined_importance['importance_call'] /= combined_importance['importance_call'].sum()\n",
    "    combined_importance['importance_put'] /= combined_importance['importance_put'].sum()\n",
    "\n",
    "    # Calculate the average normalized importance\n",
    "    combined_importance['average_importance'] = combined_importance[['importance_call', 'importance_put']].mean(axis=1)\n",
    "    \n",
    "    # Sort by the average importance and select the top N features\n",
    "    top_features = combined_importance.sort_values(by='average_importance', ascending=False).head(top_n)\n",
    "\n",
    "    # Plot the top N normalized average feature importance with a larger size\n",
    "    # plt.figure(figsize=((10, 6)))  # Adjust this as needed for the correct aspect ratio\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax = top_features.sort_values(by='average_importance', ascending=True).plot(\n",
    "        kind='barh', \n",
    "        x='feature', \n",
    "        y='average_importance', \n",
    "        color='blue', \n",
    "        alpha=0.55, \n",
    "        edgecolor='black', \n",
    "        linewidth=1,\n",
    "        ax=ax  # Use the ax from fig\n",
    "    )\n",
    "    \n",
    "    # Remove the y-label\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    # Remove the legend\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    # Add grid and title\n",
    "    plt.grid(True)\n",
    "    ax.set_title('NN (Put and Call)')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return combined_importance\n",
    "\n",
    "# Example usage: Plot the top 5 normalized features from combined Call and Put options data\n",
    "combfeat = plot_combined_feature_importance_barplot(sorted_importance_c, sorted_importance_p, top_n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_call</th>\n",
       "      <th>importance_put</th>\n",
       "      <th>average_importance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>importance</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prev_day_iv</td>\n",
       "      <td>0.426052</td>\n",
       "      <td>0.529666</td>\n",
       "      <td>0.477859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2Y_bond</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.091225</td>\n",
       "      <td>0.099592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1Y_bond</td>\n",
       "      <td>0.092431</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.055797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLOSE_vix</td>\n",
       "      <td>0.095909</td>\n",
       "      <td>0.010210</td>\n",
       "      <td>0.053060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T</td>\n",
       "      <td>0.051365</td>\n",
       "      <td>0.050650</td>\n",
       "      <td>0.051007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prev2_day_iv</td>\n",
       "      <td>0.036310</td>\n",
       "      <td>0.038351</td>\n",
       "      <td>0.037330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOW_vix</td>\n",
       "      <td>0.029910</td>\n",
       "      <td>0.028491</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cumulative_return</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.028776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10Y_RIR</td>\n",
       "      <td>0.020152</td>\n",
       "      <td>0.036195</td>\n",
       "      <td>0.028173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HIGH_vix</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>0.030462</td>\n",
       "      <td>0.021839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OPEN_vix</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>0.017747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PRC_actual</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.021932</td>\n",
       "      <td>0.017226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASK</td>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>0.015672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FF_rate</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.013787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>volume_option</td>\n",
       "      <td>0.012917</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.013053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gold_price</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>0.011399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spread_vix</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>0.008744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spread_option</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.006082</td>\n",
       "      <td>0.005793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spread_stock</td>\n",
       "      <td>0.005875</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.005731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>reces_indi</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.002313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>moneyness</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.001497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5_day_rolling_return_stock</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.001450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hi-lo_stock</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RET</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>daily_return_indicator_stock</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vol_stock</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature importance_call importance_put  \\\n",
       "                                      importance     importance   \n",
       "0                    prev_day_iv        0.426052       0.529666   \n",
       "1                        2Y_bond        0.107958       0.091225   \n",
       "3                        1Y_bond        0.092431       0.019163   \n",
       "2                      CLOSE_vix        0.095909       0.010210   \n",
       "4                              T        0.051365       0.050650   \n",
       "5                   prev2_day_iv        0.036310       0.038351   \n",
       "6                        LOW_vix        0.029910       0.028491   \n",
       "7              cumulative_return        0.021382       0.036170   \n",
       "8                        10Y_RIR        0.020152       0.036195   \n",
       "11                      HIGH_vix        0.013215       0.030462   \n",
       "14                      OPEN_vix        0.012403       0.023092   \n",
       "13                    PRC_actual        0.012519       0.021932   \n",
       "9                            ASK        0.018393       0.012951   \n",
       "10                       FF_rate        0.016286       0.011287   \n",
       "12                 volume_option        0.012917       0.013190   \n",
       "15                    gold_price        0.010127       0.012671   \n",
       "16                    spread_vix        0.006251       0.011237   \n",
       "18                 spread_option        0.005503       0.006082   \n",
       "17                  spread_stock        0.005875       0.005587   \n",
       "22                    reces_indi        0.000671       0.003954   \n",
       "24                     moneyness        0.000453       0.002540   \n",
       "20    5_day_rolling_return_stock        0.001258       0.001643   \n",
       "19                   hi-lo_stock        0.001356       0.001241   \n",
       "21                           RET        0.000886       0.001331   \n",
       "23  daily_return_indicator_stock        0.000490       0.000761   \n",
       "25                     vol_stock       -0.000072      -0.000082   \n",
       "\n",
       "   average_importance  \n",
       "                       \n",
       "0            0.477859  \n",
       "1            0.099592  \n",
       "3            0.055797  \n",
       "2            0.053060  \n",
       "4            0.051007  \n",
       "5            0.037330  \n",
       "6            0.029200  \n",
       "7            0.028776  \n",
       "8            0.028173  \n",
       "11           0.021839  \n",
       "14           0.017747  \n",
       "13           0.017226  \n",
       "9            0.015672  \n",
       "10           0.013787  \n",
       "12           0.013053  \n",
       "15           0.011399  \n",
       "16           0.008744  \n",
       "18           0.005793  \n",
       "17           0.005731  \n",
       "22           0.002313  \n",
       "24           0.001497  \n",
       "20           0.001450  \n",
       "19           0.001299  \n",
       "21           0.001108  \n",
       "23           0.000626  \n",
       "25          -0.000077  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by 'average_importance' in descending order\n",
    "sorted_combfeat = combfeat.sort_values(by=('average_importance', ''), ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_combfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301us/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m best_model_put\u001b[38;5;241m.\u001b[39mpredict(combined_x_p)\n\u001b[1;32m      7\u001b[0m feature_importance_networks \u001b[38;5;241m=\u001b[39m [mean_squared_error(combined_y_p, pred_y), importance]\n\u001b[0;32m----> 8\u001b[0m sorted_importance \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_importance_networks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#NEW\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "\n",
    "# # y_oos = best_model_put.predict(test_x_p)\n",
    "\n",
    "\n",
    "# # predictions = [mean_squared_error(combined_y_p, pred_y), y_oos]\n",
    "\n",
    "# pred_y = best_model_put.predict(combined_x_p)\n",
    "# feature_importance_networks = [mean_squared_error(combined_y_p, pred_y), importance]\n",
    "# sorted_importance = feature_importance_networks.sort_values(by='score', ascending=True) #NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyperparameter tuning with validation data...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.2765 - mse: 0.2765\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.4559 - mse: 0.4559\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.1565 - mse: 0.1565\n",
      "Epoch 2/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0992 - mse: 0.0992\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.1022 - mse: 0.1022\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0407 - mse: 0.0407\n",
      "Epoch 3/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463us/step - loss: 0.0734 - mse: 0.0734\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step - loss: 0.0744 - mse: 0.0744\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 4/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0609 - mse: 0.0609\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0234 - mse: 0.0234\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0665 - mse: 0.0665\n",
      "Epoch 5/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0556 - mse: 0.0556\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0201 - mse: 0.0201\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0612 - mse: 0.0612\n",
      "Epoch 6/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368us/step - loss: 0.0524 - mse: 0.0524\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0579 - mse: 0.0579\n",
      "Epoch 7/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0505 - mse: 0.0505\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0553 - mse: 0.0553\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 8/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0477 - mse: 0.0477\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0547 - mse: 0.0547\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 9/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0481 - mse: 0.0481\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0514 - mse: 0.0514\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0168 - mse: 0.0168\n",
      "Epoch 10/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0455 - mse: 0.0455\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0160 - mse: 0.0160\n",
      "Epoch 11/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0488 - mse: 0.0488\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0156 - mse: 0.0156\n",
      "Epoch 12/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0482 - mse: 0.0482\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0157 - mse: 0.0157\n",
      "Epoch 13/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0432 - mse: 0.0432\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0474 - mse: 0.0474\n",
      "Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - loss: 0.0155 - mse: 0.0155\n",
      "\u001b[1m 139/1521\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 0.0421 - mse: 0.0421Epoch 14/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0427 - mse: 0.0427\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 0.0471 - mse: 0.0471\n",
      "Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 0.0150 - mse: 0.0150\n",
      "\u001b[1m 144/1521\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 0.0417 - mse: 0.0417Epoch 15/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0467 - mse: 0.0467\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0147 - mse: 0.0147\n",
      "Epoch 16/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - loss: 0.0420 - mse: 0.0420\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 0.0462 - mse: 0.0462\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 17/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0419 - mse: 0.0419\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 18/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0411 - mse: 0.0411\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0433 - mse: 0.0433\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0145 - mse: 0.0145\n",
      "Epoch 19/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440us/step - loss: 0.0430 - mse: 0.0430\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444us/step - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442us/step - loss: 0.0146 - mse: 0.0146\n",
      "Epoch 20/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452us/step - loss: 0.0420 - mse: 0.0420\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451us/step - loss: 0.0440 - mse: 0.0440\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454us/step - loss: 0.0145 - mse: 0.0145\n",
      "Epoch 21/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 0.0439 - mse: 0.0439\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0143 - mse: 0.0143\n",
      "Epoch 22/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0408 - mse: 0.0408\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0142 - mse: 0.0142\n",
      "Epoch 23/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0429 - mse: 0.0429\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 24/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0408 - mse: 0.0408\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - loss: 0.0431 - mse: 0.0431\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 25/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0403 - mse: 0.0403\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0427 - mse: 0.0427\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0143 - mse: 0.0143\n",
      "Epoch 26/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0399 - mse: 0.0399\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0421 - mse: 0.0421\n",
      "Epoch 27/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0139 - mse: 0.0139\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0416 - mse: 0.0416\n",
      "Epoch 28/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406us/step - loss: 0.0381 - mse: 0.0381\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 0.0141 - mse: 0.0141\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 406us/step - loss: 0.0415 - mse: 0.0415\n",
      "Epoch 29/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0400 - mse: 0.0400\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0413 - mse: 0.0413\n",
      "Epoch 30/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 390us/step - loss: 0.0391 - mse: 0.0391\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 31/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0416 - mse: 0.0416\n",
      "Epoch 32/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - loss: 0.0391 - mse: 0.0391\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0134 - mse: 0.0134\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0406 - mse: 0.0406\n",
      "Epoch 33/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0392 - mse: 0.0392\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0134 - mse: 0.0134\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0399 - mse: 0.0399\n",
      "Epoch 34/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0387 - mse: 0.0387\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0410 - mse: 0.0410\n",
      "Epoch 35/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0370 - mse: 0.0370\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0137 - mse: 0.0137\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 36/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step - loss: 0.0381 - mse: 0.0381\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - loss: 0.0410 - mse: 0.0410\n",
      "Epoch 37/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453us/step - loss: 0.0378 - mse: 0.0378\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461us/step - loss: 0.0133 - mse: 0.0133\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 460us/step - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 38/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0397 - mse: 0.0397\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0132 - mse: 0.0132\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 39/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381us/step - loss: 0.0386 - mse: 0.0386\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0402 - mse: 0.0402\n",
      "Epoch 40/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0378 - mse: 0.0378\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 412us/step - loss: 0.0133 - mse: 0.0133\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 411us/step - loss: 0.0400 - mse: 0.0400\n",
      "Epoch 41/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0397 - mse: 0.0397\n",
      "\u001b[1m1064/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.0393 - mse: 0.0393Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0132 - mse: 0.0132\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 42/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0372 - mse: 0.0372\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0138 - mse: 0.0138\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0396 - mse: 0.0396\n",
      "Epoch 43/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0384 - mse: 0.0384\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0391 - mse: 0.0391\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0134 - mse: 0.0134\n",
      "Epoch 44/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368us/step - loss: 0.0376 - mse: 0.0376\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 0.0401 - mse: 0.0401\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0134 - mse: 0.0134\n",
      "Epoch 45/50\n",
      "Epoch 45/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 0.0388 - mse: 0.0388\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - loss: 0.0407 - mse: 0.0407\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 0.0131 - mse: 0.0131\n",
      "Epoch 46/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0368 - mse: 0.0368\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - loss: 0.0130 - mse: 0.0130\n",
      "Epoch 47/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0368 - mse: 0.0368\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 0.0405 - mse: 0.0405\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 387us/step - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 48/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396us/step - loss: 0.0377 - mse: 0.0377\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - loss: 0.0134 - mse: 0.0134\n",
      "Epoch 49/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - loss: 0.0376 - mse: 0.0376\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - loss: 0.0382 - mse: 0.0382\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - loss: 0.0129 - mse: 0.0129\n",
      "Epoch 50/50\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399us/step - loss: 0.0366 - mse: 0.0366\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0378 - mse: 0.0378\n",
      "\u001b[1m1521/1521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0131 - mse: 0.0131\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263us/step\n",
      "\u001b[1m413/761\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 243us/step[CV 2/3] END batch_size=32, epochs=50, model__dropout_rate=0.2, model__layers=2, model__neurons=32;, score=-0.023 total time=  30.8s\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253us/step\n",
      "\u001b[1m761/761\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step\n",
      "[CV 1/3] END batch_size=32, epochs=50, model__dropout_rate=0.2, model__layers=2, model__neurons=32;, score=-0.018 total time=  31.0s\n",
      "[CV 3/3] END batch_size=32, epochs=50, model__dropout_rate=0.2, model__layers=2, model__neurons=32;, score=-0.117 total time=  31.0s\n",
      "Epoch 1/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.2190 - mse: 0.2190\n",
      "Epoch 2/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328us/step - loss: 0.0652 - mse: 0.0652\n",
      "Epoch 3/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309us/step - loss: 0.0498 - mse: 0.0498\n",
      "Epoch 4/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0447 - mse: 0.0447\n",
      "Epoch 5/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0426 - mse: 0.0426\n",
      "Epoch 6/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0416 - mse: 0.0416\n",
      "Epoch 7/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379us/step - loss: 0.0398 - mse: 0.0398\n",
      "Epoch 8/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 0.0392 - mse: 0.0392\n",
      "Epoch 9/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337us/step - loss: 0.0398 - mse: 0.0398\n",
      "Epoch 10/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551us/step - loss: 0.0389 - mse: 0.0389\n",
      "Epoch 11/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312us/step - loss: 0.0377 - mse: 0.0377\n",
      "Epoch 12/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320us/step - loss: 0.0365 - mse: 0.0365\n",
      "Epoch 13/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0357 - mse: 0.0357\n",
      "Epoch 14/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0368 - mse: 0.0368\n",
      "Epoch 15/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0358 - mse: 0.0358\n",
      "Epoch 16/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - loss: 0.0357 - mse: 0.0357\n",
      "Epoch 17/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327us/step - loss: 0.0351 - mse: 0.0351\n",
      "Epoch 18/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step - loss: 0.0346 - mse: 0.0346\n",
      "Epoch 19/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - loss: 0.0348 - mse: 0.0348\n",
      "Epoch 20/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0343 - mse: 0.0343\n",
      "Epoch 21/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0340 - mse: 0.0340\n",
      "Epoch 22/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0341 - mse: 0.0341\n",
      "Epoch 23/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343us/step - loss: 0.0330 - mse: 0.0330\n",
      "Epoch 24/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294us/step - loss: 0.0340 - mse: 0.0340\n",
      "Epoch 25/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353us/step - loss: 0.0335 - mse: 0.0335\n",
      "Epoch 26/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 324us/step - loss: 0.0329 - mse: 0.0329\n",
      "Epoch 27/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315us/step - loss: 0.0325 - mse: 0.0325\n",
      "Epoch 28/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - loss: 0.0316 - mse: 0.0316\n",
      "Epoch 29/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326us/step - loss: 0.0329 - mse: 0.0329\n",
      "Epoch 30/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0323 - mse: 0.0323\n",
      "Epoch 31/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step - loss: 0.0320 - mse: 0.0320\n",
      "Epoch 32/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 305us/step - loss: 0.0321 - mse: 0.0321\n",
      "Epoch 33/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0317 - mse: 0.0317\n",
      "Epoch 34/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 425us/step - loss: 0.0320 - mse: 0.0320\n",
      "Epoch 35/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322us/step - loss: 0.0317 - mse: 0.0317\n",
      "Epoch 36/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0317 - mse: 0.0317\n",
      "Epoch 37/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 0.0316 - mse: 0.0316\n",
      "Epoch 38/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0317 - mse: 0.0317\n",
      "Epoch 39/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 365us/step - loss: 0.0314 - mse: 0.0314\n",
      "Epoch 40/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322us/step - loss: 0.0306 - mse: 0.0306\n",
      "Epoch 41/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - loss: 0.0310 - mse: 0.0310\n",
      "Epoch 42/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0314 - mse: 0.0314\n",
      "Epoch 43/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 334us/step - loss: 0.0313 - mse: 0.0313\n",
      "Epoch 44/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 328us/step - loss: 0.0307 - mse: 0.0307\n",
      "Epoch 45/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303us/step - loss: 0.0311 - mse: 0.0311\n",
      "Epoch 46/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0305 - mse: 0.0305\n",
      "Epoch 47/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0313 - mse: 0.0313\n",
      "Epoch 48/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - loss: 0.0302 - mse: 0.0302\n",
      "Epoch 49/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - loss: 0.0308 - mse: 0.0308\n",
      "Epoch 50/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0308 - mse: 0.0308\n",
      "Best Parameters: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.2, 'model__layers': 2, 'model__neurons': 32}\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step\n",
      "In-sample R²: 0.8239\n",
      "In-sample RMSE: 0.1653\n",
      "\u001b[1m1105/1105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step\n",
      "Out-of-sample R²: 0.6752\n",
      "Out-of-sample RMSE: 0.1406\n"
     ]
    }
   ],
   "source": [
    "best_model_put = train_and_evaluate(train_x_p, train_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess my data \n",
    "# # First standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train_x_c)\n",
    "# train_x_c = scaler.transform(train_x_c)\n",
    "# test_x_c = scaler.transform(test_x_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266us/step - loss: 0.2191 - mse: 0.2191\n",
      "Epoch 2/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 3/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0672 - mse: 0.0672\n",
      "Epoch 4/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 5/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 6/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0681 - mse: 0.0681\n",
      "Epoch 7/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 8/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0689 - mse: 0.0689\n",
      "Epoch 9/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0665 - mse: 0.0665\n",
      "Epoch 10/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 11/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0689 - mse: 0.0689\n",
      "Epoch 12/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0693 - mse: 0.0693\n",
      "Epoch 13/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0696 - mse: 0.0696\n",
      "Epoch 14/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - loss: 0.0671 - mse: 0.0671\n",
      "Epoch 15/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0686 - mse: 0.0686\n",
      "Epoch 16/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step - loss: 0.0689 - mse: 0.0689\n",
      "Epoch 17/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0668 - mse: 0.0668\n",
      "Epoch 18/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step - loss: 0.0683 - mse: 0.0683\n",
      "Epoch 19/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0686 - mse: 0.0686\n",
      "Epoch 20/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0677 - mse: 0.0677\n",
      "Epoch 21/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0692 - mse: 0.0692\n",
      "Epoch 22/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 298us/step - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 23/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0698 - mse: 0.0698\n",
      "Epoch 24/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0686 - mse: 0.0686\n",
      "Epoch 25/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 26/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0689 - mse: 0.0689\n",
      "Epoch 27/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0677 - mse: 0.0677\n",
      "Epoch 28/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0691 - mse: 0.0691\n",
      "Epoch 29/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296us/step - loss: 0.0688 - mse: 0.0688\n",
      "Epoch 30/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0684 - mse: 0.0684\n",
      "Epoch 31/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0677 - mse: 0.0677\n",
      "Epoch 32/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 33/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0677 - mse: 0.0677\n",
      "Epoch 34/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 35/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0682 - mse: 0.0682\n",
      "Epoch 36/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289us/step - loss: 0.0697 - mse: 0.0697\n",
      "Epoch 37/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step - loss: 0.0675 - mse: 0.0675\n",
      "Epoch 38/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0692 - mse: 0.0692\n",
      "Epoch 39/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 40/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0684 - mse: 0.0684\n",
      "Epoch 41/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 274us/step - loss: 0.0682 - mse: 0.0682\n",
      "Epoch 42/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287us/step - loss: 0.0678 - mse: 0.0678\n",
      "Epoch 43/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0687 - mse: 0.0687\n",
      "Epoch 44/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 45/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0695 - mse: 0.0695\n",
      "Epoch 46/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0680 - mse: 0.0680\n",
      "Epoch 47/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 48/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0679 - mse: 0.0679\n",
      "Epoch 49/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 288us/step - loss: 0.0690 - mse: 0.0690\n",
      "Epoch 50/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0685 - mse: 0.0685\n",
      "\u001b[1m1105/1105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212us/step\n",
      "R²: 0.5814\n",
      "RMSE: 0.1596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Best Parameters_c: {'batch_size': 32, 'epochs': 50, 'model_dropout_rate': 0, 'modellayers': 1, 'model_neurons': 8}\n",
    "\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons, layers, dropout_rate):\n",
    "    model = Sequential()\n",
    "    # Input layer using Input instead of input_dim argument\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model_p = create_model(combined_x_p.shape[1], 8, 1, 0.5)\n",
    "\n",
    "# Retrain the model on the combined training and validation set\n",
    "model_p.fit(combined_x_p, combined_y_p, batch_size=32, epochs=50, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_p = model_p.predict(test_x_p)\n",
    "\n",
    "# Calculate R² and RMSE for the best model\n",
    "r2_c = r2_score(test_y_p, predictions_p)\n",
    "rmse_c = np.sqrt(mean_squared_error(test_y_p, predictions_p))\n",
    "\n",
    "# Print the results\n",
    "print(f\"R²: {r2_c:.4f}\")\n",
    "print(f\"RMSE: {rmse_c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Call Options Model:\n",
      "Epoch 1/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368us/step - loss: 0.0913 - mse: 0.0913\n",
      "Epoch 2/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - loss: 0.0508 - mse: 0.0508\n",
      "Epoch 3/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318us/step - loss: 0.0479 - mse: 0.0479\n",
      "Epoch 4/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 312us/step - loss: 0.0478 - mse: 0.0478\n",
      "Epoch 5/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0486 - mse: 0.0486\n",
      "Epoch 6/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309us/step - loss: 0.0481 - mse: 0.0481\n",
      "Epoch 7/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0470 - mse: 0.0470\n",
      "Epoch 8/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315us/step - loss: 0.0463 - mse: 0.0463\n",
      "Epoch 9/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314us/step - loss: 0.0468 - mse: 0.0468\n",
      "Epoch 10/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 11/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 386us/step - loss: 0.0467 - mse: 0.0467\n",
      "Epoch 12/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 13/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326us/step - loss: 0.0464 - mse: 0.0464\n",
      "Epoch 14/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392us/step - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 15/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389us/step - loss: 0.0460 - mse: 0.0460\n",
      "Epoch 16/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 344us/step - loss: 0.0461 - mse: 0.0461\n",
      "Epoch 17/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 18/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 341us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 19/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 20/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 335us/step - loss: 0.0443 - mse: 0.0443\n",
      "Epoch 21/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 324us/step - loss: 0.0457 - mse: 0.0457\n",
      "Epoch 22/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 23/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 24/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 347us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 25/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352us/step - loss: 0.0453 - mse: 0.0453\n",
      "Epoch 26/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327us/step - loss: 0.0445 - mse: 0.0445\n",
      "Epoch 27/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 322us/step - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 28/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0452 - mse: 0.0452\n",
      "Epoch 29/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 339us/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 30/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 31/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 415us/step - loss: 0.0464 - mse: 0.0464\n",
      "Epoch 32/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 0.0462 - mse: 0.0462\n",
      "Epoch 33/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327us/step - loss: 0.0472 - mse: 0.0472\n",
      "Epoch 34/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 351us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 35/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 327us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 36/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311us/step - loss: 0.0454 - mse: 0.0454\n",
      "Epoch 37/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 38/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314us/step - loss: 0.0462 - mse: 0.0462\n",
      "Epoch 39/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 40/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 350us/step - loss: 0.0461 - mse: 0.0461\n",
      "Epoch 41/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320us/step - loss: 0.0460 - mse: 0.0460\n",
      "Epoch 42/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317us/step - loss: 0.0447 - mse: 0.0447\n",
      "Epoch 43/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - loss: 0.0450 - mse: 0.0450\n",
      "Epoch 44/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330us/step - loss: 0.0462 - mse: 0.0462\n",
      "Epoch 45/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316us/step - loss: 0.0463 - mse: 0.0463\n",
      "Epoch 46/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 320us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 47/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325us/step - loss: 0.0458 - mse: 0.0458\n",
      "Epoch 48/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 420us/step - loss: 0.0464 - mse: 0.0464\n",
      "Epoch 49/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 317us/step - loss: 0.0459 - mse: 0.0459\n",
      "Epoch 50/50\n",
      "\u001b[1m2261/2261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323us/step - loss: 0.0466 - mse: 0.0466\n",
      "\u001b[1m1136/1136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285us/step\n",
      "R²: -7.3745\n",
      "RMSE: 0.6904\n",
      "Evaluating Put Options Model:\n",
      "Epoch 1/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302us/step - loss: 0.1217 - mse: 0.1217\n",
      "Epoch 2/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 900us/step - loss: 0.0659 - mse: 0.0659\n",
      "Epoch 3/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 307us/step - loss: 0.0654 - mse: 0.0654\n",
      "Epoch 4/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 359us/step - loss: 0.0636 - mse: 0.0636\n",
      "Epoch 5/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step - loss: 0.0655 - mse: 0.0655\n",
      "Epoch 6/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0648 - mse: 0.0648\n",
      "Epoch 7/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 8/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0669 - mse: 0.0669\n",
      "Epoch 9/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285us/step - loss: 0.0644 - mse: 0.0644\n",
      "Epoch 10/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271us/step - loss: 0.0657 - mse: 0.0657\n",
      "Epoch 11/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 12/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 13/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - loss: 0.0660 - mse: 0.0660\n",
      "Epoch 14/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297us/step - loss: 0.0649 - mse: 0.0649\n",
      "Epoch 15/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - loss: 0.0638 - mse: 0.0638\n",
      "Epoch 16/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294us/step - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 17/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0654 - mse: 0.0654\n",
      "Epoch 18/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0645 - mse: 0.0645\n",
      "Epoch 19/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0645 - mse: 0.0645\n",
      "Epoch 20/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300us/step - loss: 0.0651 - mse: 0.0651\n",
      "Epoch 21/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 261us/step - loss: 0.0651 - mse: 0.0651\n",
      "Epoch 22/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0657 - mse: 0.0657\n",
      "Epoch 23/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 24/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 265us/step - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 25/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0646 - mse: 0.0646\n",
      "Epoch 26/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290us/step - loss: 0.0656 - mse: 0.0656\n",
      "Epoch 27/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0654 - mse: 0.0654\n",
      "Epoch 28/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 29/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0632 - mse: 0.0632\n",
      "Epoch 30/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0652 - mse: 0.0652\n",
      "Epoch 31/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267us/step - loss: 0.0642 - mse: 0.0642\n",
      "Epoch 32/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0654 - mse: 0.0654\n",
      "Epoch 33/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 34/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 35/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 36/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0658 - mse: 0.0658\n",
      "Epoch 37/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299us/step - loss: 0.0664 - mse: 0.0664\n",
      "Epoch 38/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 260us/step - loss: 0.0670 - mse: 0.0670\n",
      "Epoch 39/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259us/step - loss: 0.0641 - mse: 0.0641\n",
      "Epoch 40/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step - loss: 0.0648 - mse: 0.0648\n",
      "Epoch 41/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0658 - mse: 0.0658\n",
      "Epoch 42/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0656 - mse: 0.0656\n",
      "Epoch 43/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302us/step - loss: 0.0668 - mse: 0.0668\n",
      "Epoch 44/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255us/step - loss: 0.0650 - mse: 0.0650\n",
      "Epoch 45/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0653 - mse: 0.0653\n",
      "Epoch 46/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0659 - mse: 0.0659\n",
      "Epoch 47/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0661 - mse: 0.0661\n",
      "Epoch 48/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297us/step - loss: 0.0660 - mse: 0.0660\n",
      "Epoch 49/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 50/50\n",
      "\u001b[1m2281/2281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0672 - mse: 0.0672\n",
      "\u001b[1m1105/1105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n",
      "R²: 0.1916\n",
      "RMSE: 0.2218\n"
     ]
    }
   ],
   "source": [
    "import numpy as nq\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons, layers, dropout_rate):\n",
    "    model = Sequential()\n",
    "    # Input layer using Input instead of input_dim argument\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, combined_x, combined_y, test_x, test_y, batch_size, epochs):\n",
    "    # Retrain the model on the combined training and validation set\n",
    "    model.fit(combined_x, combined_y, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(test_x)\n",
    "\n",
    "    # Calculate R² and RMSE for the best model\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"R²: {r2:.,f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Example usage for call options\n",
    "# print(\"Evaluating Call Options Model:\")\n",
    "# model_c = create_model(combined_x_c.shape[1], neurons=8, layers=3, dropout_rate=0.2)\n",
    "# evaluate_model(model_c, combined_x_c, combined_y_c, test_x_c, test_y_c, batch_size=32, epochs=50)\n",
    "\n",
    "# Example usage for put options\n",
    "print(\"Evaluating Put Options Model:\")\n",
    "model_p = create_model(combined_x_p.shape[1], neurons=8, layers=1, dropout_rate=0.5)\n",
    "evaluate_model(model_p, combined_x_p, combined_y_p, test_x_p, test_y_p, batch_size=32, epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>5_day_rolling_return_stock</th>\n",
       "      <th>ASK</th>\n",
       "      <th>ASKHI</th>\n",
       "      <th>BID</th>\n",
       "      <th>BIDLO</th>\n",
       "      <th>PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>2Y_bond</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>FF_rate</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.862489</td>\n",
       "      <td>-1.599060</td>\n",
       "      <td>-1.565542</td>\n",
       "      <td>-0.215265</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.56898</td>\n",
       "      <td>-0.56926</td>\n",
       "      <td>-0.56901</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>-0.568991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.12461</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.862489</td>\n",
       "      <td>-1.348787</td>\n",
       "      <td>-1.565542</td>\n",
       "      <td>-0.243882</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.56898</td>\n",
       "      <td>-0.56926</td>\n",
       "      <td>-0.56901</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>-0.568991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.12461</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.862489</td>\n",
       "      <td>-1.099920</td>\n",
       "      <td>-1.565542</td>\n",
       "      <td>-0.272954</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.56898</td>\n",
       "      <td>-0.56926</td>\n",
       "      <td>-0.56901</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>-0.568991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.12461</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.862489</td>\n",
       "      <td>-0.853865</td>\n",
       "      <td>-1.565542</td>\n",
       "      <td>-0.301999</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.56898</td>\n",
       "      <td>-0.56926</td>\n",
       "      <td>-0.56901</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>-0.568991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.12461</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.862489</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-1.565542</td>\n",
       "      <td>-0.349271</td>\n",
       "      <td>-0.286276</td>\n",
       "      <td>-0.56898</td>\n",
       "      <td>-0.56926</td>\n",
       "      <td>-0.56901</td>\n",
       "      <td>-0.567166</td>\n",
       "      <td>-0.568991</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371916</td>\n",
       "      <td>1.469739</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>1.12461</td>\n",
       "      <td>0.475155</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674867</td>\n",
       "      <td>-1.353864</td>\n",
       "      <td>-0.709827</td>\n",
       "      <td>-0.133918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          T  moneyness  prev2_day_iv  prev_day_iv  5_day_rolling_return_stock  \\\n",
       "0 -0.862489  -1.599060     -1.565542    -0.215265                   -0.286276   \n",
       "1 -0.862489  -1.348787     -1.565542    -0.243882                   -0.286276   \n",
       "2 -0.862489  -1.099920     -1.565542    -0.272954                   -0.286276   \n",
       "3 -0.862489  -0.853865     -1.565542    -0.301999                   -0.286276   \n",
       "4 -0.862489  -0.242243     -1.565542    -0.349271                   -0.286276   \n",
       "\n",
       "       ASK    ASKHI      BID     BIDLO       PRC  ...   1Y_bond   2Y_bond  \\\n",
       "0 -0.56898 -0.56926 -0.56901 -0.567166 -0.568991  ...  1.371916  1.469739   \n",
       "1 -0.56898 -0.56926 -0.56901 -0.567166 -0.568991  ...  1.371916  1.469739   \n",
       "2 -0.56898 -0.56926 -0.56901 -0.567166 -0.568991  ...  1.371916  1.469739   \n",
       "3 -0.56898 -0.56926 -0.56901 -0.567166 -0.568991  ...  1.371916  1.469739   \n",
       "4 -0.56898 -0.56926 -0.56901 -0.567166 -0.568991  ...  1.371916  1.469739   \n",
       "\n",
       "   CLOSE_vix  FF_rate  HIGH_vix   LOW_vix  OPEN_vix  gold_price  reces_indi  \\\n",
       "0   0.476238  1.12461  0.475155  0.662651  0.674867   -1.353864   -0.709827   \n",
       "1   0.476238  1.12461  0.475155  0.662651  0.674867   -1.353864   -0.709827   \n",
       "2   0.476238  1.12461  0.475155  0.662651  0.674867   -1.353864   -0.709827   \n",
       "3   0.476238  1.12461  0.475155  0.662651  0.674867   -1.353864   -0.709827   \n",
       "4   0.476238  1.12461  0.475155  0.662651  0.674867   -1.353864   -0.709827   \n",
       "\n",
       "   spread_vix  \n",
       "0   -0.133918  \n",
       "1   -0.133918  \n",
       "2   -0.133918  \n",
       "3   -0.133918  \n",
       "4   -0.133918  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_x_p.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6547025 ],\n",
       "       [-0.6540263 ],\n",
       "       [-0.64889556],\n",
       "       ...,\n",
       "       [ 0.829493  ],\n",
       "       [ 0.90898424],\n",
       "       [ 0.98446095]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from tensorflow import keras\n",
    "# from keras.layers import Dense\n",
    "# from keras.models import Sequential, load_model\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # define the model\n",
    "# def create_model(bias, activity):\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(units=32,\n",
    "#                   activation='relu',\n",
    "#                   input_dim= train_x.shape[1],\n",
    "#                   # # kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "#                   bias_regularizer= regularizers.L2(bias),\n",
    "#                   activity_regularizer= regularizers.L2(activity)\n",
    "#                   ))\n",
    "#   model.add(Dense(units=32, activation= 'relu',\n",
    "#                   bias_regularizer=regularizers.L2(bias),\n",
    "#                   activity_regularizer=regularizers.L2(activity) #activation\n",
    "#                   ))\n",
    "#   model.add(Dense(units=1, activation= 'linear')) #miss linear\n",
    "\n",
    "#   # compile the model\n",
    "#   model.compile(optimizer= RMSprop(learning_rate=0.01), #'rmsprop',#Optimizer_trial,\n",
    "#                 loss= 'mean_squared_error',\n",
    "#                 metrics=['mse'])\n",
    "#   return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(neurons=32, layers=1, dropout_rate=0.0, bias=0.01, activity=0.01):\n",
    "#     model = Sequential()\n",
    "#     # Input layer (first hidden layer)\n",
    "#     model.add(Dense(neurons, activation='relu', input_dim=train_x.shape[1],\n",
    "#                     bias_regularizer=regularizers.L2(bias),\n",
    "#                     activity_regularizer=regularizers.L2(activity)))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu',\n",
    "#                         bias_regularizer=regularizers.L2(bias),\n",
    "#                         activity_regularizer=regularizers.L2(activity)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "# grid_search = GridSearchCV(estimator=model,\n",
    "#                            param_grid=param_grid,\n",
    "#                            scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "#                            cv=3,  # 3-fold cross-validation\n",
    "#                            verbose=1)  # Verbose for tracking progress\n",
    "\n",
    "# # Run the grid search\n",
    "# grid_search.fit(train_x, train_y)\n",
    "\n",
    "# # Get the best estimator and parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Make predictions using the best model\n",
    "# predictions = best_model.predict(test_x)\n",
    "\n",
    "# # Calculate R² and RMSE for the best model\n",
    "# r2 = r2_score(test_y, predictions)\n",
    "# rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"R²: {r2:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "# import time\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from keras_tuner import KerasRegressor\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# # from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# # from keras_tuner import KerasRegressor\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# def create_model(hidden_layers=1, units=64, activation='relu', learning_rate=0.001):\n",
    "#     \"\"\"\n",
    "#     Function to create a Keras Sequential model with the given hyperparameters.\n",
    "#     \"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=units, activation=activation, input_dim=X_train.shape[1]))\n",
    "    \n",
    "#     for _ in range(hidden_layers - 1):\n",
    "#         model.add(Dense(units=units, activation=activation))\n",
    "        \n",
    "#     model.add(Dense(1))  # Output layer\n",
    "\n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def prepare_data_with_gridsearch_nn(train_data, validate_data, test_data, option_type, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, perform hyperparameter tuning using Year 1 (train) and Year 2 (validation),\n",
    "#     retrain the model on Year 1 + Year 2, and evaluate on Year 3 (test) for Neural Networks.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset (Year 1).\n",
    "#     validate_data (pd.DataFrame): The validation dataset (Year 2).\n",
    "#     test_data (pd.DataFrame): The testing dataset (Year 3).\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     verbose (bool): If True, prints progress information for hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train, validation, and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features (Year 1)\n",
    "#     y_train = train_data['impl_volatility']  # Training target (Year 1)\n",
    "\n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Validation features (Year 2)\n",
    "#     y_validate = validate_data['impl_volatility']  # Validation target (Year 2)\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features (Year 3)\n",
    "#     y_test = test_data['impl_volatility']  # Test target (Year 3)\n",
    "\n",
    "#     # Define the hyperparameter grid for NN\n",
    "#     param_grid = {\n",
    "#         'hidden_layers': [1, 2, 3],                # Number of hidden layers\n",
    "#         'units': [32, 64, 128],                    # Number of units per layer\n",
    "#         'activation': ['relu', 'tanh'],            # Activation function\n",
    "#         'learning_rate': [0.001, 0.01],            # Learning rate for Adam optimizer\n",
    "#         'batch_size': [32, 64],                    # Batch size\n",
    "#         'epochs': [50, 100],                       # Number of epochs\n",
    "#     }\n",
    "\n",
    "#     # Generate all combinations of hyperparameters\n",
    "#     param_combinations = list(itertools.product(\n",
    "#         param_grid['hidden_layers'], \n",
    "#         param_grid['units'], \n",
    "#         param_grid['activation'], \n",
    "#         param_grid['learning_rate'], \n",
    "#         param_grid['batch_size'], \n",
    "#         param_grid['epochs']\n",
    "#     ))\n",
    "\n",
    "#     total_combinations = len(param_combinations)\n",
    "    \n",
    "#     # Initialize variables to store the best model and best score\n",
    "#     best_rmse_val = np.inf\n",
    "#     best_params = None\n",
    "#     best_nn_model = None\n",
    "\n",
    "#     print(f\"Running manual hyperparameter tuning for {option_type} Options with Neural Networks...\")\n",
    "    \n",
    "#     # Iterate over all hyperparameter combinations with progress tracking\n",
    "#     for i, (hidden_layers, units, activation, learning_rate, batch_size, epochs) in enumerate(param_combinations):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Create the Keras model with the current set of hyperparameters\n",
    "#         model = create_model(hidden_layers=hidden_layers, units=units, activation=activation, learning_rate=learning_rate)\n",
    "\n",
    "#         # Early stopping to prevent overfitting\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "#         # Train the model on the training data (Year 1)\n",
    "#         history = model.fit(X_train, y_train, \n",
    "#                             validation_data=(X_validate, y_validate),\n",
    "#                             batch_size=batch_size, \n",
    "#                             epochs=epochs, \n",
    "#                             verbose=0, \n",
    "#                             callbacks=[early_stopping])\n",
    "\n",
    "#         # Validate the model on the validation data (Year 2)\n",
    "#         y_val_pred = model.predict(X_validate)\n",
    "#         rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "\n",
    "#         # Track the best performing hyperparameters based on validation RMSE\n",
    "#         if rmse_val < best_rmse_val:\n",
    "#             best_rmse_val = rmse_val\n",
    "#             best_params = {\n",
    "#                 'hidden_layers': hidden_layers,\n",
    "#                 'units': units,\n",
    "#                 'activation': activation,\n",
    "#                 'learning_rate': learning_rate,\n",
    "#                 'batch_size': batch_size,\n",
    "#                 'epochs': epochs\n",
    "#             }\n",
    "#             best_nn_model = model\n",
    "\n",
    "#         # Verbose output to track progress\n",
    "#         if verbose:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\"Combination {i + 1}/{total_combinations} completed in {elapsed_time:.2f} seconds.\")\n",
    "#             print(f\"Current RMSE (Validation): {rmse_val:.4f}\")\n",
    "#             print(f\"Best RMSE so far: {best_rmse_val:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nBest Parameters for {option_type} Options with Neural Networks: {best_params}\")\n",
    "    \n",
    "#     # Retrain the model on combined Year 1 (train) and Year 2 (validation)\n",
    "#     print(\"Retraining the model on Year 1 and Year 2 combined...\")\n",
    "#     X_combined = pd.concat([X_train, X_validate])\n",
    "#     y_combined = pd.concat([y_train, y_validate])\n",
    "#     best_nn_model.fit(X_combined, y_combined, batch_size=best_params['batch_size'], epochs=best_params['epochs'], verbose=0)\n",
    "\n",
    "#     # In-sample (combined Year 1 + Year 2) predictions\n",
    "#     y_combined_pred = best_nn_model.predict(X_combined)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on combined Year 1 + Year 2)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(y_combined, y_combined_pred))\n",
    "#     r2_combined = r2_score(y_combined, y_combined_pred)\n",
    "    \n",
    "#     print(f\"\\nIn-Sample Performance for {option_type} Options (Year 1 + Year 2):\")\n",
    "#     print(f\"RMSE (Training + Validation): {rmse_combined:.4f}\")\n",
    "#     print(f\"R² (Training + Validation): {r2_combined:.4f}\")\n",
    "\n",
    "#     # After retraining, evaluate performance on the test data (Year 3)\n",
    "#     y_test_pred = best_nn_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the final results\n",
    "#     print(f\"\\nPerformance on Test Data (Year 3) for {option_type} Options:\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch_nn(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch_nn(data_train_p, data_validate_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
