{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load in the specific data needed\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import datetime\n",
    "import hvplot.polars\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.interpolate import bisplrep, bisplev\n",
    "from datetime import timedelta\n",
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_scaled_p.parquet')\n",
    "data_train_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_scaled_c.parquet')\n",
    "\n",
    "data_val_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_val_tech_scaled_p.parquet')\n",
    "data_val_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_val_tech_scaled_c.parquet')\n",
    "\n",
    "data_test_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_p.parquet')\n",
    "data_test_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_scaled_c.parquet')\n",
    "\n",
    "tot_data_train_c = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_val_tech_scaled_c.parquet')\n",
    "tot_data_train_p = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_val_tech_scaled_p.parquet')\n",
    "\n",
    "firm_data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Firm_variables/daily_firm_data_median_new.parquet')\n",
    "\n",
    "data_train = pd.concat([data_train_p, data_train_c], axis=0)\n",
    "data_val = pd.concat([data_val_p, data_val_c], axis=0)\n",
    "data_test = pd.concat([data_test_p, data_test_c], axis=0)\n",
    "data_train_tot = pd.concat([tot_data_train_c, tot_data_train_p], axis=0)\n",
    "\n",
    "# # List of columns to drop\n",
    "# columns_to_drop = ['trading_days_till_exp'] + list(firm_data.columns[2:])\n",
    "# # columns_to_drop = ['trading_days_till_exp']\n",
    "\n",
    "# # Drop columns from datasets if they exist\n",
    "# data_train = data_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "# data_val = data_val.drop(columns=columns_to_drop, errors='ignore')\n",
    "# data_test = data_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "# data_train_tot = data_train_tot.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure that data_val and data_test have the same column order as data_train\n",
    "data_val = data_val[data_train.columns]\n",
    "data_test = data_test[data_train.columns]\n",
    "tot_data_train = data_train_tot[data_train.columns]\n",
    "\n",
    "data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features for Call and Put options\n",
    "top_features_c = ['cp_flag', 'Ticker', 'date', 'moneyness', 'impl_volatility', \n",
    "                  'prev_day_iv', 'idiovol', 'T', \n",
    "                  'prev2_day_iv', 'moneyness', '1Y_bond', 'vol_stock']\n",
    "\n",
    "top_features_p = ['cp_flag', 'Ticker', 'date', 'moneyness', 'impl_volatility', \n",
    "                  'prev_day_iv', 'T', 'prev2_day_iv', \n",
    "                  'idiovol', 'moneyness', 'vol_stock']\n",
    "\n",
    "# Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C'][top_features_c]\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# Prepare validation data for Call and Put options\n",
    "data_validate_c = data_val[data_val['cp_flag'] == 'C'][top_features_c]\n",
    "data_validate_p = data_val[data_val['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# Prepare the total train data for Call and Put options\n",
    "data_train_tot_c = tot_data_train[tot_data_train['cp_flag'] == 'C'][top_features_c]\n",
    "data_train_tot_p = tot_data_train[tot_data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C'][top_features_c]\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P'][top_features_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, validation, and test data for Call options\n",
    "train_x_c = data_train_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "train_y_c = data_train_c['impl_volatility']\n",
    "\n",
    "validate_x_c = data_validate_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "validate_y_c = data_validate_c['impl_volatility']\n",
    "\n",
    "combined_x_c = data_train_tot_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "combined_y_c = data_train_tot_c['impl_volatility']\n",
    "\n",
    "test_x_c = data_test_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "test_y_c = data_test_c['impl_volatility']\n",
    "\n",
    "# Prepare train, validation, and test data for Put options\n",
    "train_x_p = data_train_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "train_y_p = data_train_p['impl_volatility']\n",
    "\n",
    "validate_x_p = data_validate_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "validate_y_p = data_validate_p['impl_volatility']\n",
    "\n",
    "combined_x_p = data_train_tot_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "combined_y_p = data_train_tot_p['impl_volatility']\n",
    "\n",
    "test_x_p = data_test_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "test_y_p = data_test_p['impl_volatility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from scikeras.wrappers import KerasRegressor  # Use SciKeras instead\n",
    "# from keras.optimizers import RMSprop\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(neurons=32, layers=1, dropout_rate=0.0, bias=0.01, activity=0.01):\n",
    "#     model = Sequential()\n",
    "#     # Input layer (first hidden layer)\n",
    "#     model.add(Dense(neurons, activation='relu', input_dim=train_x.shape[1],\n",
    "#                     bias_regularizer=regularizers.L2(bias),\n",
    "#                     activity_regularizer=regularizers.L2(activity)))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu',\n",
    "#                         bias_regularizer=regularizers.L2(bias),\n",
    "#                         activity_regularizer=regularizers.L2(activity)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Function to perform hyperparameter tuning, retrain the model, and test\n",
    "# def train_and_evaluate(train_x, train_y, validate_x, validate_y, combined_x, combined_y, test_x, test_y):\n",
    "#     # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "#     grid_search = GridSearchCV(estimator=model,\n",
    "#                                param_grid=param_grid,\n",
    "#                                scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "#                                cv=3,  # 3-fold cross-validation\n",
    "#                                verbose=1)  # Verbose for tracking progress\n",
    "\n",
    "#     # Hyperparameter tuning on training and validation sets\n",
    "#     print(\"Running hyperparameter tuning...\")\n",
    "#     grid_search.fit(train_x, train_y, validation_data=(validate_x, validate_y))\n",
    "\n",
    "#     # Get the best estimator and parameters\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "\n",
    "#     print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "#     # Retrain on the combined training and validation set\n",
    "#     best_model.fit(combined_x, combined_y)\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predictions = best_model.predict(test_x)\n",
    "\n",
    "#     # Calculate R² and RMSE for the best model\n",
    "#     r2 = r2_score(test_y, predictions)\n",
    "#     rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f\"R²: {r2:.4f}\")\n",
    "#     print(f\"RMSE: {rmse:.4f}\")\n",
    "#     return best_model\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# print(\"\\nEvaluating Call options...\")\n",
    "# best_model_call = train_and_evaluate(train_x_c, train_y_c, validate_x_c, validate_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# print(\"\\nEvaluating Put options...\")\n",
    "# best_model_put = train_and_evaluate(train_x_p, train_y_p, validate_x_p, validate_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Define the model function with variable neurons, layers, and dropout rate\n",
    "def create_model(input_dim, neurons=32, layers=1, dropout_rate=0.0, bias=0.01, activity=0.01):\n",
    "    model = Sequential()\n",
    "    # Input layer using Input instead of input_dim argument\n",
    "    model.add(Input(shape=(input_dim,)))  # Define the input shape explicitly\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(neurons, activation='relu',\n",
    "                    bias_regularizer=regularizers.L2(bias),\n",
    "                    activity_regularizer=regularizers.L2(activity)))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "    # Additional hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu',\n",
    "                        bias_regularizer=regularizers.L2(bias),\n",
    "                        activity_regularizer=regularizers.L2(activity)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "# Wrapping the model in KerasRegressor\n",
    "def create_keras_regressor(input_dim):\n",
    "    return KerasRegressor(model=create_model, input_dim=input_dim, verbose=1)  # Set verbose=1 for model fit\n",
    "\n",
    "# Define the parameter grid for trials\n",
    "param_grid = {\n",
    "    'model__neurons': [8, 16, 32, 64],    # Number of neurons in each hidden layer\n",
    "    'model__layers': [1, 2, 3, 4],        # Number of hidden layers\n",
    "    'model__dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "    'batch_size': [32],               # Batch size for training\n",
    "    'epochs': [50],                  # Number of epochs\n",
    "}\n",
    "\n",
    "# Function to perform hyperparameter tuning, retrain the model, and test\n",
    "def train_and_evaluate(train_x, train_y, validate_x, validate_y, combined_x, combined_y, test_x, test_y):\n",
    "    # Get input dimension from the training data\n",
    "    input_dim = train_x.shape[1]\n",
    "    \n",
    "    # Create KerasRegressor with the correct input dimension\n",
    "    model = create_keras_regressor(input_dim)\n",
    "\n",
    "    # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "                               cv=3,  # 3-fold cross-validation\n",
    "                               verbose=3)  # Verbose=3 for detailed progress tracking\n",
    "\n",
    "    # Hyperparameter tuning using validation data\n",
    "    print(\"Running hyperparameter tuning with validation data...\")\n",
    "    grid_search.fit(train_x, train_y,\n",
    "                    validation_data=(validate_x, validate_y),  # Use validation set for tuning\n",
    "                    verbose=1)\n",
    "\n",
    "    # Get the best estimator and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Retrain on the combined training and validation set with verbose output\n",
    "    best_model.fit(combined_x, combined_y, verbose=1)  # Verbose=1 for training progress tracking\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = best_model.predict(test_x)\n",
    "\n",
    "    # Calculate R² and RMSE for the best model\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "# Call the function for Call options data\n",
    "print(\"\\nEvaluating Call options...\")\n",
    "best_model_call = train_and_evaluate(train_x_c, train_y_c, validate_x_c, validate_y_c, combined_x_c, combined_y_c, test_x_c, test_y_c)\n",
    "\n",
    "# Call the function for Put options data\n",
    "print(\"\\nEvaluating Put options...\")\n",
    "# best_model_put = train_and_evaluate(train_x_p, train_y_p, validate_x_p, validate_y_p, combined_x_p, combined_y_p, test_x_p, test_y_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from tensorflow import keras\n",
    "# from keras.layers import Dense\n",
    "# from keras.models import Sequential, load_model\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # define the model\n",
    "# def create_model(bias, activity):\n",
    "#   model = Sequential()\n",
    "#   model.add(Dense(units=32,\n",
    "#                   activation='relu',\n",
    "#                   input_dim= train_x.shape[1],\n",
    "#                   # # kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-5),\n",
    "#                   bias_regularizer= regularizers.L2(bias),\n",
    "#                   activity_regularizer= regularizers.L2(activity)\n",
    "#                   ))\n",
    "#   model.add(Dense(units=32, activation= 'relu',\n",
    "#                   bias_regularizer=regularizers.L2(bias),\n",
    "#                   activity_regularizer=regularizers.L2(activity) #activation\n",
    "#                   ))\n",
    "#   model.add(Dense(units=1, activation= 'linear')) #miss linear\n",
    "\n",
    "#   # compile the model\n",
    "#   model.compile(optimizer= RMSprop(learning_rate=0.01), #'rmsprop',#Optimizer_trial,\n",
    "#                 loss= 'mean_squared_error',\n",
    "#                 metrics=['mse'])\n",
    "#   return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras.backend as K\n",
    "# from keras import regularizers\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from keras.optimizers import RMSprop\n",
    "\n",
    "# # Define the model function with variable neurons, layers, and dropout rate\n",
    "# def create_model(neurons=32, layers=1, dropout_rate=0.0, bias=0.01, activity=0.01):\n",
    "#     model = Sequential()\n",
    "#     # Input layer (first hidden layer)\n",
    "#     model.add(Dense(neurons, activation='relu', input_dim=train_x.shape[1],\n",
    "#                     bias_regularizer=regularizers.L2(bias),\n",
    "#                     activity_regularizer=regularizers.L2(activity)))\n",
    "#     model.add(Dropout(dropout_rate))  # Dropout layer after the first hidden layer\n",
    "\n",
    "#     # Additional hidden layers\n",
    "#     for _ in range(layers - 1):\n",
    "#         model.add(Dense(neurons, activation='relu',\n",
    "#                         bias_regularizer=regularizers.L2(bias),\n",
    "#                         activity_regularizer=regularizers.L2(activity)))\n",
    "#         model.add(Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     model.add(Dense(1, activation='linear'))  # Output layer for regression (linear)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer=RMSprop(learning_rate=0.01),\n",
    "#                   loss='mean_squared_error',\n",
    "#                   metrics=['mse'])\n",
    "#     return model\n",
    "\n",
    "# # Wrapping the model in KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for trials\n",
    "# param_grid = {\n",
    "#     'neurons': [8, 16, 32, 64],   # Number of neurons in each hidden layer\n",
    "#     'layers': [1, 2, 3, 4],       # Number of hidden layers\n",
    "#     'dropout_rate': [0, 0.2, 0.5], # Dropout rate\n",
    "#     'batch_size': [32, 64],       # Batch size for training\n",
    "#     'epochs': [50, 100],          # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV with the model, parameter grid, and scoring\n",
    "# grid_search = GridSearchCV(estimator=model,\n",
    "#                            param_grid=param_grid,\n",
    "#                            scoring='neg_mean_squared_error',  # Scoring based on MSE\n",
    "#                            cv=3,  # 3-fold cross-validation\n",
    "#                            verbose=1)  # Verbose for tracking progress\n",
    "\n",
    "# # Run the grid search\n",
    "# grid_search.fit(train_x, train_y)\n",
    "\n",
    "# # Get the best estimator and parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Make predictions using the best model\n",
    "# predictions = best_model.predict(test_x)\n",
    "\n",
    "# # Calculate R² and RMSE for the best model\n",
    "# r2 = r2_score(test_y, predictions)\n",
    "# rmse = np.sqrt(mean_squared_error(test_y, predictions))\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"R²: {r2:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "# import time\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from keras_tuner import KerasRegressor\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# # from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# # from keras_tuner import KerasRegressor\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# def create_model(hidden_layers=1, units=64, activation='relu', learning_rate=0.001):\n",
    "#     \"\"\"\n",
    "#     Function to create a Keras Sequential model with the given hyperparameters.\n",
    "#     \"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=units, activation=activation, input_dim=X_train.shape[1]))\n",
    "    \n",
    "#     for _ in range(hidden_layers - 1):\n",
    "#         model.add(Dense(units=units, activation=activation))\n",
    "        \n",
    "#     model.add(Dense(1))  # Output layer\n",
    "\n",
    "#     optimizer = Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def prepare_data_with_gridsearch_nn(train_data, validate_data, test_data, option_type, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, perform hyperparameter tuning using Year 1 (train) and Year 2 (validation),\n",
    "#     retrain the model on Year 1 + Year 2, and evaluate on Year 3 (test) for Neural Networks.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset (Year 1).\n",
    "#     validate_data (pd.DataFrame): The validation dataset (Year 2).\n",
    "#     test_data (pd.DataFrame): The testing dataset (Year 3).\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     verbose (bool): If True, prints progress information for hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train, validation, and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features (Year 1)\n",
    "#     y_train = train_data['impl_volatility']  # Training target (Year 1)\n",
    "\n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Validation features (Year 2)\n",
    "#     y_validate = validate_data['impl_volatility']  # Validation target (Year 2)\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features (Year 3)\n",
    "#     y_test = test_data['impl_volatility']  # Test target (Year 3)\n",
    "\n",
    "#     # Define the hyperparameter grid for NN\n",
    "#     param_grid = {\n",
    "#         'hidden_layers': [1, 2, 3],                # Number of hidden layers\n",
    "#         'units': [32, 64, 128],                    # Number of units per layer\n",
    "#         'activation': ['relu', 'tanh'],            # Activation function\n",
    "#         'learning_rate': [0.001, 0.01],            # Learning rate for Adam optimizer\n",
    "#         'batch_size': [32, 64],                    # Batch size\n",
    "#         'epochs': [50, 100],                       # Number of epochs\n",
    "#     }\n",
    "\n",
    "#     # Generate all combinations of hyperparameters\n",
    "#     param_combinations = list(itertools.product(\n",
    "#         param_grid['hidden_layers'], \n",
    "#         param_grid['units'], \n",
    "#         param_grid['activation'], \n",
    "#         param_grid['learning_rate'], \n",
    "#         param_grid['batch_size'], \n",
    "#         param_grid['epochs']\n",
    "#     ))\n",
    "\n",
    "#     total_combinations = len(param_combinations)\n",
    "    \n",
    "#     # Initialize variables to store the best model and best score\n",
    "#     best_rmse_val = np.inf\n",
    "#     best_params = None\n",
    "#     best_nn_model = None\n",
    "\n",
    "#     print(f\"Running manual hyperparameter tuning for {option_type} Options with Neural Networks...\")\n",
    "    \n",
    "#     # Iterate over all hyperparameter combinations with progress tracking\n",
    "#     for i, (hidden_layers, units, activation, learning_rate, batch_size, epochs) in enumerate(param_combinations):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Create the Keras model with the current set of hyperparameters\n",
    "#         model = create_model(hidden_layers=hidden_layers, units=units, activation=activation, learning_rate=learning_rate)\n",
    "\n",
    "#         # Early stopping to prevent overfitting\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "#         # Train the model on the training data (Year 1)\n",
    "#         history = model.fit(X_train, y_train, \n",
    "#                             validation_data=(X_validate, y_validate),\n",
    "#                             batch_size=batch_size, \n",
    "#                             epochs=epochs, \n",
    "#                             verbose=0, \n",
    "#                             callbacks=[early_stopping])\n",
    "\n",
    "#         # Validate the model on the validation data (Year 2)\n",
    "#         y_val_pred = model.predict(X_validate)\n",
    "#         rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "\n",
    "#         # Track the best performing hyperparameters based on validation RMSE\n",
    "#         if rmse_val < best_rmse_val:\n",
    "#             best_rmse_val = rmse_val\n",
    "#             best_params = {\n",
    "#                 'hidden_layers': hidden_layers,\n",
    "#                 'units': units,\n",
    "#                 'activation': activation,\n",
    "#                 'learning_rate': learning_rate,\n",
    "#                 'batch_size': batch_size,\n",
    "#                 'epochs': epochs\n",
    "#             }\n",
    "#             best_nn_model = model\n",
    "\n",
    "#         # Verbose output to track progress\n",
    "#         if verbose:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\"Combination {i + 1}/{total_combinations} completed in {elapsed_time:.2f} seconds.\")\n",
    "#             print(f\"Current RMSE (Validation): {rmse_val:.4f}\")\n",
    "#             print(f\"Best RMSE so far: {best_rmse_val:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nBest Parameters for {option_type} Options with Neural Networks: {best_params}\")\n",
    "    \n",
    "#     # Retrain the model on combined Year 1 (train) and Year 2 (validation)\n",
    "#     print(\"Retraining the model on Year 1 and Year 2 combined...\")\n",
    "#     X_combined = pd.concat([X_train, X_validate])\n",
    "#     y_combined = pd.concat([y_train, y_validate])\n",
    "#     best_nn_model.fit(X_combined, y_combined, batch_size=best_params['batch_size'], epochs=best_params['epochs'], verbose=0)\n",
    "\n",
    "#     # In-sample (combined Year 1 + Year 2) predictions\n",
    "#     y_combined_pred = best_nn_model.predict(X_combined)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on combined Year 1 + Year 2)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(y_combined, y_combined_pred))\n",
    "#     r2_combined = r2_score(y_combined, y_combined_pred)\n",
    "    \n",
    "#     print(f\"\\nIn-Sample Performance for {option_type} Options (Year 1 + Year 2):\")\n",
    "#     print(f\"RMSE (Training + Validation): {rmse_combined:.4f}\")\n",
    "#     print(f\"R² (Training + Validation): {r2_combined:.4f}\")\n",
    "\n",
    "#     # After retraining, evaluate performance on the test data (Year 3)\n",
    "#     y_test_pred = best_nn_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the final results\n",
    "#     print(f\"\\nPerformance on Test Data (Year 3) for {option_type} Options:\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch_nn(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch_nn(data_train_p, data_validate_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
