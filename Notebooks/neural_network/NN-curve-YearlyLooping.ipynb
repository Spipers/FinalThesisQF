{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pbibvvcrYDnn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","\n","from numpy import mean\n","from numpy import std\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_squared_error\n","\n","# from sklearn.ensemble import RandomForestRegressor\n","# from sklearn.model_selection import GridSearchCV\n","\n","import keras.backend as K\n","from keras import regularizers\n","from tensorflow import keras\n","from keras.layers import Dense\n","from keras.models import Sequential, load_model\n","from sklearn.model_selection import GridSearchCV\n","from keras.wrappers.scikit_learn import KerasRegressor\n","from keras.optimizers import RMSprop\n","from keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1728,"status":"ok","timestamp":1689752103064,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"},"user_tz":-120},"id":"sbsRE6MTUdKd","outputId":"ccbb4d66-de91-4e9e-c7c6-92ce81c54913"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# link to drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"elapsed":35342,"status":"ok","timestamp":1689752138401,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"},"user_tz":-120},"id":"03-xISFM2Ffj","outputId":"e8fa4957-d7b0-4297-ea90-1bbdda602f1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["             secid       -80       -50       -20     level     slope  \\\n","date                                                                   \n","1996-01-31  100862  0.307688  0.315627  0.358095  0.315627  0.042468   \n","1996-01-31  100871  0.705411  0.615432  0.651160  0.615432  0.035728   \n","1996-01-31  100892  0.270090  0.216768  0.216508  0.216768 -0.000260   \n","1996-01-31  100896  0.430796  0.436291  0.429517  0.436291 -0.006774   \n","1996-01-31  100903  0.262405  0.252683  0.373839  0.252683  0.121156   \n","\n","               curve  permno      beta    betasq  ...      cash   cinvest  \\\n","date                                              ...                       \n","1996-01-31  0.017264   54594 -0.502126 -0.886445  ... -0.868511  0.071798   \n","1996-01-31  0.062854   50906  0.551365  0.193703  ... -0.738289  0.080067   \n","1996-01-31  0.026531   57904 -0.536730 -0.902575  ... -0.958006  0.066337   \n","1996-01-31 -0.006135   77520  0.349363 -0.101800  ... -0.968644  0.060739   \n","1996-01-31  0.065439   80303 -0.186662 -0.682642  ... -0.584473  0.077826   \n","\n","                roaq    roavol    ms  baspread    maxret    retvol  \\\n","date                                                                 \n","1996-01-31  0.396279 -0.953113  0.00 -0.689049 -0.662673 -0.619551   \n","1996-01-31  0.538981 -0.900059  0.75 -0.112575 -0.390130 -0.223204   \n","1996-01-31  0.367446 -0.998951  0.25 -0.786859 -0.947305 -0.877289   \n","1996-01-31  0.459243 -0.859083 -0.25 -0.747736 -0.649524 -0.703492   \n","1996-01-31  0.605842 -0.875346 -0.50 -0.807682 -0.825541 -0.737079   \n","\n","            std_dolvol  std_turn  \n","date                              \n","1996-01-31    0.327234 -0.936622  \n","1996-01-31   -0.342296 -0.748948  \n","1996-01-31   -0.536138 -0.955821  \n","1996-01-31   -0.331142 -0.818216  \n","1996-01-31   -0.400503 -0.866238  \n","\n","[5 rows x 38 columns]"],"text/html":["\n","\n","  <div id=\"df-11d46c74-0a7b-4796-aee3-926721ac2234\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>secid</th>\n","      <th>-80</th>\n","      <th>-50</th>\n","      <th>-20</th>\n","      <th>level</th>\n","      <th>slope</th>\n","      <th>curve</th>\n","      <th>permno</th>\n","      <th>beta</th>\n","      <th>betasq</th>\n","      <th>...</th>\n","      <th>cash</th>\n","      <th>cinvest</th>\n","      <th>roaq</th>\n","      <th>roavol</th>\n","      <th>ms</th>\n","      <th>baspread</th>\n","      <th>maxret</th>\n","      <th>retvol</th>\n","      <th>std_dolvol</th>\n","      <th>std_turn</th>\n","    </tr>\n","    <tr>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1996-01-31</th>\n","      <td>100862</td>\n","      <td>0.307688</td>\n","      <td>0.315627</td>\n","      <td>0.358095</td>\n","      <td>0.315627</td>\n","      <td>0.042468</td>\n","      <td>0.017264</td>\n","      <td>54594</td>\n","      <td>-0.502126</td>\n","      <td>-0.886445</td>\n","      <td>...</td>\n","      <td>-0.868511</td>\n","      <td>0.071798</td>\n","      <td>0.396279</td>\n","      <td>-0.953113</td>\n","      <td>0.00</td>\n","      <td>-0.689049</td>\n","      <td>-0.662673</td>\n","      <td>-0.619551</td>\n","      <td>0.327234</td>\n","      <td>-0.936622</td>\n","    </tr>\n","    <tr>\n","      <th>1996-01-31</th>\n","      <td>100871</td>\n","      <td>0.705411</td>\n","      <td>0.615432</td>\n","      <td>0.651160</td>\n","      <td>0.615432</td>\n","      <td>0.035728</td>\n","      <td>0.062854</td>\n","      <td>50906</td>\n","      <td>0.551365</td>\n","      <td>0.193703</td>\n","      <td>...</td>\n","      <td>-0.738289</td>\n","      <td>0.080067</td>\n","      <td>0.538981</td>\n","      <td>-0.900059</td>\n","      <td>0.75</td>\n","      <td>-0.112575</td>\n","      <td>-0.390130</td>\n","      <td>-0.223204</td>\n","      <td>-0.342296</td>\n","      <td>-0.748948</td>\n","    </tr>\n","    <tr>\n","      <th>1996-01-31</th>\n","      <td>100892</td>\n","      <td>0.270090</td>\n","      <td>0.216768</td>\n","      <td>0.216508</td>\n","      <td>0.216768</td>\n","      <td>-0.000260</td>\n","      <td>0.026531</td>\n","      <td>57904</td>\n","      <td>-0.536730</td>\n","      <td>-0.902575</td>\n","      <td>...</td>\n","      <td>-0.958006</td>\n","      <td>0.066337</td>\n","      <td>0.367446</td>\n","      <td>-0.998951</td>\n","      <td>0.25</td>\n","      <td>-0.786859</td>\n","      <td>-0.947305</td>\n","      <td>-0.877289</td>\n","      <td>-0.536138</td>\n","      <td>-0.955821</td>\n","    </tr>\n","    <tr>\n","      <th>1996-01-31</th>\n","      <td>100896</td>\n","      <td>0.430796</td>\n","      <td>0.436291</td>\n","      <td>0.429517</td>\n","      <td>0.436291</td>\n","      <td>-0.006774</td>\n","      <td>-0.006135</td>\n","      <td>77520</td>\n","      <td>0.349363</td>\n","      <td>-0.101800</td>\n","      <td>...</td>\n","      <td>-0.968644</td>\n","      <td>0.060739</td>\n","      <td>0.459243</td>\n","      <td>-0.859083</td>\n","      <td>-0.25</td>\n","      <td>-0.747736</td>\n","      <td>-0.649524</td>\n","      <td>-0.703492</td>\n","      <td>-0.331142</td>\n","      <td>-0.818216</td>\n","    </tr>\n","    <tr>\n","      <th>1996-01-31</th>\n","      <td>100903</td>\n","      <td>0.262405</td>\n","      <td>0.252683</td>\n","      <td>0.373839</td>\n","      <td>0.252683</td>\n","      <td>0.121156</td>\n","      <td>0.065439</td>\n","      <td>80303</td>\n","      <td>-0.186662</td>\n","      <td>-0.682642</td>\n","      <td>...</td>\n","      <td>-0.584473</td>\n","      <td>0.077826</td>\n","      <td>0.605842</td>\n","      <td>-0.875346</td>\n","      <td>-0.50</td>\n","      <td>-0.807682</td>\n","      <td>-0.825541</td>\n","      <td>-0.737079</td>\n","      <td>-0.400503</td>\n","      <td>-0.866238</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 38 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d46c74-0a7b-4796-aee3-926721ac2234')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-c5443d8d-c206-4062-9352-ed589fca8b4b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c5443d8d-c206-4062-9352-ed589fca8b4b')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-c5443d8d-c206-4062-9352-ed589fca8b4b button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-11d46c74-0a7b-4796-aee3-926721ac2234 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-11d46c74-0a7b-4796-aee3-926721ac2234');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":7}],"source":["def initialise(data):\n","  data.index = data[\"date\"]\n","  data.index = pd.to_datetime(data.index)\n","  data.drop(columns=[\"date\"], inplace=True)\n","  return data\n","\n","data_level = initialise(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/data/Low resolution/data_level_mapped_selection10.csv'))\n","data_slope = initialise(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/data/Low resolution/data_slope_mapped_selection10.csv'))\n","data_curve = initialise(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/data/Low resolution/data_curve_mapped_selection10.csv'))\n","\n","data_level.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuLc1x7FHA7N"},"outputs":[],"source":["def samplesplitting(data,train,validate,oos,i):\n","    train_data    = data[(data.index.year >= 1996 )         & (data.index.year < i - validate)]\n","    validate_data = data[(data.index.year >= i - validate)  & (data.index.year < i)]\n","    total_train   = data[(data.index.year >= 1996 )         & (data.index.year < i)]\n","    oos_data      = data[(data.index.year == i)]\n","    pred_data     = data[(data.index.year >= 1996 + train + validate) & (data.index.year <= i)]\n","\n","    print('train=%d, validate=%d, oos=%d' % (len(train_data), len(validate_data), len(oos_data)))\n","    return train_data, validate_data, oos_data, total_train, pred_data\n","\n","def split_xy(data, kind):\n","  data_x = data.iloc[:,8:]\n","  data_y = data[kind]\n","  return data_x, data_y"]},{"cell_type":"markdown","metadata":{"id":"26NgyfqbGr5g"},"source":["# Level"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":643940,"status":"ok","timestamp":1686488529451,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"},"user_tz":-120},"id":"Um7r1u-AYbiw","outputId":"8815d6b3-45da-4088-8270-802d21b52a53"},"outputs":[{"name":"stdout","output_type":"stream","text":["train=408000, validate=120000, oos=24000\n","Fitting 2 folds for each of 1 candidates, totalling 2 fits\n","[CV 1/2; 1/1] START activity=1e-06, batch_size=100, bias=0.0001, epochs=50......\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-7-f6b829329de5>:51: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","  RegModel=KerasRegressor(create_model, verbose=0)\n"]},{"name":"stdout","output_type":"stream","text":["[CV 1/2; 1/1] END activity=1e-06, batch_size=100, bias=0.0001, epochs=50;, score=-0.042 total time= 2.7min\n","[CV 2/2; 1/1] START activity=1e-06, batch_size=100, bias=0.0001, epochs=50......\n","[CV 2/2; 1/1] END activity=1e-06, batch_size=100, bias=0.0001, epochs=50;, score=-0.066 total time= 2.7min\n","Best Hyperparameters: {'activity': 1e-06, 'batch_size': 100, 'bias': 0.0001, 'epochs': 50}\n","Iteration:  9\n","train=408000, validate=120000, oos=24000\n","Yearly: 0.3068934350927355\n"]}],"source":["# Complete number of observations is 982285\n","# We take the initial train, validate and oos sample to be 8, 5 and 13 years respectively (Ratios similar to Gu Kelly Xiu)\n","# On average one month contains 3000 options, therefore we loop over 12 * 3000 options, which is similar to looping over a year\n","trainsize = 11 * 2000 * 12\n","validatesize = 5 * 2000 * 12\n","oossize = 2000 * 12\n","\n","# Initialise predictionstring\n","IV = 'level'\n","oos_pred_level = pd.DataFrame()\n","\n","# define the model\n","def create_model(bias, activity):\n","  model = Sequential()\n","  model.add(Dense(units=32,\n","                  activation='relu',\n","                  input_dim= train_x.shape[1],\n","                  # # kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-5),\n","                  bias_regularizer= regularizers.L2(bias),\n","                  activity_regularizer= regularizers.L2(activity)\n","                  ))\n","  model.add(Dense(units=32, activation= 'relu',\n","                  bias_regularizer=regularizers.L2(bias),\n","                  activity_regularizer=regularizers.L2(activity) #activation\n","                  ))\n","  model.add(Dense(units=1, activation= 'linear')) #miss linear\n","\n","  # compile the model\n","  model.compile(optimizer= RMSprop(learning_rate=0.1), #'rmsprop',#Optimizer_trial,\n","                loss= 'mean_squared_error',\n","                metrics=['mse'])\n","  return model\n","\n","window = 9\n","train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","train_x, train_y              = split_xy(train,IV)\n","validate_x, validate_y        = split_xy(validate,IV)\n","oos_x, oos_y                  = split_xy(oos,IV)\n","total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","Parameter_Trials={'batch_size':[100],\n","                      'epochs':[50],\n","                      'bias':  [1e-4] ,\n","                      'activity':  [1e-6]\n","                    #'Optimizer_trial':['rmsprop'], #'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'\n","                  #'activation': ['linear']#['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n","                  # 'units1': [64, 32],\n","                  # 'units2': [32, 16]\n","                 } #learn rate, momentum\n","\n","RegModel=KerasRegressor(create_model, verbose=0)\n","grid_search=GridSearchCV(estimator=RegModel, param_grid=Parameter_Trials, cv=2, verbose = 10)\n","model_final = grid_search.fit(total_train_x, total_train_y)\n","best_params = grid_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)\n","\n","for window in range (9,10):\n","  print('Iteration: ', window)\n","\n","  # Sample splitting\n","  train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","  train_x, train_y              = split_xy(train,IV)\n","  validate_x, validate_y        = split_xy(validate,IV)\n","  oos_x, oos_y                  = split_xy(oos,IV)\n","  total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","  # predictions = pd.DataFrame(index=range(10), columns=['score', 'oos_prediction'])\n","\n","  # for network in range(0,10):\n","  #   model=KerasRegressor(create_model, verbose=0)\n","  #   model.fit(train_x, train_y, batch_size=5, epochs=30)\n","\n","  #   pred_val = model.predict(validate_x)\n","  #   y_hat_oos = model.predict(oos_x)\n","  #   print(network)\n","  #   print('R^2: %.8f' % (r2_score(validate_y, pred_val)))\n","  #   print('MSE: %.8f' % (mean_squared_error(validate_y, pred_val)))\n","  #   predictions.iloc[network,:] = [mean_squared_error(y_validate, pred_val), y_hat_oos]\n","\n","  # sorted_predictions = predictions.sort_values(by='score', ascending=True)\n","  # y_hat = pd.DataFrame(mean(sorted_predictions.iloc[0:3,1]))\n","  # y_hat.index = oos_y.index\n","\n","  # Fit model\n","  # print('Nr of trees: ', n_estimators_final)\n","  # print('Max Depth: ', max_depth_final)\n","  # print('Max Features: ', max_features_final)\n","  # model_final = RandomForestRegressor(n_estimators=n_estimators_final,\n","  #                                    max_depth=max_depth_final,\n","  #                                    max_features=max_features_final)\n","  # model_final.fit(total_train_x, total_train_y)\n","  y_hat = pd.DataFrame(model_final.predict(oos_x))\n","  print('Yearly:',r2_score(oos_y, y_hat))\n","  oos_pred_level = pd.concat([oos_pred_level, y_hat], ignore_index=True) #oos_pred_level = oos_pred_level.append(y_hat)\n","  # Optimisation method 2 GridSearch\n","\n","oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/level_predictions_NN_tuning9.csv', index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQrwEvGhf8c8","outputId":"deb05cb3-2271-490c-e7b7-b2c64421238c"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-5-9faa668deb4e>:63: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","  model_final = KerasRegressor(create_model, epochs=50, batch_size=100, verbose=0)\n"]},{"name":"stdout","output_type":"stream","text":["Iteration:  0\n","train=192000, validate=120000, oos=24000\n","0\n","R^2: 0.62654228\n","MSE: 0.02501595\n","1\n","R^2: 0.59990210\n","MSE: 0.02680043\n","2\n","R^2: 0.60821373\n","MSE: 0.02624368\n","3\n","R^2: 0.59730815\n","MSE: 0.02697419\n","4\n","R^2: 0.61775214\n","MSE: 0.02560475\n","5\n","R^2: 0.60763638\n","MSE: 0.02628235\n","6\n","R^2: 0.61400094\n","MSE: 0.02585603\n","7\n","R^2: 0.61214205\n","MSE: 0.02598054\n","8\n","R^2: 0.60199290\n","MSE: 0.02666038\n","9\n","R^2: 0.60493025\n","MSE: 0.02646362\n","Yearly: 0.3147790756908757\n","Total: 0.3147790756908757\n","Iteration:  1\n","train=216000, validate=120000, oos=24000\n","0\n","R^2: 0.58209944\n","MSE: 0.02855618\n","1\n","R^2: 0.57367586\n","MSE: 0.02913178\n"]}],"source":["# Complete number of observations is 982285\n","# We take the initial train, validate and oos sample to be 8, 5 and 13 years respectively (Ratios similar to Gu Kelly Xiu)\n","# On average one month contains 3000 options, therefore we loop over 12 * 3000 options, which is similar to looping over a year\n","trainsize = 8 * 2000 * 12\n","validatesize = 5 * 2000 * 12\n","oossize = 2000 * 12\n","\n","# Initialise predictionstring\n","IV = 'level'\n","oos_pred_level = pd.DataFrame()\n","\n","# #start of last prediction\n","# def isolate(data):\n","#   data = data.iloc[:,1:]\n","#   return data\n","# oos_pred_level   = isolate(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/level_predictions_NN_1_progress.csv'))\n","# oos_pred_level.columns = ['Predictions']\n","\n","# define the model\n","def create_model():\n","  model = Sequential()\n","  model.add(Dense(units=32,\n","                  activation='relu',\n","                  input_dim= train_x.shape[1],\n","                  bias_regularizer= regularizers.L2(1e-4),\n","                  activity_regularizer= regularizers.L2(1e-6)\n","                  ))\n","  model.add(Dense(units=32, activation= 'relu',\n","                  bias_regularizer=regularizers.L2(1e-4),\n","                  activity_regularizer=regularizers.L2(1e-6) #activation\n","                  ))\n","  model.add(Dense(units=1, activation= 'linear')) #miss linear\n","\n","  # compile the model\n","  model.compile(optimizer= Adam(), # was 0.01\n","                loss= 'mean_squared_error',\n","                metrics=['mse'])\n","  return model\n","\n","# window = 9\n","# train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","# train_x, train_y              = split_xy(train,IV)\n","# validate_x, validate_y        = split_xy(validate,IV)\n","# oos_x, oos_y                  = split_xy(oos,IV)\n","# total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","# Parameter_Trials={'batch_size':[100],\n","#                       'epochs':[50],\n","#                       'bias':  [1e-4] ,\n","#                       'activity':  [1e-6]\n","                    #'Optimizer_trial':['rmsprop'], #'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'\n","                  #'activation': ['linear']#['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n","                  # 'units1': [64, 32],\n","                  # 'units2': [32, 16]\n","                #  } #learn rate, momentum\n","\n","# RegModel=KerasRegressor(create_model, verbose=0)\n","# grid_search=GridSearchCV(estimator=RegModel, param_grid=Parameter_Trials, cv=2, verbose = 10)\n","# model_final = grid_search.fit(total_train_x, total_train_y)\n","# best_params = grid_search.best_params_\n","# print(\"Best Hyperparameters:\", best_params)\n","\n","model_final = KerasRegressor(create_model, epochs=50, batch_size=100, verbose=0)\n","\n","for window in range (0,10):\n","  print('Iteration: ', window)\n","\n","  # Sample splitting\n","  train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","  train_x, train_y              = split_xy(train,IV)\n","  validate_x, validate_y        = split_xy(validate,IV)\n","  oos_x, oos_y                  = split_xy(oos,IV)\n","  total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","  predictions = pd.DataFrame(index=range(10), columns=['score', 'oos_prediction'])\n","\n","  for network in range(0,10):\n","    # model=KerasRegressor(create_model, verbose=0)\n","    model_final.fit(total_train_x, total_train_y)\n","\n","    pred_val = model_final.predict(validate_x)\n","    y_hat_oos = model_final.predict(oos_x)\n","    print(network)\n","    print('R^2: %.8f' % (r2_score(validate_y, pred_val)))\n","    print('MSE: %.8f' % (mean_squared_error(validate_y, pred_val)))\n","    predictions.iloc[network,:] = [mean_squared_error(validate_y, pred_val), y_hat_oos]\n","\n","  sorted_predictions = predictions.sort_values(by='score', ascending=True)\n","  y_hat = pd.DataFrame(mean(sorted_predictions.iloc[0:3,1]))\n","  y_hat.index = oos_y.index\n","  y_hat.columns = ['Predictions']\n","\n","  # Fit model\n","  # print('Nr of trees: ', n_estimators_final)\n","  # print('Max Depth: ', max_depth_final)\n","  # print('Max Features: ', max_features_final)\n","  # model_final = RandomForestRegressor(n_estimators=n_estimators_final,\n","  #                                    max_depth=max_depth_final,\n","  #                                    max_features=max_features_final)\n","  # model_final.fit(total_train_x, total_train_y)\n","  # y_hat = pd.DataFrame(model_final.predict(oos_x))\n","\n","  print('Yearly:',r2_score(oos_y, y_hat))\n","  oos_pred_level = pd.concat([oos_pred_level, y_hat], ignore_index=True) #oos_pred_level = oos_pred_level.append(y_hat)\n","  print('Total:',r2_score(total_oos[IV], oos_pred_level))\n","  oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/level_predictions_NN_1_progress.csv', index=True)\n","\n","oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/level_predictions_NN_1.csv', index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9agI1A18tjp_"},"outputs":[],"source":["oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/RF/level_predictions_RF.csv', index=True)"]},{"cell_type":"markdown","metadata":{"id":"urhPkYV9GuHb"},"source":["# Slope"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrpdhht0G2Mx"},"outputs":[],"source":["# Complete number of observations is 982285\n","# We take the initial train, validate and oos sample to be 8, 5 and 13 years respectively (Ratios similar to Gu Kelly Xiu)\n","# On average one month contains 3000 options, therefore we loop over 12 * 3000 options, which is similar to looping over a year\n","trainsize = 11 * 2000 * 12\n","validatesize = 5 * 2000 * 12\n","oossize = 2000 * 12\n","\n","# Initialise predictionstring\n","IV = 'slope'\n","oos_pred_slope = pd.DataFrame()\n","\n","# #start of last prediction\n","# def isolate(data):\n","#   data = data.iloc[:,1:]\n","#   return data\n","# oos_pred_slope   = isolate(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/slope_predictions_NN_1_progress.csv'))\n","# oos_pred_slope.columns = ['Predictions']\n","\n","# define the model\n","def create_model():\n","  model = Sequential()\n","  model.add(Dense(units=32,\n","                  activation='relu',\n","                  input_dim= train_x.shape[1],\n","                  bias_regularizer= regularizers.L2(1e-4),\n","                  activity_regularizer= regularizers.L2(1e-6)\n","                  ))\n","  model.add(Dense(units=32, activation= 'relu',\n","                  bias_regularizer=regularizers.L2(1e-4),\n","                  activity_regularizer=regularizers.L2(1e-6) #activation\n","                  ))\n","  model.add(Dense(units=1, activation= 'linear')) #miss linear\n","\n","  # compile the model\n","  model.compile(optimizer= Adam(), # was 0.01\n","                loss= 'mean_squared_error',\n","                metrics=['mse'])\n","  return model\n","\n","# window = 9\n","# train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","# train_x, train_y              = split_xy(train,IV)\n","# validate_x, validate_y        = split_xy(validate,IV)\n","# oos_x, oos_y                  = split_xy(oos,IV)\n","# total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","# Parameter_Trials={'batch_size':[100],\n","#                       'epochs':[50],\n","#                       'bias':  [1e-4] ,\n","#                       'activity':  [1e-6]\n","                    #'Optimizer_trial':['rmsprop'], #'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'\n","                  #'activation': ['linear']#['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n","                  # 'units1': [64, 32],\n","                  # 'units2': [32, 16]\n","                #  } #learn rate, momentum\n","\n","# RegModel=KerasRegressor(create_model, verbose=0)\n","# grid_search=GridSearchCV(estimator=RegModel, param_grid=Parameter_Trials, cv=2, verbose = 10)\n","# model_final = grid_search.fit(total_train_x, total_train_y)\n","# best_params = grid_search.best_params_\n","# print(\"Best Hyperparameters:\", best_params)\n","\n","model_final = KerasRegressor(create_model, epochs=50, batch_size=100, verbose=0)\n","\n","for window in range (0,10):\n","  print('Iteration: ', window)\n","\n","  # Sample splitting\n","  train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)\n","  train_x, train_y              = split_xy(train,IV)\n","  validate_x, validate_y        = split_xy(validate,IV)\n","  oos_x, oos_y                  = split_xy(oos,IV)\n","  total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","  predictions = pd.DataFrame(index=range(10), columns=['score', 'oos_prediction'])\n","\n","  for network in range(0,10):\n","    # model=KerasRegressor(create_model, verbose=0)\n","    model_final.fit(total_train_x, total_train_y)\n","\n","    pred_val = model_final.predict(validate_x)\n","    y_hat_oos = model_final.predict(oos_x)\n","    print(network)\n","    print('R^2: %.8f' % (r2_score(validate_y, pred_val)))\n","    print('MSE: %.8f' % (mean_squared_error(validate_y, pred_val)))\n","    predictions.iloc[network,:] = [mean_squared_error(validate_y, pred_val), y_hat_oos]\n","\n","  sorted_predictions = predictions.sort_values(by='score', ascending=True)\n","  y_hat = pd.DataFrame(mean(sorted_predictions.iloc[0:3,1]))\n","  y_hat.index = oos_y.index\n","  y_hat.columns = ['Predictions']\n","\n","  # Fit model\n","  # print('Nr of trees: ', n_estimators_final)\n","  # print('Max Depth: ', max_depth_final)\n","  # print('Max Features: ', max_features_final)\n","  # model_final = RandomForestRegressor(n_estimators=n_estimators_final,\n","  #                                    max_depth=max_depth_final,\n","  #                                    max_features=max_features_final)\n","  # model_final.fit(total_train_x, total_train_y)\n","  # y_hat = pd.DataFrame(model_final.predict(oos_x))\n","\n","  print('Yearly:',r2_score(oos_y, y_hat))\n","  oos_pred_slope = pd.concat([oos_pred_slope, y_hat], ignore_index=True) #oos_pred_level = oos_pred_level.append(y_hat)\n","  print('Total:',r2_score(total_oos[IV], oos_pred_slope))\n","  oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/slope_predictions_NN_1_progress.csv', index=True)\n","\n","oos_pred_level.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/slope_predictions_NN_1.csv', index=True)\n"]},{"cell_type":"markdown","metadata":{"id":"eoc2Dp86Gw5F"},"source":["# Curvature"]},{"cell_type":"code","source":["trainsize = 11\n","validatesize = 5\n","oossize = 1\n","\n","def isolate(data):\n","  data = data.iloc[:,1:]\n","  return data\n","oos_pred_curve   = isolate(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/curve_predictions_NN_progress_yearly.csv'))\n","\n","window = 2018\n","train, validate, oos, total_train, total_oos = samplesplitting(data_level, trainsize, validatesize, oossize, window)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQv5mZ-NZVxG","executionInfo":{"status":"ok","timestamp":1688818422943,"user_tz":-120,"elapsed":1211,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"}},"outputId":"5a143545-8fe3-4471-bdac-4a8441746f2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train=402892, validate=164247, oos=30002\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6zRhQ1gKG2_J","outputId":"c77111a8-49b3-45a5-b0e9-52e54871b9c9","executionInfo":{"status":"ok","timestamp":1689793754606,"user_tz":-120,"elapsed":861522,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train=504261, validate=148707, oos=27899\n","Iteration:  2012\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-11-9e6ac0dd179f>:65: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n","  model_final = KerasRegressor(create_model, epochs=50, batch_size=100, verbose=0)\n"]},{"output_type":"stream","name":"stdout","text":["train=210839, validate=158189, oos=33864\n","0\n","R^2: 0.05157142\n","MSE: 0.00597253\n","1\n","R^2: 0.05618201\n","MSE: 0.00594350\n","2\n","R^2: 0.04411384\n","MSE: 0.00601950\n","3\n","R^2: 0.06004548\n","MSE: 0.00591917\n","4\n","R^2: 0.05089596\n","MSE: 0.00597679\n","Yearly: -0.011041960626488878\n","Total: -0.011041960626488878\n","Iteration:  2013\n","train=241135, validate=161757, oos=34182\n","0\n","R^2: 0.06133028\n","MSE: 0.00769562\n","1\n","R^2: 0.05939688\n","MSE: 0.00771147\n","2\n","R^2: 0.05769541\n","MSE: 0.00772542\n","3\n","R^2: 0.06091107\n","MSE: 0.00769905\n","4\n","R^2: 0.05802547\n","MSE: 0.00772271\n","Yearly: 0.0006214541261077011\n","Total: -0.002880379301016589\n","Iteration:  2014\n","train=272284, validate=164790, oos=34113\n","0\n","R^2: 0.04808857\n","MSE: 0.00876703\n","1\n","R^2: 0.04481384\n","MSE: 0.00879719\n","2\n","R^2: 0.05456316\n","MSE: 0.00870740\n","3\n","R^2: 0.05345913\n","MSE: 0.00871756\n","4\n","R^2: 0.04716476\n","MSE: 0.00877553\n","Yearly: -0.03053999998087842\n","Total: -0.0060252505575266735\n","Iteration:  2015\n","train=303150, validate=168037, oos=33074\n","0\n","R^2: 0.06810979\n","MSE: 0.01073017\n","1\n","R^2: 0.06715513\n","MSE: 0.01074116\n","2\n","R^2: 0.07112566\n","MSE: 0.01069544\n","3\n","R^2: 0.06178763\n","MSE: 0.01080296\n","4\n","R^2: 0.05987811\n","MSE: 0.01082495\n","Yearly: -0.0013528744560222794\n","Total: -0.0005604279285202907\n","Iteration:  2016\n","train=335237, validate=169024, oos=31906\n","0\n","R^2: 0.06614481\n","MSE: 0.01269601\n","1\n","R^2: 0.07089843\n","MSE: 0.01263138\n","2\n","R^2: 0.06877281\n","MSE: 0.01266028\n","3\n","R^2: 0.06381304\n","MSE: 0.01272771\n","4\n","R^2: 0.07015934\n","MSE: 0.01264143\n","Yearly: -0.053266297532111206\n","Total: -0.0025455606381279416\n","Iteration:  2017\n","train=369028, validate=167139, oos=30972\n","0\n","R^2: 0.07171248\n","MSE: 0.01425175\n","1\n","R^2: 0.08476403\n","MSE: 0.01405137\n","2\n","R^2: 0.07059445\n","MSE: 0.01426891\n","3\n","R^2: 0.07568589\n","MSE: 0.01419074\n","4\n","R^2: 0.08636998\n","MSE: 0.01402671\n","Yearly: -0.00786582220784271\n","Total: -0.00329413791223776\n","Iteration:  2018\n","train=402892, validate=164247, oos=30002\n","0\n","R^2: 0.07558923\n","MSE: 0.01444992\n","1\n","R^2: 0.08009199\n","MSE: 0.01437954\n","2\n","R^2: 0.07263867\n","MSE: 0.01449604\n","3\n","R^2: 0.07604518\n","MSE: 0.01444279\n","4\n","R^2: 0.07740978\n","MSE: 0.01442146\n","Yearly: -0.00937952485530058\n","Total: -0.0038319101191641725\n","Iteration:  2019\n","train=437074, validate=160067, oos=28538\n","0\n","R^2: 0.08024364\n","MSE: 0.01545894\n","1\n","R^2: 0.06866729\n","MSE: 0.01565351\n","2\n","R^2: 0.07030641\n","MSE: 0.01562596\n","3\n","R^2: 0.06921751\n","MSE: 0.01564426\n","4\n","R^2: 0.06751873\n","MSE: 0.01567281\n","Yearly: 0.00037564007754797846\n","Total: -0.0029022483466984994\n","Iteration:  2020\n","train=471187, validate=154492, oos=27289\n","0\n","R^2: 0.07856833\n","MSE: 0.01550736\n","1\n","R^2: 0.06411808\n","MSE: 0.01575055\n","2\n","R^2: 0.07393866\n","MSE: 0.01558527\n","3\n","R^2: 0.07392104\n","MSE: 0.01558557\n","4\n","R^2: 0.07598937\n","MSE: 0.01555076\n","Yearly: 0.03457161995104452\n","Total: 0.006065556614997525\n","Iteration:  2021\n","train=504261, validate=148707, oos=27899\n","0\n","R^2: 0.07143339\n","MSE: 0.01699343\n","1\n","R^2: 0.07221835\n","MSE: 0.01697907\n","2\n","R^2: 0.07998743\n","MSE: 0.01683689\n","3\n","R^2: 0.07502488\n","MSE: 0.01692770\n","4\n","R^2: 0.06906424\n","MSE: 0.01703679\n","Yearly: 0.0033379913662329175\n","Total: 0.008408690029391153\n"]}],"source":["from sklearn.inspection import permutation_importance\n","# Complete number of observations is 982285\n","# We take the initial train, validate and oos sample to be 8, 5 and 13 years respectively (Ratios similar to Gu Kelly Xiu)\n","# On average one month contains 3000 options, therefore we loop over 12 * 3000 options, which is similar to looping over a year\n","trainsize = 11\n","validatesize = 5\n","oossize = 1\n","\n","# Initialise predictionstring\n","IV = 'curve'\n","oos_pred_curve = pd.DataFrame()\n","\n","# #start of last prediction\n","# def isolate(data):\n","#   data = data.iloc[:,1:]\n","#   return data\n","# oos_pred_curve   = isolate(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/curve_predictions_NN_2_progress_yearly.csv'))\n","# oos_pred_curve.columns = ['Predictions']\n","\n","# define the model\n","def create_model():\n","  model = Sequential()\n","  model.add(Dense(units=32,\n","                  activation='relu',\n","                  input_dim= train_x.shape[1],\n","                  bias_regularizer= regularizers.L2(1e-4),\n","                  activity_regularizer= regularizers.L2(1e-6)\n","                  ))\n","  model.add(Dense(units=32, activation= 'relu',\n","                  bias_regularizer=regularizers.L2(1e-4),\n","                  activity_regularizer=regularizers.L2(1e-6) #activation\n","                  ))\n","  model.add(Dense(units=1, activation= 'linear')) #miss linear\n","\n","  # compile the model\n","  model.compile(optimizer= Adam(), # was 0.01\n","                loss= 'mean_squared_error',\n","                metrics=['mse'])\n","  return model\n","\n","window = 2021\n","train, validate, oos, total_train, total_oos = samplesplitting(data_curve, trainsize, validatesize, oossize, window)\n","train_x, train_y              = split_xy(train,IV)\n","validate_x, validate_y        = split_xy(validate,IV)\n","oos_x, oos_y                  = split_xy(oos,IV)\n","total_train_x, total_train_y  = split_xy(total_train,IV)\n","feature_importance = np.zeros(total_train_x.shape[1]) #new\n","\n","# Parameter_Trials={'batch_size':[100],\n","#                       'epochs':[50],\n","#                       'bias':  [1e-4] ,\n","#                       'activity':  [1e-6]\n","                    #'Optimizer_trial':['rmsprop'], #'SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'\n","                  #'activation': ['linear']#['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n","                  # 'units1': [64, 32],\n","                  # 'units2': [32, 16]\n","                #  } #learn rate, momentum\n","\n","# RegModel=KerasRegressor(create_model, verbose=0)\n","# grid_search=GridSearchCV(estimator=RegModel, param_grid=Parameter_Trials, cv=2, verbose = 10)\n","# model_final = grid_search.fit(total_train_x, total_train_y)\n","# best_params = grid_search.best_params_\n","# print(\"Best Hyperparameters:\", best_params)\n","\n","model_final = KerasRegressor(create_model, epochs=50, batch_size=100, verbose=0)\n","\n","for window in range(2012, 2021+1):\n","  print('Iteration: ', window)\n","\n","  # Sample splitting\n","  train, validate, oos, total_train, total_oos = samplesplitting(data_curve, trainsize, validatesize, oossize, window)\n","  train_x, train_y              = split_xy(train,IV)\n","  validate_x, validate_y        = split_xy(validate,IV)\n","  oos_x, oos_y                  = split_xy(oos,IV)\n","  total_train_x, total_train_y  = split_xy(total_train,IV)\n","\n","  predictions = pd.DataFrame(index=range(5), columns=['score', 'oos_prediction'])\n","  feature_importance_networks = pd.DataFrame(index=range(5), columns=['score', 'importances']) #new\n","\n","  for network in range(0,5):\n","    # model=KerasRegressor(create_model, verbose=0)\n","    model_final.fit(total_train_x, total_train_y)\n","\n","    # Calculate feature importance using permutation importance #NEW\n","    results = permutation_importance(model_final, total_train_x, total_train_y, n_repeats=1, random_state=42) #NEW\n","    # Get the feature importances and feature names\n","    importance = results.importances_mean #NEW\n","\n","    pred_val = model_final.predict(validate_x)\n","    y_hat_oos = model_final.predict(oos_x)\n","    print(network)\n","    print('R^2: %.8f' % (r2_score(validate_y, pred_val)))\n","    print('MSE: %.8f' % (mean_squared_error(validate_y, pred_val)))\n","    predictions.iloc[network,:] = [mean_squared_error(validate_y, pred_val), y_hat_oos]\n","    feature_importance_networks.iloc[network,:] = [mean_squared_error(validate_y, pred_val), importance] #NEW\n","\n","  sorted_predictions = predictions.sort_values(by='score', ascending=True)\n","  y_hat = pd.DataFrame(mean(sorted_predictions.iloc[0:2,1]))\n","  y_hat.index = oos_y.index\n","  y_hat.columns = ['Predictions']\n","\n","  # Obtain feature importance\n","  sorted_importance = feature_importance_networks.sort_values(by='score', ascending=True) #NEW\n","  model_feat_imp = pd.DataFrame(mean(sorted_importance.iloc[0:2,1])) #NEW\n","  feature_importance += np.array(model_feat_imp).flatten() #NEW\n","\n","  # Fit model\n","  # print('Nr of trees: ', n_estimators_final)\n","  # print('Max Depth: ', max_depth_final)\n","  # print('Max Features: ', max_features_final)\n","  # model_final = RandomForestRegressor(n_estimators=n_estimators_final,\n","  #                                    max_depth=max_depth_final,\n","  #                                    max_features=max_features_final)\n","  # model_final.fit(total_train_x, total_train_y)\n","  # y_hat = pd.DataFrame(model_final.predict(oos_x))\n","\n","  print('Yearly:',r2_score(oos_y, y_hat))\n","  oos_pred_curve = pd.concat([oos_pred_curve, y_hat], ignore_index=True) #oos_pred_level = oos_pred_level.append(y_hat)\n","  print('Total:',r2_score(total_oos[IV], oos_pred_curve))\n","  oos_pred_curve.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/curve_predictions_NN_2_progress_yearly.csv', index=True)\n","\n","feature_importance_mean = feature_importance / 10 #NEW\n","np.savetxt('/content/drive/MyDrive/Msc Thesis/code/Results/NN/feature_importance_mean_curve_2_yearly.csv', feature_importance_mean, delimiter=',') #NEW\n","\n","oos_pred_curve.to_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/curve_predictions_NN_2_yearly.csv', index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":973,"status":"ok","timestamp":1686684452152,"user":{"displayName":"Maud van lent","userId":"14567433046769998042"},"user_tz":-120},"id":"ZEZ5Mfn4OXQB","outputId":"6995487d-07f4-41c5-a222-06d46e9ba5a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["        Predictions\n","0          0.042977\n","1          0.063609\n","2          0.034104\n","3          0.034903\n","4          0.034587\n","...             ...\n","191995     0.065792\n","191996     0.027477\n","191997     0.075264\n","191998     0.085852\n","191999     0.067550\n","\n","[192000 rows x 1 columns]\n"]}],"source":["#start of last prediction\n","def isolate(data):\n","  data = data.iloc[:,1:]\n","  return data\n","oos_pred_curve   = isolate(pd.read_csv('/content/drive/MyDrive/Msc Thesis/code/Results/NN/curve_predictions_NN_1_progress.csv'))\n","\n","print(oos_pred_curve)"]}],"metadata":{"colab":{"collapsed_sections":["26NgyfqbGr5g","urhPkYV9GuHb"],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}