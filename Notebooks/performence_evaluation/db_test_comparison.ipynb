{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM Test Statistic for Calls: 7175.329374924573\n",
      "DM Test Statistic for Puts: 12101.872043841688\n",
      "Average DM Test Statistic: 9638.60070938313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_45122/1440618638.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_variance = ols_model.bse[0]**2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function to calculate Newey-West standard error\n",
    "def newey_west_variance(differences, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West variance for the series of differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - differences: Array-like of differences (e.g., error differences between two models)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - Newey-West variance of the differences\n",
    "    \"\"\"\n",
    "    # Create a simple OLS model for the differences\n",
    "    X = np.ones(len(differences))  # Constant term\n",
    "    ols_model = sm.OLS(differences, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    \n",
    "    # Return the estimated variance (squared standard error of the constant term)\n",
    "    nw_variance = ols_model.bse[0]**2\n",
    "    return nw_variance\n",
    "\n",
    "# Function to compute the DM test statistic\n",
    "def dm_test_statistic(errors1, errors2, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Diebold-Mariano test statistic between two sets of errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - errors1: First set of errors (e.g., from the first model)\n",
    "    - errors2: Second set of errors (e.g., from the second model)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - DM test statistic\n",
    "    \"\"\"\n",
    "    # Calculate the error differences\n",
    "    differences = errors1 - errors2\n",
    "    \n",
    "    # Calculate the average difference (mean of d_t)\n",
    "    mean_diff = np.mean(differences)\n",
    "    \n",
    "    # Calculate Newey-West variance\n",
    "    nw_variance = newey_west_variance(differences, lag)\n",
    "    \n",
    "    # Compute the DM test statistic\n",
    "    dm_statistic = mean_diff / np.sqrt(nw_variance / len(differences))\n",
    "    \n",
    "    return dm_statistic\n",
    "\n",
    "# Load the CSV files\n",
    "dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_ols.csv')\n",
    "adhbs_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_adhbs.csv')\n",
    "\n",
    "# Ensure both dataframes have the same length (if not, pad with zeros)\n",
    "max_length = max(len(dm_test_data), len(adhbs_test_data))\n",
    "\n",
    "dm_test_data = dm_test_data.reindex(range(max_length), fill_value=0)\n",
    "adhbs_test_data = adhbs_test_data.reindex(range(max_length), fill_value=0)\n",
    "\n",
    "# Calculate DM test statistic for Call errors\n",
    "dm_call_errors = dm_test_data['Call Errors']\n",
    "adhbs_call_errors = adhbs_test_data['Call Errors']\n",
    "\n",
    "dm_statistic_call = dm_test_statistic(dm_call_errors, adhbs_call_errors, lag=1)\n",
    "\n",
    "# Calculate DM test statistic for Put errors\n",
    "dm_put_errors = dm_test_data['Put Errors']\n",
    "adhbs_put_errors = adhbs_test_data['Put Errors']\n",
    "\n",
    "dm_statistic_put = dm_test_statistic(dm_put_errors, adhbs_put_errors, lag=1)\n",
    "\n",
    "# Calculate the average of the two DM test statistics\n",
    "dm_statistic_avg = (dm_statistic_call + dm_statistic_put) / 2\n",
    "\n",
    "# Output the DM test statistics\n",
    "print(f\"DM Test Statistic for Calls: {dm_statistic_call}\")\n",
    "print(f\"DM Test Statistic for Puts: {dm_statistic_put}\")\n",
    "print(f\"Average DM Test Statistic: {dm_statistic_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02048935609173135\n",
      "7.202397430647932e-06\n",
      "0.02788108710405005\n",
      "8.371082981263438e-06\n",
      "DM Test Statistic for Calls: 7.634661197209441\n",
      "DM Test Statistic for Puts: 9.636490102765851\n",
      "Average DM Test Statistic: 8.635575649987647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_45122/2131938685.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_variance = ols_model.bse[0]**2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function to calculate Newey-West standard error\n",
    "def newey_west_variance(differences, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West variance for the series of differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - differences: Array-like of differences (e.g., error differences between two models)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - Newey-West variance of the differences\n",
    "    \"\"\"\n",
    "    # Create a simple OLS model for the differences\n",
    "    X = np.ones(len(differences))  # Constant term\n",
    "    ols_model = sm.OLS(differences, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    \n",
    "    # Return the estimated variance (squared standard error of the constant term)\n",
    "    nw_variance = ols_model.bse[0]**2\n",
    "    return nw_variance\n",
    "\n",
    "# Function to compute the DM test statistic\n",
    "def dm_test_statistic(errors1, errors2, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Diebold-Mariano test statistic between two sets of errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - errors1: First set of errors (e.g., from the first model)\n",
    "    - errors2: Second set of errors (e.g., from the second model)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - DM test statistic\n",
    "    \"\"\"\n",
    "    # Calculate the error differences\n",
    "    # Step 1: Square both sets of errors\n",
    "    squared_errors1 = np.square(errors1)\n",
    "    squared_errors2 = np.square(errors2)\n",
    "\n",
    "    # Step 2: Calculate the difference between the squared errors\n",
    "    squared_diff = squared_errors1 - squared_errors2\n",
    "\n",
    "    # Step 3: Calculate the average difference (mean of d_t)\n",
    "    mean_diff = np.mean(squared_diff)\n",
    "    print(mean_diff)\n",
    "    # Calculate Newey-West variance\n",
    "    nw_variance = newey_west_variance(squared_diff, lag)\n",
    "    print(nw_variance)\n",
    "\n",
    "    # Compute the DM test statistic\n",
    "    dm_statistic = mean_diff / np.sqrt(nw_variance)\n",
    "    \n",
    "    return dm_statistic\n",
    "\n",
    "# Load the CSV files\n",
    "adhbs_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_adhbs.csv')\n",
    "ols_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_ols.csv')\n",
    "enet_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_enet.csv')\n",
    "rf_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_rf.csv')\n",
    "xgb_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_xgb.csv')\n",
    "nn_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_nn.csv')\n",
    "\n",
    "# Ensure both dataframes have the same length (if not, pad with zeros)\n",
    "max_length = max(len(dm_test_data), len(adhbs_test_data))\n",
    "\n",
    "dm_test_data = dm_test_data.reindex(range(max_length), fill_value=0)\n",
    "adhbs_test_data = adhbs_test_data.reindex(range(max_length), fill_value=0)\n",
    "\n",
    "# Calculate DM test statistic for Call errors\n",
    "dm_call_errors = dm_test_data['Call Errors']\n",
    "adhbs_call_errors = adhbs_test_data['Call Errors']\n",
    "\n",
    "dm_statistic_call = dm_test_statistic(dm_call_errors, adhbs_call_errors, lag=1)\n",
    "\n",
    "# Calculate DM test statistic for Put errors\n",
    "dm_put_errors = dm_test_data['Put Errors']\n",
    "adhbs_put_errors = adhbs_test_data['Put Errors']\n",
    "\n",
    "dm_statistic_put = dm_test_statistic(dm_put_errors, adhbs_put_errors, lag=1)\n",
    "\n",
    "# Calculate the average of the two DM test statistics\n",
    "dm_statistic_avg = (dm_statistic_call + dm_statistic_put) / 2\n",
    "\n",
    "# Output the DM test statistics\n",
    "print(f\"DM Test Statistic for Calls: {dm_statistic_call}\")\n",
    "print(f\"DM Test Statistic for Puts: {dm_statistic_put}\")\n",
    "print(f\"Average DM Test Statistic: {dm_statistic_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ADHBS    OLS   ENet     NN     RF XGBoost\n",
      "ADHBS      NaN -63.11 -68.12 -90.09 -72.43  -71.27\n",
      "OLS      63.11    NaN -20.19 -13.09 -19.31  -21.33\n",
      "ENet     68.12  20.19    NaN  -7.07 -11.46  -14.65\n",
      "NN       90.09  13.09   7.07    NaN  -1.61   -5.41\n",
      "RF       72.43  19.31  11.46   1.61    NaN   -6.58\n",
      "XGBoost  71.27  21.33  14.65   5.41   6.58     NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_45122/3529540611.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_variance = ols_model.bse[0]**2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function to calculate Newey-West standard error\n",
    "def newey_west_variance(differences, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West variance for the series of differences.\n",
    "    \n",
    "    Parameters:\n",
    "    - differences: Array-like of differences (e.g., error differences between two models)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - Newey-West variance of the differences\n",
    "    \"\"\"\n",
    "    # Create a simple OLS model for the differences\n",
    "    X = np.ones(len(differences))  # Constant term\n",
    "    ols_model = sm.OLS(differences, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    \n",
    "    # Return the estimated variance (squared standard error of the constant term)\n",
    "    nw_variance = ols_model.bse[0]**2\n",
    "    return nw_variance\n",
    "\n",
    "# Function to compute the DM test statistic\n",
    "def dm_test_statistic(errors1, errors2, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Diebold-Mariano test statistic between two sets of errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - errors1: First set of errors (e.g., from the first model)\n",
    "    - errors2: Second set of errors (e.g., from the second model)\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - DM test statistic\n",
    "    \"\"\"\n",
    "    # Step 1: Square both sets of errors\n",
    "    squared_errors1 = np.square(errors1)\n",
    "    squared_errors2 = np.square(errors2)\n",
    "\n",
    "    # Step 2: Calculate the difference between the squared errors\n",
    "    squared_diff = squared_errors1 - squared_errors2\n",
    "\n",
    "    # Step 3: Calculate the average difference (mean of d_t)\n",
    "    mean_diff = np.mean(squared_diff)\n",
    "\n",
    "    # Step 4: Calculate Newey-West variance\n",
    "    nw_variance = newey_west_variance(squared_diff, lag)\n",
    "\n",
    "    # Step 5: Compute the DM test statistic\n",
    "    dm_statistic = mean_diff / np.sqrt(nw_variance)\n",
    "    \n",
    "    return dm_statistic\n",
    "\n",
    "# Load the CSV files\n",
    "adhbs_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_adhbs.csv')\n",
    "ols_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_ols.csv')\n",
    "enet_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_enet.csv')\n",
    "rf_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_rf.csv')\n",
    "xgb_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_xgb.csv')\n",
    "nn_dm_test_data = pd.read_csv('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_nn.csv')\n",
    "\n",
    "# Store the data in a dictionary for easier access\n",
    "models = {\n",
    "    \"ADHBS\": adhbs_dm_test_data,\n",
    "    \"OLS\": ols_dm_test_data,\n",
    "    \"ENet\": enet_dm_test_data,\n",
    "    \"NN\": nn_dm_test_data,\n",
    "    \"RF\": rf_dm_test_data,\n",
    "    \"XGBoost\": xgb_dm_test_data,\n",
    "    # \"LSTM\": lstm_dm_test_data\n",
    "}\n",
    "\n",
    "# Prepare an empty DataFrame to store the results\n",
    "dm_table = pd.DataFrame(index=models.keys(), columns=models.keys())\n",
    "\n",
    "# Perform pairwise DM tests for the average of Calls and Puts\n",
    "for model1_name, model1_data in models.items():\n",
    "    for model2_name, model2_data in models.items():\n",
    "        if model1_name != model2_name:\n",
    "            # Ensure both dataframes have the same length (pad with zeros if needed)\n",
    "            max_length = max(len(model1_data), len(model2_data))\n",
    "            model1_data_padded = model1_data.reindex(range(max_length), fill_value=0)\n",
    "            model2_data_padded = model2_data.reindex(range(max_length), fill_value=0)\n",
    "            \n",
    "            # Calculate DM test for Call errors\n",
    "            call_errors1 = model1_data_padded['Call Errors']\n",
    "            call_errors2 = model2_data_padded['Call Errors']\n",
    "            dm_statistic_call = dm_test_statistic(call_errors1, call_errors2)\n",
    "            \n",
    "            # Calculate DM test for Put errors\n",
    "            put_errors1 = model1_data_padded['Put Errors']\n",
    "            put_errors2 = model2_data_padded['Put Errors']\n",
    "            dm_statistic_put = dm_test_statistic(put_errors1, put_errors2)\n",
    "            \n",
    "            # Calculate the average of the two DM test statistics\n",
    "            dm_statistic_avg = (dm_statistic_call + dm_statistic_put) / 2\n",
    "            \n",
    "            # Only store upper triangular results to avoid repetition\n",
    "            if model1_name in dm_table.columns and model2_name in dm_table.index:\n",
    "                dm_table.loc[model2_name, model1_name] = round(dm_statistic_avg, 2)\n",
    "\n",
    "# Display the resulting DM test table\n",
    "print(dm_table)\n",
    "\n",
    "# Optionally, save the DM test table to a CSV file for later use\n",
    "# dm_table.to_csv('/path/to/dm_test_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
