{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_nonscaled.parquet')\n",
    "\n",
    "data_test = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_nonscaled.parquet')\n",
    "\n",
    "firm_data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Firm_variables/daily_firm_data_median_new.parquet')\n",
    "\n",
    "# Top features\n",
    "top_features_c = ['cp_flag',\t'Ticker',\t'date',\t'moneyness',\t'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond']\n",
    "top_features_p = ['cp_flag',\t'Ticker',\t'date',\t'moneyness',\t'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock']\n",
    "\n",
    "# # List of columns to drop\n",
    "columns_to_drop = ['trading_days_till_exp'] + list(firm_data.columns[2:]) + ['moneyness_squared', 'tau_squared', 'moneyness_tau', 'best_offer_option', 'best_bid_option']\n",
    "# # columns_to_drop = ['trading_days_till_exp']\n",
    "\n",
    "\n",
    "# # Drop columns from datasets if they exist\n",
    "data_train = data_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "data_test = data_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure that data_val and data_test have the same column order as data_train\n",
    "data_test = data_test[data_train.columns]\n",
    "\n",
    "data_trains = data_train.copy()\n",
    "\n",
    "# data_train = data_trains[data_trains['date'] < '2020-01-01']\n",
    "# data_validate = data_trains[data_trains['date'] >= '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_flag</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>impl_volatility</th>\n",
       "      <th>T</th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>volume_option</th>\n",
       "      <th>spread_option</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>reces_indi</th>\n",
       "      <th>10Y_RIR</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>2Y_bond</th>\n",
       "      <th>OPEN_vix</th>\n",
       "      <th>HIGH_vix</th>\n",
       "      <th>LOW_vix</th>\n",
       "      <th>CLOSE_vix</th>\n",
       "      <th>spread_vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>-1.860</td>\n",
       "      <td>0.558181</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.505761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>786</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.5342</td>\n",
       "      <td>2.5442</td>\n",
       "      <td>31.72</td>\n",
       "      <td>31.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>28.57</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>-1.682</td>\n",
       "      <td>0.541292</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.495051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>982</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.5342</td>\n",
       "      <td>2.5442</td>\n",
       "      <td>31.72</td>\n",
       "      <td>31.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>28.57</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>-1.505</td>\n",
       "      <td>0.525111</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.484170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2614</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.5342</td>\n",
       "      <td>2.5442</td>\n",
       "      <td>31.72</td>\n",
       "      <td>31.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>28.57</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>-1.330</td>\n",
       "      <td>0.507597</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.473299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7308</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.5342</td>\n",
       "      <td>2.5442</td>\n",
       "      <td>31.72</td>\n",
       "      <td>31.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>28.57</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.468786</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.455607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11884</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>449.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.5342</td>\n",
       "      <td>2.5442</td>\n",
       "      <td>31.72</td>\n",
       "      <td>31.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>28.57</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145306</th>\n",
       "      <td>C</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.467</td>\n",
       "      <td>0.623205</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.554273</td>\n",
       "      <td>0.486818</td>\n",
       "      <td>1054</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>670.825806</td>\n",
       "      <td>3.042903</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.82</td>\n",
       "      <td>20.56</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145307</th>\n",
       "      <td>C</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.535</td>\n",
       "      <td>0.629719</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.554273</td>\n",
       "      <td>0.486818</td>\n",
       "      <td>25829</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>670.825806</td>\n",
       "      <td>3.042903</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.82</td>\n",
       "      <td>20.56</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145308</th>\n",
       "      <td>C</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.670</td>\n",
       "      <td>0.639880</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.584788</td>\n",
       "      <td>0.511182</td>\n",
       "      <td>1927</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>670.825806</td>\n",
       "      <td>3.042903</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.82</td>\n",
       "      <td>20.56</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145309</th>\n",
       "      <td>C</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.804</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.601543</td>\n",
       "      <td>0.523462</td>\n",
       "      <td>3447</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>670.825806</td>\n",
       "      <td>3.042903</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.82</td>\n",
       "      <td>20.56</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145310</th>\n",
       "      <td>C</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.936</td>\n",
       "      <td>0.659719</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.619541</td>\n",
       "      <td>0.537101</td>\n",
       "      <td>1862</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>670.825806</td>\n",
       "      <td>3.042903</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.1443</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.82</td>\n",
       "      <td>20.56</td>\n",
       "      <td>21.31</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145311 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_flag Ticker       date  moneyness  impl_volatility         T  \\\n",
       "0            P   AAPL 2019-01-02     -1.860         0.558181  0.011905   \n",
       "1            P   AAPL 2019-01-02     -1.682         0.541292  0.011905   \n",
       "2            P   AAPL 2019-01-02     -1.505         0.525111  0.011905   \n",
       "3            P   AAPL 2019-01-02     -1.330         0.507597  0.011905   \n",
       "4            P   AAPL 2019-01-02     -0.895         0.468786  0.011905   \n",
       "...        ...    ...        ...        ...              ...       ...   \n",
       "145306       C   TSLA 2020-12-31      1.467         0.623205  0.023810   \n",
       "145307       C   TSLA 2020-12-31      1.535         0.629719  0.023810   \n",
       "145308       C   TSLA 2020-12-31      1.670         0.639880  0.023810   \n",
       "145309       C   TSLA 2020-12-31      1.804         0.648805  0.023810   \n",
       "145310       C   TSLA 2020-12-31      1.936         0.659719  0.023810   \n",
       "\n",
       "        prev_day_iv  prev2_day_iv  volume_option  spread_option  ...  \\\n",
       "0          0.505761      0.000000            786           0.01  ...   \n",
       "1          0.495051      0.000000            982           0.01  ...   \n",
       "2          0.484170      0.000000           2614           0.01  ...   \n",
       "3          0.473299      0.000000           7308           0.01  ...   \n",
       "4          0.455607      0.000000          11884           0.02  ...   \n",
       "...             ...           ...            ...            ...  ...   \n",
       "145306     0.554273      0.486818           1054           0.10  ...   \n",
       "145307     0.554273      0.486818          25829           0.06  ...   \n",
       "145308     0.584788      0.511182           1927           0.04  ...   \n",
       "145309     0.601543      0.523462           3447           0.06  ...   \n",
       "145310     0.619541      0.537101           1862           0.06  ...   \n",
       "\n",
       "        gold_price  reces_indi  10Y_RIR  1Y_bond  2Y_bond  OPEN_vix  HIGH_vix  \\\n",
       "0       449.000000    0.130000     2.66   2.5342   2.5442     31.72     31.74   \n",
       "1       449.000000    0.130000     2.66   2.5342   2.5442     31.72     31.74   \n",
       "2       449.000000    0.130000     2.66   2.5342   2.5442     31.72     31.74   \n",
       "3       449.000000    0.130000     2.66   2.5342   2.5442     31.72     31.74   \n",
       "4       449.000000    0.130000     2.66   2.5342   2.5442     31.72     31.74   \n",
       "...            ...         ...      ...      ...      ...       ...       ...   \n",
       "145306  670.825806    3.042903     0.93   0.1105   0.1443     21.50     21.82   \n",
       "145307  670.825806    3.042903     0.93   0.1105   0.1443     21.50     21.82   \n",
       "145308  670.825806    3.042903     0.93   0.1105   0.1443     21.50     21.82   \n",
       "145309  670.825806    3.042903     0.93   0.1105   0.1443     21.50     21.82   \n",
       "145310  670.825806    3.042903     0.93   0.1105   0.1443     21.50     21.82   \n",
       "\n",
       "        LOW_vix  CLOSE_vix  spread_vix  \n",
       "0         28.57      28.57        3.17  \n",
       "1         28.57      28.57        3.17  \n",
       "2         28.57      28.57        3.17  \n",
       "3         28.57      28.57        3.17  \n",
       "4         28.57      28.57        3.17  \n",
       "...         ...        ...         ...  \n",
       "145306    20.56      21.31        1.26  \n",
       "145307    20.56      21.31        1.26  \n",
       "145308    20.56      21.31        1.26  \n",
       "145309    20.56      21.31        1.26  \n",
       "145310    20.56      21.31        1.26  \n",
       "\n",
       "[145311 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Assuming 'data_train' is already defined and loaded\n",
    "# data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "# data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# def prepare_data(data, option_type):\n",
    "#     \"\"\"\n",
    "#     Prepare the data and train Random Forest for a given dataset.\n",
    "#     Prints the in-sample RMSE and R².\n",
    "    \n",
    "#     Parameters:\n",
    "#     data (pd.DataFrame): The input dataset.\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train data\n",
    "#     X = data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Features\n",
    "#     y = data['impl_volatility']  # Target variable\n",
    "    \n",
    "#     # Initialize the RandomForestRegressor\n",
    "#     rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Train the model\n",
    "#     rf_model.fit(X, y)\n",
    "\n",
    "#     # In-sample (training set) predictions\n",
    "#     y_train_pred = rf_model.predict(X)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on Training Data)\n",
    "#     rmse_train = np.sqrt(mean_squared_error(y, y_train_pred))\n",
    "#     r2_train = r2_score(y, y_train_pred)\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f\"\\nIn-Sample Performance for {option_type} Options:\")\n",
    "#     print(f\"Random Forest RMSE (Training): {rmse_train:.4f}\")\n",
    "#     print(f\"Random Forest R² (Training): {r2_train:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data(data_train_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data(data_train_p, 'Put')\n",
    "\n",
    "\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "\n",
    "# y_test_c = data_test_c['impl_volatility']\n",
    "# y_test_p = data_test_p['impl_volatility']\n",
    "\n",
    "# X_test_c = data_test_c.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "# X_test_p = data_test_p.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = rf_model.predict(X_test_c)\n",
    "\n",
    "# # Evaluate model performance\n",
    "# rmse_rf = np.sqrt(mean_squared_error(y_test_c, y_pred))\n",
    "# r2_rf = r2_score(y_test_c, y_pred)\n",
    "\n",
    "# print(f\"Random Forest RMSE: {rmse_rf:.4f}\")\n",
    "# print(f\"Random Forest R²: {r2_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Assuming 'data_train' and 'data_test' are already defined and loaded\n",
    "\n",
    "# # Prepare train data for Call and Put options\n",
    "# data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "# data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "# def prepare_data(train_data, test_data, option_type):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, train Random Forest, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset.\n",
    "#     test_data (pd.DataFrame): The testing dataset.\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "#     y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "#     y_test = test_data['impl_volatility']  # Test target\n",
    "    \n",
    "#     # Initialize the RandomForestRegressor\n",
    "#     rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Train the model\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "\n",
    "#     # In-sample (training set) predictions\n",
    "#     y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "#     # Out-of-sample (test set) predictions\n",
    "#     y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on Training Data)\n",
    "#     rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "#     r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f\"\\nPerformance for {option_type} Options:\")\n",
    "#     print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "#     print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data(data_train_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Top features for Call and Put options\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'moneyness', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond']\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'moneyness', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock']\n",
    "\n",
    "top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond', 'spread_stock', 'spread_vix', 'moneyness','5_day_rolling_return_stock', 'RET','PRC_actual','CLOSE_vix','OPEN_vix','reces_indi' ]\n",
    "# top_features_c =['prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond', 'spread_stock', 'spread_vix', 'moneyness', '5_day_rolling_return_stock']\n",
    "# top_features_p =['prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '5_day_rolling_return_stock', 'RET', 'moneyness', 'PRC_actual', 'CLOSE_vix']\n",
    "\n",
    "# # Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C'][top_features_c]\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # # Prepare validation data for Call and Put options\n",
    "# # data_validate_c = data_validate[data_validate['cp_flag'] == 'C'][top_features_c]\n",
    "# # data_validate_p = data_validate[data_validate['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C'][top_features_c]\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P'][top_features_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_day_iv</th>\n",
       "      <th>T</th>\n",
       "      <th>vol_stock</th>\n",
       "      <th>prev2_day_iv</th>\n",
       "      <th>hi-lo_stock</th>\n",
       "      <th>1Y_bond</th>\n",
       "      <th>spread_stock</th>\n",
       "      <th>spread_vix</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>5_day_rolling_return_stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.337114</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>95485520.0</td>\n",
       "      <td>0.325650</td>\n",
       "      <td>2.59002</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.00999</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.013952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.332640</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>95485520.0</td>\n",
       "      <td>0.312515</td>\n",
       "      <td>2.59002</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.00999</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.013952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.330864</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>95485520.0</td>\n",
       "      <td>0.308907</td>\n",
       "      <td>2.59002</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.00999</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.013952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>95485520.0</td>\n",
       "      <td>0.306933</td>\n",
       "      <td>2.59002</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.00999</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.013952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.328318</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>95485520.0</td>\n",
       "      <td>0.305940</td>\n",
       "      <td>2.59002</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.00999</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.013952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>0.718058</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>20025526.0</td>\n",
       "      <td>0.773147</td>\n",
       "      <td>40.57984</td>\n",
       "      <td>0.4878</td>\n",
       "      <td>0.51990</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.556</td>\n",
       "      <td>0.209492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36348</th>\n",
       "      <td>0.735985</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>20025526.0</td>\n",
       "      <td>0.789361</td>\n",
       "      <td>40.57984</td>\n",
       "      <td>0.4878</td>\n",
       "      <td>0.51990</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.658</td>\n",
       "      <td>0.209492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36349</th>\n",
       "      <td>0.754168</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>20025526.0</td>\n",
       "      <td>0.807648</td>\n",
       "      <td>40.57984</td>\n",
       "      <td>0.4878</td>\n",
       "      <td>0.51990</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.760</td>\n",
       "      <td>0.209492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36350</th>\n",
       "      <td>0.773210</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>20025526.0</td>\n",
       "      <td>0.821686</td>\n",
       "      <td>40.57984</td>\n",
       "      <td>0.4878</td>\n",
       "      <td>0.51990</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.861</td>\n",
       "      <td>0.209492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>0.791207</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>20025526.0</td>\n",
       "      <td>0.838985</td>\n",
       "      <td>40.57984</td>\n",
       "      <td>0.4878</td>\n",
       "      <td>0.51990</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.209492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36352 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prev_day_iv         T   vol_stock  prev2_day_iv  hi-lo_stock  1Y_bond  \\\n",
       "0         0.337114  0.019841  95485520.0      0.325650      2.59002   0.1074   \n",
       "1         0.332640  0.019841  95485520.0      0.312515      2.59002   0.1074   \n",
       "2         0.330864  0.019841  95485520.0      0.308907      2.59002   0.1074   \n",
       "3         0.329431  0.019841  95485520.0      0.306933      2.59002   0.1074   \n",
       "4         0.328318  0.019841  95485520.0      0.305940      2.59002   0.1074   \n",
       "...            ...       ...         ...           ...          ...      ...   \n",
       "36347     0.718058  0.007937  20025526.0      0.773147     40.57984   0.4878   \n",
       "36348     0.735985  0.007937  20025526.0      0.789361     40.57984   0.4878   \n",
       "36349     0.754168  0.007937  20025526.0      0.807648     40.57984   0.4878   \n",
       "36350     0.773210  0.007937  20025526.0      0.821686     40.57984   0.4878   \n",
       "36351     0.791207  0.007937  20025526.0      0.838985     40.57984   0.4878   \n",
       "\n",
       "       spread_stock  spread_vix  moneyness  5_day_rolling_return_stock  \n",
       "0           0.00999        1.92     -0.475                    0.013952  \n",
       "1           0.00999        1.92     -0.345                    0.013952  \n",
       "2           0.00999        1.92     -0.216                    0.013952  \n",
       "3           0.00999        1.92     -0.088                    0.013952  \n",
       "4           0.00999        1.92      0.039                    0.013952  \n",
       "...             ...         ...        ...                         ...  \n",
       "36347       0.51990        2.04      1.556                    0.209492  \n",
       "36348       0.51990        2.04      1.658                    0.209492  \n",
       "36349       0.51990        2.04      1.760                    0.209492  \n",
       "36350       0.51990        2.04      1.861                    0.209492  \n",
       "36351       0.51990        2.04      1.961                    0.209492  \n",
       "\n",
       "[36352 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# # Prepare validation data for Call and Put options\n",
    "# data_validate_c = data_validate[data_validate['cp_flag'] == 'C'][top_features_c]\n",
    "# data_validate_p = data_validate[data_validate['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import itertools\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# def prepare_data_with_gridsearch(train_data, validate_data, test_data, option_type, verbose=True):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Prepare the data, train Random Forest by manually tuning hyperparameters with validation data, \n",
    "#     and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset (year 1).\n",
    "#     validate_data (pd.DataFrame): The validation dataset (year 2).\n",
    "#     test_data (pd.DataFrame): The testing dataset.\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     verbose (bool): If True, prints progress information for hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train, validation, and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "#     y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Validation features\n",
    "#     y_validate = validate_data['impl_volatility']  # Validation target\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "#     y_test = test_data['impl_volatility']  # Test target\n",
    "    \n",
    "#     # Define the hyperparameter grid\n",
    "#     param_grid = {\n",
    "#         'max_features': [5, 10, 15],                 # Use fewer features to reduce model complexity\n",
    "#         'n_estimators': [100, 200, 300],             # Increase number of trees for better generalization\n",
    "#         'min_samples_leaf': [5, 10, 15],             # Ensure minimum leaf size is larger to reduce overfitting\n",
    "#         'min_samples_split': [2, 5],                 # Ensure reasonable splits, prevent unnecessary splits\n",
    "#         'max_depth': [5, 10, 15, 20]                 # Limit the depth of trees to prevent overfitting\n",
    "#     }\n",
    "    \n",
    "#     # Generate all combinations of hyperparameters\n",
    "#     param_combinations = list(itertools.product(\n",
    "#         param_grid['max_features'], \n",
    "#         param_grid['n_estimators'], \n",
    "#         param_grid['min_samples_leaf'], \n",
    "#         param_grid['min_samples_split'], \n",
    "#         param_grid['max_depth']\n",
    "#     ))\n",
    "    \n",
    "#     total_combinations = len(param_combinations)\n",
    "    \n",
    "#     # Initialize variables to store the best model and best score\n",
    "#     best_rmse_val = np.inf\n",
    "#     best_params = None\n",
    "#     best_rf_model = None\n",
    "    \n",
    "#     print(f\"Running manual hyperparameter tuning for {option_type} Options...\")\n",
    "    \n",
    "#     # Iterate over all hyperparameter combinations with progress tracking\n",
    "#     for i, (max_features, n_estimators, min_samples_leaf, min_samples_split, max_depth) in enumerate(param_combinations):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Initialize the RandomForestRegressor with the current set of hyperparameters\n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             max_features=max_features, \n",
    "#             n_estimators=n_estimators,\n",
    "#             min_samples_leaf=min_samples_leaf,\n",
    "#             min_samples_split=min_samples_split,\n",
    "#             max_depth=max_depth\n",
    "#         )\n",
    "    \n",
    "#         # Train the model on the training data (Year 1)\n",
    "#         rf_model.fit(X_train, y_train)\n",
    "    \n",
    "#         # Validate the model on the validation data (Year 2)\n",
    "#         y_val_pred = rf_model.predict(X_validate)\n",
    "#         rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "    \n",
    "#         # Track the best performing hyperparameters based on validation RMSE\n",
    "#         if rmse_val < best_rmse_val:\n",
    "#             best_rmse_val = rmse_val\n",
    "#             best_params = {\n",
    "#                 'max_features': max_features,\n",
    "#                 'n_estimators': n_estimators,\n",
    "#                 'min_samples_leaf': min_samples_leaf,\n",
    "#                 'min_samples_split': min_samples_split,\n",
    "#                 'max_depth': max_depth\n",
    "#             }\n",
    "#             best_rf_model = rf_model\n",
    "    \n",
    "#         # Verbose output to track progress\n",
    "#         if verbose:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\"Combination {i + 1}/{total_combinations} completed in {elapsed_time:.2f} seconds.\")\n",
    "#             print(f\"Current RMSE (Validation): {rmse_val:.4f}\")\n",
    "#             print(f\"Best RMSE so far: {best_rmse_val:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nBest Parameters for {option_type} Options: {best_params}\")\n",
    "    \n",
    "#     # After finding the best hyperparameters, evaluate performance on the test data\n",
    "#     # In-sample (training set) predictions\n",
    "#     y_train_pred = best_rf_model.predict(X_train)\n",
    "    \n",
    "#     # Validate the predictions\n",
    "#     y_val_pred = best_rf_model.predict(X_validate)\n",
    "    \n",
    "#     # Out-of-sample (test set) predictions\n",
    "#     y_test_pred = best_rf_model.predict(X_test)\n",
    "    \n",
    "#     # Evaluate In-Sample Performance (on Training Data)\n",
    "#     rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "#     r2_train = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "#     # Evaluate Validation Set Performance\n",
    "#     rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "#     r2_val = r2_score(y_validate, y_val_pred)\n",
    "    \n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "#     # Print the results\n",
    "#     print(f\"\\nPerformance for {option_type} Options:\")\n",
    "#     print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "#     print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "#     print(f\"Validation RMSE: {rmse_val:.4f}\")\n",
    "#     print(f\"Validation R²: {r2_val:.4f}\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "# prepare_data_with_gridsearch(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# prepare_data_with_gridsearch(data_train_p, data_validate_p, data_test_p, 'Put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import itertools\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# def prepare_data_with_gridsearch(train_data, validate_data, test_data, option_type, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, perform hyperparameter tuning using Year 1 (train) and Year 2 (validation),\n",
    "#     retrain the model on Year 1 + Year 2, and evaluate on Year 3 (test).\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset (Year 1).\n",
    "#     validate_data (pd.DataFrame): The validation dataset (Year 2).\n",
    "#     test_data (pd.DataFrame): The testing dataset (Year 3).\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     verbose (bool): If True, prints progress information for hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train, validation, and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features (Year 1)\n",
    "#     y_train = train_data['impl_volatility']  # Training target (Year 1)\n",
    "\n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Validation features (Year 2)\n",
    "#     y_validate = validate_data['impl_volatility']  # Validation target (Year 2)\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features (Year 3)\n",
    "#     y_test = test_data['impl_volatility']  # Test target (Year 3)\n",
    "\n",
    "#     # Define the hyperparameter grid\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [50, 75, 100],             # Increase number of trees for better generalization\n",
    "#         'min_samples_leaf': [1,2, 4, 7],             # Ensure minimum leaf size is larger to reduce overfitting\n",
    "#         'min_samples_split': [2, 7, 12],                 # Ensure reasonable splits, prevent unnecessary splits\n",
    "#         'max_depth': [10, 20, None]                 # Limit the depth of trees to prevent overfitting\n",
    "#     }\n",
    "#     # Best params Call\n",
    "#     # param_grid = {'max_features': [20], 'n_estimators': [50], 'min_samples_leaf': [7], 'min_samples_split': [2], 'max_depth': [None]}\n",
    "\n",
    "#     # Best params Put\n",
    "\n",
    "\n",
    "#     # Generate all combinations of hyperparameters\n",
    "#     param_combinations = list(itertools.product(\n",
    "#         param_grid['n_estimators'], \n",
    "#         param_grid['min_samples_leaf'], \n",
    "#         param_grid['min_samples_split'], \n",
    "#         param_grid['max_depth']\n",
    "#     ))\n",
    "\n",
    "#     total_combinations = len(param_combinations)\n",
    "    \n",
    "#     # Initialize variables to store the best model and best score\n",
    "#     best_rmse_val = np.inf\n",
    "#     best_params = None\n",
    "#     best_rf_model = None\n",
    "\n",
    "#     print(f\"Running manual hyperparameter tuning for {option_type} Options...\")\n",
    "    \n",
    "#     # Iterate over all hyperparameter combinations with progress tracking\n",
    "#     for i, (n_estimators, min_samples_leaf, min_samples_split, max_depth) in enumerate(param_combinations):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Initialize the RandomForestRegressor with the current set of hyperparameters\n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             n_estimators=n_estimators,\n",
    "#             min_samples_leaf=min_samples_leaf,\n",
    "#             min_samples_split=min_samples_split,\n",
    "#             max_depth=max_depth\n",
    "#         )\n",
    "\n",
    "#         # Train the model on the training data (Year 1)\n",
    "#         rf_model.fit(X_train, y_train)\n",
    "\n",
    "#         # Validate the model on the validation data (Year 2)\n",
    "#         y_val_pred = rf_model.predict(X_validate)\n",
    "#         rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "\n",
    "#         # Track the best performing hyperparameters based on validation RMSE\n",
    "#         if rmse_val < best_rmse_val:\n",
    "#             best_rmse_val = rmse_val\n",
    "#             best_params = {\n",
    "#                 'n_estimators': n_estimators,\n",
    "#                 'min_samples_leaf': min_samples_leaf,\n",
    "#                 'min_samples_split': min_samples_split,\n",
    "#                 'max_depth': max_depth\n",
    "#             }\n",
    "#             best_rf_model = rf_model\n",
    "\n",
    "#         # Verbose output to track progress\n",
    "#         if verbose:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\"Combination {i + 1}/{total_combinations} completed in {elapsed_time:.2f} seconds.\")\n",
    "#             print(f\"Current RMSE (Validation): {rmse_val:.4f}\")\n",
    "#             print(f\"Best RMSE so far: {best_rmse_val:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nBest Parameters for {option_type} Options: {best_params}\")\n",
    "    \n",
    "#     # Retrain the model on combined Year 1 (train) and Year 2 (validation)\n",
    "#     print(\"Retraining the model on Year 1 and Year 2 combined...\")\n",
    "#     X_combined = pd.concat([X_train, X_validate])\n",
    "#     y_combined = pd.concat([y_train, y_validate])\n",
    "#     best_rf_model.fit(X_combined, y_combined)\n",
    "\n",
    "#     # In-sample (combined Year 1 + Year 2) predictions\n",
    "#     y_combined_pred = best_rf_model.predict(X_combined)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on combined Year 1 + Year 2)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(y_combined, y_combined_pred))\n",
    "#     r2_combined = r2_score(y_combined, y_combined_pred)\n",
    "    \n",
    "#     print(f\"\\nIn-Sample Performance for {option_type} Options (Year 1 + Year 2):\")\n",
    "#     print(f\"RMSE (Training + Validation): {rmse_combined:.4f}\")\n",
    "#     print(f\"R² (Training + Validation): {r2_combined:.4f}\")\n",
    "\n",
    "#     # After retraining, evaluate performance on the test data (Year 3)\n",
    "#     y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the final results\n",
    "#     print(f\"\\nPerformance on Test Data (Year 3) for {option_type} Options:\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch(data_train_p, data_validate_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import itertools\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import numpy as np\n",
    "\n",
    "# def prepare_data_with_gridsearch(train_data, validate_data, test_data, option_type, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, perform hyperparameter tuning using Year 1 (train) and Year 2 (validation),\n",
    "#     retrain the model on Year 1 + Year 2, and evaluate on Year 3 (test).\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset (Year 1).\n",
    "#     validate_data (pd.DataFrame): The validation dataset (Year 2).\n",
    "#     test_data (pd.DataFrame): The testing dataset (Year 3).\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     verbose (bool): If True, prints progress information for hyperparameter tuning.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train, validation, and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features (Year 1)\n",
    "#     y_train = train_data['impl_volatility']  # Training target (Year 1)\n",
    "\n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Validation features (Year 2)\n",
    "#     y_validate = validate_data['impl_volatility']  # Validation target (Year 2)\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features (Year 3)\n",
    "#     y_test = test_data['impl_volatility']  # Test target (Year 3)\n",
    "\n",
    "#     # Define the hyperparameter grid\n",
    "#     param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'min_samples_leaf': [10, 20, 30],\n",
    "#     'min_samples_split': [5, 10, 15],\n",
    "#     'max_depth': [5, 10, 15]  # Limit depth to avoid overfitting            # Limit the depth of trees to prevent overfitting\n",
    "#     }\n",
    "#     # Best params Call\n",
    "#     # param_grid = {'max_features': [20], 'n_estimators': [50], 'min_samples_leaf': [7], 'min_samples_split': [2], 'max_depth': [None]}\n",
    "\n",
    "#     # Best params Put\n",
    "\n",
    "\n",
    "#     # Generate all combinations of hyperparameters\n",
    "#     param_combinations = list(itertools.product(\n",
    "#         param_grid['n_estimators'], \n",
    "#         param_grid['min_samples_leaf'], \n",
    "#         param_grid['min_samples_split'], \n",
    "#         param_grid['max_depth']\n",
    "#     ))\n",
    "\n",
    "#     total_combinations = len(param_combinations)\n",
    "    \n",
    "#     # Initialize variables to store the best model and best score\n",
    "#     best_rmse_val = np.inf\n",
    "#     best_params = None\n",
    "#     best_rf_model = None\n",
    "\n",
    "#     print(f\"Running manual hyperparameter tuning for {option_type} Options...\")\n",
    "    \n",
    "#     # Iterate over all hyperparameter combinations with progress tracking\n",
    "#     for i, (n_estimators, min_samples_leaf, min_samples_split, max_depth) in enumerate(param_combinations):\n",
    "#         start_time = time.time()\n",
    "        \n",
    "#         # Initialize the RandomForestRegressor with the current set of hyperparameters\n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             n_estimators=n_estimators,\n",
    "#             min_samples_leaf=min_samples_leaf,\n",
    "#             min_samples_split=min_samples_split,\n",
    "#             max_depth=max_depth\n",
    "#         )\n",
    "\n",
    "#         # Train the model on the training data (Year 1)\n",
    "#         rf_model.fit(X_train, y_train)\n",
    "\n",
    "#         # Validate the model on the validation data (Year 2)\n",
    "#         y_val_pred = rf_model.predict(X_validate)\n",
    "#         rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "\n",
    "#         # Track the best performing hyperparameters based on validation RMSE\n",
    "#         if rmse_val < best_rmse_val:\n",
    "#             best_rmse_val = rmse_val\n",
    "#             best_params = {\n",
    "#                 'n_estimators': n_estimators,\n",
    "#                 'min_samples_leaf': min_samples_leaf,\n",
    "#                 'min_samples_split': min_samples_split,\n",
    "#                 'max_depth': max_depth\n",
    "#             }\n",
    "#             best_rf_model = rf_model\n",
    "\n",
    "#         # Verbose output to track progress\n",
    "#         if verbose:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\"Combination {i + 1}/{total_combinations} completed in {elapsed_time:.2f} seconds.\")\n",
    "#             print(f\"Current RMSE (Validation): {rmse_val:.4f}\")\n",
    "#             print(f\"Best RMSE so far: {best_rmse_val:.4f}\")\n",
    "    \n",
    "#     print(f\"\\nBest Parameters for {option_type} Options: {best_params}\")\n",
    "    \n",
    "#     # Retrain the model on combined Year 1 (train) and Year 2 (validation)\n",
    "#     print(\"Retraining the model on Year 1 and Year 2 combined...\")\n",
    "#     X_combined = pd.concat([X_train, X_validate])\n",
    "#     y_combined = pd.concat([y_train, y_validate])\n",
    "#     best_rf_model.fit(X_combined, y_combined)\n",
    "\n",
    "#     # In-sample (combined Year 1 + Year 2) predictions\n",
    "#     y_combined_pred = best_rf_model.predict(X_combined)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on combined Year 1 + Year 2)\n",
    "#     rmse_combined = np.sqrt(mean_squared_error(y_combined, y_combined_pred))\n",
    "#     r2_combined = r2_score(y_combined, y_combined_pred)\n",
    "    \n",
    "#     print(f\"\\nIn-Sample Performance for {option_type} Options (Year 1 + Year 2):\")\n",
    "#     print(f\"RMSE (Training + Validation): {rmse_combined:.4f}\")\n",
    "#     print(f\"R² (Training + Validation): {r2_combined:.4f}\")\n",
    "\n",
    "#     # After retraining, evaluate performance on the test data (Year 3)\n",
    "#     y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the final results\n",
    "#     print(f\"\\nPerformance on Test Data (Year 3) for {option_type} Options:\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch(data_train_p, data_validate_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data_with_gridsearch(train_data, validate_data, test_data, option_type):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, train Random Forest using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset.\n",
    "#     test_data (pd.DataFrame): The testing dataset.\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "#     y_train = train_data['impl_volatility']  # Training target\n",
    "\n",
    "#     # Prepare the validation data\n",
    "#     X_validate = validate_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "#     y_validate = validate_data['impl_volatility']  # Training target\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "#     y_test = test_data['impl_volatility']  # Test target\n",
    "    \n",
    "#     # # Define the parameter grid for GridSearchCV\n",
    "#     # param_grid = {\n",
    "#     #     'n_estimators': [100, 200],      # Number of trees in the forest\n",
    "#     #     'max_depth': [10, 20, None],     # Maximum depth of the tree\n",
    "#     #     'min_samples_split': [2, 5],     # Minimum number of samples required to split an internal node\n",
    "#     #     'min_samples_leaf': [1, 2],      # Minimum number of samples required to be at a leaf node\n",
    "#     #     'bootstrap': [True, False]       # Whether bootstrap samples are used when building trees\n",
    "#     # }\n",
    "\n",
    "#     param_grid = {\n",
    "#     'max_features': [5, 10, 15],                 # Use fewer features to reduce model complexity\n",
    "#     'n_estimators': [100, 200, 300],             # Increase number of trees for better generalization\n",
    "#     'min_samples_leaf': [5, 10, 15],             # Ensure minimum leaf size is larger to reduce overfitting\n",
    "#     'min_samples_split': [2, 5],                 # Ensure reasonable splits, prevent unnecessary splits\n",
    "#     'max_depth': [5, 10, 15, 20]                 # Limit the depth of trees to prevent overfitting\n",
    "#     }   \n",
    "\n",
    "#     # Initialize the RandomForestRegressor\n",
    "#     rf_model = RandomForestRegressor()\n",
    "\n",
    "#     # Initialize GridSearchCV\n",
    "#     grid_search = GridSearchCV(estimator=rf_model, param_grid=(param_grid), \n",
    "#                                cv=2, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "#     # Train the model using GridSearchCV\n",
    "#     print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Get the best parameters and the best estimator\n",
    "#     best_rf_model = grid_search.best_estimator_\n",
    "#     print(f\"\\nBest Parameters for {option_type} Options: {grid_search.best_params_}\")\n",
    "\n",
    "#     # In-sample (training set) predictions\n",
    "#     y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "#     # Validate the predictions\n",
    "#     y_val_pred = best_rf_model.predict(X_validate)\n",
    "\n",
    "#     # Out-of-sample (test set) predictions\n",
    "#     y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate In-Sample Performance (on Training Data)\n",
    "#     rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "#     r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "#     # Evaluate Validation Set Performance\n",
    "#     rmse_val = np.sqrt(mean_squared_error(y_validate, y_val_pred))\n",
    "#     r2_val = r2_score(y_validate, y_val_pred)\n",
    "\n",
    "#     # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#     rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#     r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f\"\\nPerformance for {option_type} Options:\")\n",
    "#     print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "#     print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "#     print(f\"Validation RMSE: {rmse_val:.4f}\")\n",
    "#     print(f\"Validation R²: {r2_val:.4f}\")\n",
    "#     print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#     print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch(data_train_c, data_validate_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch(data_train_p, data_validate_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Call Options...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  33.0s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  34.9s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  35.6s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  35.7s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  36.8s\n",
      "\n",
      "Best Parameters for Call Options: {'max_depth': 15, 'max_features': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "\n",
      "Performance for Call Options:\n",
      "In-Sample RMSE (Training): 0.0410\n",
      "In-Sample R² (Training): 0.9875\n",
      "Out-of-Sample RMSE (Test): 0.1356\n",
      "Out-of-Sample R² (Test): 0.6769\n",
      "Running GridSearchCV for Put Options...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  38.7s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  40.1s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  41.0s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  42.0s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  42.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m feature_importance_c \u001b[38;5;241m=\u001b[39m prepare_data_with_gridsearch(data_train_c, data_test_c, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Call the function for Put options data\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m feature_importance_p \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_with_gridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPut\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Average feature importance plot\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_average_feature_importance\u001b[39m(feature_importance_call, feature_importance_put, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "Cell \u001b[0;32mIn[10], line 51\u001b[0m, in \u001b[0;36mprepare_data_with_gridsearch\u001b[0;34m(train_data, test_data, option_type)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train the model using GridSearchCV\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning GridSearchCV for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Options...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Get the best parameters and the best estimator\u001b[39;00m\n\u001b[1;32m     54\u001b[0m best_rf_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1056\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1054\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/FinalThesisQF/FinalThesisQF/myenv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data_train' and 'data_test' are already defined and loaded\n",
    "\n",
    "# Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "def prepare_data_with_gridsearch(train_data, test_data, option_type):\n",
    "    \"\"\"\n",
    "    Prepare the data, train Random Forest using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): The training dataset.\n",
    "    test_data (pd.DataFrame): The testing dataset.\n",
    "    option_type (str): Call or Put option type for labeling the print output.\n",
    "    \n",
    "    Returns:\n",
    "    feature_importances (pd.Series): The importance of features from the best model.\n",
    "    \"\"\"\n",
    "    # Prepare the train and test data\n",
    "    X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "    y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "    X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "    y_test = test_data['impl_volatility']  # Test target\n",
    "\n",
    "    # Define the parameter grid for GridSearchCV for Call and Put separately\n",
    "    if option_type == 'Call':\n",
    "        param_grid = {'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [4], 'min_samples_split': [2], 'max_depth': [15]}  # Call\n",
    "    else:\n",
    "        param_grid = {'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [1], 'min_samples_split': [3], 'max_depth': [15]}  # Put\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=42, max_samples=0.8)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Train the model using GridSearchCV\n",
    "    print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and the best estimator\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    print(f\"\\nBest Parameters for {option_type} Options: {grid_search.best_params_}\")\n",
    "\n",
    "    # In-sample (training set) predictions\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Out-of-sample (test set) predictions\n",
    "    y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate In-Sample Performance (on Training Data)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\nPerformance for {option_type} Options:\")\n",
    "    print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "    print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "    print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "    print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "    # Extract and return feature importances\n",
    "    feature_importances = pd.Series(best_rf_model.feature_importances_, index=X_train.columns)\n",
    "    return feature_importances\n",
    "\n",
    "# Call the function for Call options data\n",
    "feature_importance_c = prepare_data_with_gridsearch(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# Call the function for Put options data\n",
    "feature_importance_p = prepare_data_with_gridsearch(data_train_p, data_test_p, 'Put')\n",
    "\n",
    "\n",
    "# Average feature importance plot\n",
    "def plot_average_feature_importance(feature_importance_call, feature_importance_put, top_n=5):\n",
    "    \"\"\"\n",
    "    Create a bar plot showing the top N average feature importance of both Call and Put options.\n",
    "\n",
    "    Parameters:\n",
    "    feature_importance_call (pd.Series): Feature importance for Call options.\n",
    "    feature_importance_put (pd.Series): Feature importance for Put options.\n",
    "    top_n (int): Number of top features to plot.\n",
    "    \"\"\"\n",
    "    # Combine the importance of features from both Call and Put\n",
    "    combined_importance = pd.concat([feature_importance_call, feature_importance_put], axis=1, keys=[\"Call\", \"Put\"]).fillna(0)\n",
    "\n",
    "    # Calculate the average importance\n",
    "    combined_importance[\"Average\"] = combined_importance.mean(axis=1)\n",
    "\n",
    "    # Select the top N features based on average importance\n",
    "    top_features = combined_importance[\"Average\"].sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    # Plot the top N average feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = top_features.sort_values(ascending=True).plot(kind='barh', color='blue', alpha=0.55, edgecolor='black', linewidth=1)\n",
    "    plt.grid(True)  # Add grid\n",
    "    ax.set_title('RF (Put and Call)')\n",
    "    # ax.set_xlabel('Importance')  # Add x-label\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Plot the top 5 features from the 15 chosen\n",
    "# plot_average_feature_importance(feature_importance_c, feature_importance_p, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Put Options...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(37883) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  37.6s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  39.8s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  39.9s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  40.9s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  42.2s\n",
      "\n",
      "Best Parameters for Put Options: {'max_depth': 15, 'max_features': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "\n",
      "Performance for Put Options:\n",
      "In-Sample RMSE (Training): 0.0452\n",
      "In-Sample R² (Training): 0.9868\n",
      "Out-of-Sample RMSE (Test): 0.1312\n",
      "Out-of-Sample R² (Test): 0.7172\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "# Assuming 'data_train' and 'data_test' are already defined and loaded\n",
    "\n",
    "# Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "def prepare_data_with_gridsearch(train_data, test_data, option_type):\n",
    "    \"\"\"\n",
    "    Prepare the data, train Random Forest using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): The training dataset.\n",
    "    test_data (pd.DataFrame): The testing dataset.\n",
    "    option_type (str): Call or Put option type for labeling the print output.\n",
    "    \"\"\"\n",
    "    # Prepare the train and test data\n",
    "    X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "    y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "    X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "    y_test = test_data['impl_volatility']  # Test target\n",
    "    \n",
    "    # # Define the parameter grid for GridSearchCV\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 200],      # Number of trees in the forest\n",
    "    #     'max_depth': [10, 20, None],     # Maximum depth of the tree\n",
    "    #     'min_samples_split': [2, 5],     # Minimum number of samples required to split an internal node\n",
    "    #     'min_samples_leaf': [1, 2],      # Minimum number of samples required to be at a leaf node\n",
    "    #     'bootstrap': [True, False]       # Whether bootstrap samples are used when building trees\n",
    "    # }\n",
    "\n",
    "    # Hyperparameters Best\n",
    "    # param_grid = {\n",
    "    #     'max_features': [5,10,15, 20, 25, 30],\n",
    "    #     'max_depth': [3,4,5, 6, 7 ,8, 10],\n",
    "    #     'n_estimators': [50, 100,150, 200,300, 400, 500, 600, 700],\n",
    "    # } np.linspace(0, 1, 10)\n",
    "\n",
    "    # param_grid = {\n",
    "    # # 'n_estimators': [50, 75, 100, 200, 300],\n",
    "    # # 'min_samples_leaf': [2, 7, 12, 20],\n",
    "    # # 'min_samples_split': [1, 2,4, 5, 10, 15],\n",
    "    # # 'max_depth': [10, 15, 20, None]  # Limit depth to avoid overfitting\n",
    "\n",
    "    # 'max_features': [15],\n",
    "    # 'n_estimators': [50, 75, 100],\n",
    "    # 'min_samples_leaf': [1,2, 4],\n",
    "    # 'min_samples_split': [2, 7, 12],\n",
    "    # 'max_depth': [10, 20, None]  # Limit depth to avoid overfitting\n",
    "    # }  \n",
    "\n",
    "    param_grid = { 'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [4], 'min_samples_split': [2], 'max_depth': [15]} #Put\n",
    "    param_grid = { 'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [1], 'min_samples_split': [3], 'max_depth': [15]} #Call\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=42, max_samples=0.8)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=(param_grid), \n",
    "                               cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Train the model using GridSearchCV\n",
    "    print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and the best estimator\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    print(f\"\\nBest Parameters for {option_type} Options: {grid_search.best_params_}\")\n",
    "\n",
    "    # In-sample (training set) predictions\n",
    "    y_train_pred = best_rf_model.predict(X_train)\n",
    "\n",
    "    # Out-of-sample (test set) predictions\n",
    "    y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate In-Sample Performance (on Training Data)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\nPerformance for {option_type} Options:\")\n",
    "    print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "    print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "    print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "    print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# Call the function for Call options data\n",
    "prepare_data_with_gridsearch(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# Call the function for Put options data\n",
    "prepare_data_with_gridsearch(data_train_p, data_test_p, 'Put')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Call Options...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  35.2s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  37.3s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  37.6s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  38.0s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=1, min_samples_split=3, n_estimators=150; total time=  39.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18941/2311389172.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_std_error = ols_model.bse[0]  # Newey-West standard error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Options Newey-West Standard Error: 0.0010\n",
      "Running GridSearchCV for Put Options...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  37.5s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  39.6s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  40.1s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  41.2s\n",
      "[CV] END max_depth=15, max_features=15, min_samples_leaf=4, min_samples_split=2, n_estimators=150; total time=  42.2s\n",
      "Put Options Newey-West Standard Error: 0.0010\n",
      "Test errors and Newey-West statistics saved to /Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_rf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/wn87d6495j1_vytb8m33q3mm0000gn/T/ipykernel_18941/2311389172.py:33: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  nw_std_error = ols_model.bse[0]  # Newey-West standard error\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "\n",
    "# Function to calculate Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West standard error for the series of prediction errors.\n",
    "    \n",
    "    Parameters:\n",
    "    - errors: Array-like of prediction errors\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    - Newey-West standard error of the prediction errors\n",
    "    \"\"\"\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    nw_std_error = ols_model.bse[0]  # Newey-West standard error\n",
    "    return nw_std_error\n",
    "\n",
    "# Function to prepare data, train Random Forest using GridSearchCV, and evaluate performance\n",
    "def prepare_data_with_gridsearch(train_data, test_data, option_type):\n",
    "    \"\"\"\n",
    "    Prepare the data, train Random Forest using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): The training dataset.\n",
    "    test_data (pd.DataFrame): The testing dataset.\n",
    "    option_type (str): Call or Put option type for labeling the print output.\n",
    "    \n",
    "    Returns:\n",
    "    y_test (pd.Series): True values of test data for implied volatility\n",
    "    y_test_pred (np.array): Predicted values of implied volatility for test data\n",
    "    nw_std_error (float): Newey-West standard error for test errors\n",
    "    \"\"\"\n",
    "    # Prepare the train and test data\n",
    "    X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "    y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "    X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "    y_test = test_data['impl_volatility']  # Test target\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    param_grid = { 'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [4], 'min_samples_split': [2], 'max_depth': [15]} if option_type == 'Put' else \\\n",
    "                 { 'max_features': [15], 'n_estimators': [150], 'min_samples_leaf': [1], 'min_samples_split': [3], 'max_depth': [15]}  # Call\n",
    "\n",
    "    # Initialize the RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=42, max_samples=0.8)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                               cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Train the model using GridSearchCV\n",
    "    print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "    # Out-of-sample (test set) predictions\n",
    "    y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate test errors and Newey-West standard error\n",
    "    errors_test = y_test - y_test_pred\n",
    "    nw_std_error = newey_west_standard_error(errors_test, lag=1)\n",
    "\n",
    "    print(f\"{option_type} Options Newey-West Standard Error: {nw_std_error:.4f}\")\n",
    "\n",
    "    return y_test, y_test_pred, nw_std_error\n",
    "\n",
    "# Call the function for Call options data\n",
    "y_test_c, y_test_pred_c, calls_nw_std_error = prepare_data_with_gridsearch(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# Call the function for Put options data\n",
    "y_test_p, y_test_pred_p, puts_nw_std_error = prepare_data_with_gridsearch(data_train_p, data_test_p, 'Put')\n",
    "\n",
    "# Calculate errors for Calls and Puts\n",
    "calls_errors = y_test_c - y_test_pred_c\n",
    "puts_errors = y_test_p - y_test_pred_p\n",
    "\n",
    "# Pad the shorter error list with zeros to ensure both lists have the same length\n",
    "max_length = max(len(calls_errors), len(puts_errors))\n",
    "\n",
    "puts_errors_padded = np.pad(puts_errors, (0, max_length - len(puts_errors)), 'constant', constant_values=0)\n",
    "calls_errors_padded = np.pad(calls_errors, (0, max_length - len(calls_errors)), 'constant', constant_values=0)\n",
    "\n",
    "# Save the results to a DataFrame and CSV\n",
    "dm_test_data = pd.DataFrame({\n",
    "    'Put Errors': puts_errors_padded,\n",
    "    'Call Errors': calls_errors_padded,\n",
    "    'Put Newey-West Std Error': [puts_nw_std_error] * max_length,  # Constant value for all rows\n",
    "    'Call Newey-West Std Error': [calls_nw_std_error] * max_length  # Constant value for all rows\n",
    "})\n",
    "\n",
    "# Specify the file path for the CSV\n",
    "file_path = '/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/performence_evaluation/dm_test_errors_rf.csv'\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "dm_test_data.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Test errors and Newey-West statistics saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Assuming 'data_train' and 'data_test' are already defined and loaded\n",
    "\n",
    "# # Prepare train data for Call and Put options\n",
    "# data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "# data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P']\n",
    "\n",
    "# def prepare_data_with_gridsearch(train_data, test_data, option_type):\n",
    "#     \"\"\"\n",
    "#     Prepare the data, train Random Forest using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "#     Additionally, select the best models with in-sample R² closest to 0.80 and 0.85.\n",
    "    \n",
    "#     Parameters:\n",
    "#     train_data (pd.DataFrame): The training dataset.\n",
    "#     test_data (pd.DataFrame): The testing dataset.\n",
    "#     option_type (str): Call or Put option type for labeling the print output.\n",
    "#     \"\"\"\n",
    "#     # Prepare the train and test data\n",
    "#     X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "#     y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "#     X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "#     y_test = test_data['impl_volatility']  # Test target\n",
    "    \n",
    "#     # Define the parameter grid for GridSearchCV\n",
    "#     param_grid = {\n",
    "#         'max_features': [15, 20, 25],                 # Use fewer features to reduce model complexity\n",
    "#         'n_estimators': [100, 200, 300],             # Increase number of trees for better generalization\n",
    "#         'min_samples_leaf': [5, 10, 15],             # Ensure minimum leaf size is larger to reduce overfitting\n",
    "#         'min_samples_split': [2, 5],                 # Ensure reasonable splits, prevent unnecessary splits\n",
    "#         'max_depth': [15, None]                 # Limit the depth of trees to prevent overfitting\n",
    "#     }    \n",
    "\n",
    "#     # Initialize the RandomForestRegressor\n",
    "#     rf_model = RandomForestRegressor()\n",
    "\n",
    "#     # Initialize GridSearchCV\n",
    "#     grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "#                                cv=2, n_jobs=-1, verbose=2)\n",
    "\n",
    "#     # Train the model using GridSearchCV\n",
    "#     print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Initialize tracking variables for the models closest to r² = 0.80 and r² = 0.85\n",
    "#     closest_to_80 = {'model': None, 'params': None, 'r2_train': float('inf')}\n",
    "#     closest_to_85 = {'model': None, 'params': None, 'r2_train': float('inf')}\n",
    "\n",
    "#     # Iterate over the results and select the best models with in-sample R² closest to 0.80 and 0.85\n",
    "#     for idx, params in enumerate(grid_search.cv_results_['params']):\n",
    "#         # Get the estimator with the current parameters\n",
    "#         model = grid_search.estimator.set_params(**params)\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # In-sample (training set) predictions\n",
    "#         y_train_pred = model.predict(X_train)\n",
    "#         r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "#         # Track the model closest to r² = 0.80\n",
    "#         if abs(r2_train - 0.80) < abs(closest_to_80['r2_train'] - 0.80):\n",
    "#             closest_to_80['model'] = model\n",
    "#             closest_to_80['params'] = params\n",
    "#             closest_to_80['r2_train'] = r2_train\n",
    "        \n",
    "#         # Track the model closest to r² = 0.85\n",
    "#         if abs(r2_train - 0.85) < abs(closest_to_85['r2_train'] - 0.85):\n",
    "#             closest_to_85['model'] = model\n",
    "#             closest_to_85['params'] = params\n",
    "#             closest_to_85['r2_train'] = r2_train\n",
    "\n",
    "#     # Evaluate models closest to r² = 0.80 and r² = 0.85\n",
    "#     for target_r2, model_info in zip([0.80, 0.85], [closest_to_80, closest_to_85]):\n",
    "#         if model_info['model'] is None:\n",
    "#             print(f\"\\nNo suitable model found for {option_type} Options with in-sample R² closest to {target_r2}.\")\n",
    "#             continue\n",
    "        \n",
    "#         # Print the best parameters\n",
    "#         print(f\"\\nBest Parameters for {option_type} Options (R² closest to {target_r2}): {model_info['params']}\")\n",
    "#         print(f\"In-Sample R² (Training): {model_info['r2_train']:.4f}\")\n",
    "\n",
    "#         # Out-of-sample (test set) predictions\n",
    "#         y_test_pred = model_info['model'].predict(X_test)\n",
    "\n",
    "#         # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "#         rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "#         r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "#         # Print the results\n",
    "#         print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "#         print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "\n",
    "# # Call the function for Call options data\n",
    "# prepare_data_with_gridsearch(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# # Call the function for Put options data\n",
    "# prepare_data_with_gridsearch(data_train_p, data_test_p, 'Put')\n",
    "\n",
    "# print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
