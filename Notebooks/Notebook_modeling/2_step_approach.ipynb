{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load in the specific data needed\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import random\n",
    "import datetime\n",
    "import hvplot.polars\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.interpolate import bisplrep, bisplev\n",
    "from datetime import timedelta\n",
    "from patsy import dmatrix\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_train_val_tech_standardized_filter.parquet')\n",
    "datat = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_test_tech_scaled_all_sso_filter.parquet')\n",
    "\n",
    "# data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_train_tech_nonscaled.parquet')\n",
    "# datat = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_test_tech_nonscaled.parquet')\n",
    "\n",
    "impl_volatility = 'impl_volatility_h_1_step'\n",
    "target = 'impl_volatility_h_1_step'\n",
    "\n",
    "# data_sso = pd.read_parquet('//Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_train_val_tech_scaled_sso.parquet')\n",
    "# datat_sso = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Modeling/data_set_test_tech_scaled_sso.parquet')\n",
    "\n",
    "# Filter data to ensure 'prc_option' is below a certain number, e.g., 0.5\n",
    "# prc_filter = 0.125\n",
    "\n",
    "# data = data[data['prc_option'] > prc_filter]\n",
    "# datat = datat[datat['prc_option'] > prc_filter]\n",
    "# data_sso = data_sso[data_sso['prc_option'] > prc_filter]\n",
    "# datat_sso = datat_sso[datat_sso['prc_option'] > prc_filter]\n",
    "\n",
    "\n",
    "test_data = datat[datat['date'] >= '2021-01-01']\n",
    "# total_train = data[data['date'] < '2021-01-01']\n",
    "\n",
    "basic = ['moneyness', 'T', 'moneyness_squared', 'tau_squared', 'moneyness_tau']\n",
    "macro_features = ['1Y_bond','2Y_bond','FF_rate', 'gold_price','reces_indi','10Y_RIR', 'CLOSE_vix', 'hi-lo_vix']\n",
    "tickers = ['ticker_AAPL', 'ticker_AMZN', 'ticker_META', 'ticker_MSFT','ticker_NVDA', 'ticker_TSLA'  ]\n",
    "stock_specific = ['BID', 'ASK', 'daily_return_indicator_stock', '5_day_rolling_return_stock', 'hi-lo_stock', ]\n",
    "option_specific = ['spread_option', 'volume', 'prc_option', 'previous_iv', 'previous_iv_all']\n",
    "\n",
    "totalList_ahbs = basic\n",
    "totalList = ['moneyness', 'T'] + macro_features + tickers + stock_specific + option_specific\n",
    "# totalList_nn = basic + ['previous_iv',\n",
    "#  'spread_option',\n",
    "#  '2Y_bond',\n",
    "#  'RET',\n",
    "#  'cp_flag',\n",
    "#  'ticker_TSLA',\n",
    "#  'ticker_AMZN']\n",
    "totalList_nn = basic + ['previous_iv', 'cp_flag', 'ticker_TSLA', 'ticker_AMZN']\n",
    "# totalList_xgb = ['T', 'moneyness', 'previous_iv', 'cp_flag', 'ticker_TSLA']\n",
    "totalList_xgb = basic + ['previous_iv', 'cp_flag', 'ticker_TSLA', 'ticker_AMZN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moneyness',\n",
       " 'T',\n",
       " 'moneyness_squared',\n",
       " 'tau_squared',\n",
       " 'moneyness_tau',\n",
       " 'previous_iv',\n",
       " 'cp_flag',\n",
       " 'ticker_TSLA',\n",
       " 'ticker_AMZN']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalList_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moneyness',\n",
       " 'T',\n",
       " '1Y_bond',\n",
       " '2Y_bond',\n",
       " 'FF_rate',\n",
       " 'gold_price',\n",
       " 'reces_indi',\n",
       " '10Y_RIR',\n",
       " 'CLOSE_vix',\n",
       " 'hi-lo_vix',\n",
       " 'ticker_AAPL',\n",
       " 'ticker_AMZN',\n",
       " 'ticker_META',\n",
       " 'ticker_MSFT',\n",
       " 'ticker_NVDA',\n",
       " 'ticker_TSLA',\n",
       " 'BID',\n",
       " 'ASK',\n",
       " 'daily_return_indicator_stock',\n",
       " '5_day_rolling_return_stock',\n",
       " 'hi-lo_stock',\n",
       " 'spread_option',\n",
       " 'volume',\n",
       " 'prc_option',\n",
       " 'previous_iv',\n",
       " 'previous_iv_all']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Training ADHBS model...\n",
      "Step 2: Training Neural Network on ADHBS errors...\n",
      "Epoch 1/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312us/step - loss: 0.0520 - mse: 0.0520\n",
      "Epoch 2/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 280us/step - loss: 0.0320 - mse: 0.0320\n",
      "Epoch 3/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281us/step - loss: 0.0301 - mse: 0.0301\n",
      "Epoch 4/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0294 - mse: 0.0294\n",
      "Epoch 5/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step - loss: 0.0291 - mse: 0.0291\n",
      "Epoch 6/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step - loss: 0.0289 - mse: 0.0289\n",
      "Epoch 7/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0287 - mse: 0.0287\n",
      "Epoch 8/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281us/step - loss: 0.0286 - mse: 0.0286\n",
      "Epoch 9/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 276us/step - loss: 0.0286 - mse: 0.0286\n",
      "Epoch 10/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 282us/step - loss: 0.0285 - mse: 0.0285\n",
      "Epoch 11/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step - loss: 0.0284 - mse: 0.0284\n",
      "Epoch 12/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step - loss: 0.0283 - mse: 0.0283\n",
      "Epoch 13/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 294us/step - loss: 0.0282 - mse: 0.0282\n",
      "Epoch 14/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0282 - mse: 0.0282\n",
      "Epoch 15/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 254us/step - loss: 0.0282 - mse: 0.0282\n",
      "Epoch 16/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0281 - mse: 0.0281\n",
      "Epoch 17/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0281 - mse: 0.0281\n",
      "Epoch 18/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 299us/step - loss: 0.0281 - mse: 0.0281\n",
      "Epoch 19/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step - loss: 0.0280 - mse: 0.0280\n",
      "Epoch 20/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step - loss: 0.0280 - mse: 0.0280\n",
      "Epoch 21/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0280 - mse: 0.0280\n",
      "Epoch 22/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262us/step - loss: 0.0279 - mse: 0.0279\n",
      "Epoch 23/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0279 - mse: 0.0279\n",
      "Epoch 24/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 277us/step - loss: 0.0279 - mse: 0.0279\n",
      "Epoch 25/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0279 - mse: 0.0279\n",
      "Epoch 26/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 27/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 28/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 29/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 285us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 30/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 31/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 290us/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 32/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 33/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 34/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 35/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 36/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 300us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 37/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 257us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 38/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 269us/step - loss: 0.0277 - mse: 0.0277\n",
      "Epoch 39/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 40/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 41/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 285us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 42/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 43/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 44/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 285us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 45/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 267us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 46/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 47/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 256us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 48/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 49/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 283us/step - loss: 0.0276 - mse: 0.0276\n",
      "Epoch 50/50\n",
      "\u001b[1m5405/5405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268us/step - loss: 0.0276 - mse: 0.0276\n",
      "\u001b[1m3139/3139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200us/step\n",
      "Two-step model - RMSE: 0.1332, R²: 0.8598\n",
      "Newey-West Standard Error (Based on Daily Averages): 0.004557272017550339\n",
      "Results saved to /Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_nn.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    return ols_model.bse[0]  # Standard error of the constant term\n",
    "\n",
    "# Function to compute Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West standard error for the series of prediction errors.\n",
    "    Parameters:\n",
    "    - errors: Array-like of prediction errors\n",
    "    - lag: Maximum lag to use for the Newey-West estimator (default is 1)\n",
    "    Returns:\n",
    "    - Newey-West standard error of the prediction errors\n",
    "    \"\"\"\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    return ols_model.bse[0]  # Standard error of the constant term\n",
    "\n",
    "# Two-step approach with Newey-West error calculation\n",
    "def two_step_model_with_newey_west(train_data, test_data, adhbs_features, nn_features, target, output_csv_path):\n",
    "    \"\"\"\n",
    "    Execute the two-step modeling process (ADHBS + NN) with Newey-West standard error computation.\n",
    "    Parameters:\n",
    "    - train_data: Training dataset\n",
    "    - test_data: Testing dataset\n",
    "    - adhbs_features: Features for the ADHBS model\n",
    "    - nn_features: Features for the Neural Network\n",
    "    - target: Target variable name\n",
    "    - output_csv_path: Path to save the results CSV\n",
    "    \"\"\"\n",
    "    # Step 1: ADHBS Model\n",
    "    print(\"Step 1: Training ADHBS model...\")\n",
    "    y_train = train_data[target]\n",
    "    X_train_adhbs = sm.add_constant(train_data[adhbs_features])\n",
    "    adhbs_model = sm.OLS(y_train, X_train_adhbs).fit()\n",
    "\n",
    "    train_data['ADHBS_Predictions'] = adhbs_model.predict(X_train_adhbs)\n",
    "    train_data['Errors'] = train_data[target] - train_data['ADHBS_Predictions']\n",
    "\n",
    "    # Step 1.1: ADHBS Predictions for test data\n",
    "    X_test_adhbs = sm.add_constant(test_data[adhbs_features])\n",
    "    test_data['ADHBS_Predictions'] = adhbs_model.predict(X_test_adhbs)\n",
    "    test_data['Errors'] = test_data[target] - test_data['ADHBS_Predictions']\n",
    "\n",
    "    # Step 2: Neural Network to correct errors\n",
    "    print(\"Step 2: Training Neural Network on ADHBS errors...\")\n",
    "    nn_model = create_nn_model(input_dim=len(nn_features))\n",
    "    nn_model.fit(train_data[nn_features].values, train_data['Errors'].values, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "    test_data['NN_Corrections'] = nn_model.predict(test_data[nn_features].values).flatten()\n",
    "    test_data['Final_Predictions'] = test_data['ADHBS_Predictions'] + test_data['NN_Corrections']\n",
    "\n",
    "    # Compute performance metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_data[target], test_data['Final_Predictions']))\n",
    "    r2 = r2_score(test_data[target], test_data['Final_Predictions'])\n",
    "    print(f\"Two-step model - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Compute Newey-West standard error\n",
    "    test_data['Prediction_Errors'] = test_data[target] - test_data['Final_Predictions']\n",
    "    daily_avg_errors = test_data.groupby('date')['Prediction_Errors'].mean()\n",
    "    nw_std_error = newey_west_standard_error(daily_avg_errors.values, lag=1)\n",
    "    print(f\"Newey-West Standard Error (Based on Daily Averages): {nw_std_error}\")\n",
    "\n",
    "    # Save the results to a CSV\n",
    "    result_df = pd.DataFrame({\n",
    "        'Date': daily_avg_errors.index,\n",
    "        'Daily Avg Errors': daily_avg_errors.values,\n",
    "        'Newey-West Std Error': [nw_std_error] * len(daily_avg_errors)\n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return test_data, nw_std_error\n",
    "\n",
    "# # Step 1: ADHBS Model\n",
    "def adhbs_model(data, features, target, title):\n",
    "    y = data[target]\n",
    "    X = sm.add_constant(data[features])  # Add constant for intercept\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Predictions and error calculations\n",
    "    data['Predictions'] = model.predict(X)\n",
    "    data['Errors'] = y - data['Predictions']\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y, data['Predictions']))\n",
    "    r2 = r2_score(y, data['Predictions'])\n",
    "    print(f\"{title} RMSE: {rmse}, R²: {r2}\")\n",
    "\n",
    "    # Feature importance\n",
    "    coefs = model.params.abs()\n",
    "    importance = coefs / coefs.sum()\n",
    "    top_features = importance.sort_values(ascending=False).head(10)\n",
    "\n",
    "    return model, data['Errors']\n",
    "\n",
    "# Step 2: Neural Network to Correct Errors\n",
    "def create_nn_model(input_dim, neurons=32, layers=1, dropout_rate=0):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "    for layer in range(layers):\n",
    "        model.add(tf.keras.layers.Dense(max(1, neurons // (2 ** layer)), activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def train_nn(data, features, target, epochs=50, batch_size=32):\n",
    "\n",
    "    X = data[features].values  # Use features directly without additional scaling\n",
    "    y = data[target].values  # Target values\n",
    "\n",
    "    # Create and train the neural network\n",
    "    model = create_nn_model(X.shape[1], neurons=32, layers=1, dropout_rate=0.05)\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Make predictions and evaluate performance\n",
    "    predictions = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2 = r2_score(y, predictions)\n",
    "\n",
    "    print(f\"NN Model RMSE: {rmse}, R²: {r2}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "def two_step_model_oos(train_data, test_data, totalList_ahbs, nn_features, target):\n",
    "    \"\"\"\n",
    "    Execute the two-step modeling process (ADHBS + NN) for out-of-sample predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataset (in-sample data).\n",
    "    - test_data: Testing dataset (out-of-sample data).\n",
    "    - adhbs_features: Features for the ADHBS model.\n",
    "    - nn_features: Features for the Neural Network.\n",
    "    - target: Target column name (implied volatility in this case).\n",
    "    \n",
    "    Returns:\n",
    "    - Final OOS predictions.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Running ADHBS Model on Training Data...\")\n",
    "    adhbs_model_trained, train_errors = adhbs_model(train_data, totalList_ahbs, target, \"ADHBS Model (Train)\")\n",
    "\n",
    "    # Predict on the test data using the trained ADHBS model\n",
    "    print(\"Step 1.1: Generating ADHBS Predictions for Test Data...\")\n",
    "    X_test = sm.add_constant(test_data[totalList_ahbs])\n",
    "    test_data['ADHBS_Predictions'] = adhbs_model_trained.predict(X_test)\n",
    "\n",
    "    # Calculate residuals for the test data\n",
    "    test_data['Errors'] = test_data[target] - test_data['ADHBS_Predictions']\n",
    "\n",
    "    print(\"Step 2: Training Neural Network on Training Data Errors...\")\n",
    "    train_data['Errors'] = train_errors\n",
    "    nn_model = train_nn(train_data, nn_features, 'Errors')  # Train NN on training data errors\n",
    "\n",
    "    print(\"Step 2.1: Generating NN Corrections for Test Data...\")\n",
    "    X_nn_test = test_data[nn_features].values\n",
    "    test_data['NN_Corrections'] = nn_model.predict(X_nn_test).flatten()\n",
    "\n",
    "    # Combine ADHBS predictions and NN corrections for final OOS predictions\n",
    "    test_data['Final_Predictions'] = test_data['ADHBS_Predictions'] + test_data['NN_Corrections']\n",
    "\n",
    "    # Evaluate performance on the out-of-sample data\n",
    "    rmse_oos = np.sqrt(mean_squared_error(test_data[target], test_data['Final_Predictions']))\n",
    "    r2_oos = r2_score(test_data[target], test_data['Final_Predictions'])\n",
    "\n",
    "    print(f\"Out-of-Sample RMSE: {rmse_oos:.4f}\")\n",
    "    print(f\"Out-of-Sample R²: {r2_oos:.4f}\")\n",
    "\n",
    "    return test_data[['date', 'Ticker', target, 'Final_Predictions', 'ADHBS_Predictions', 'NN_Corrections', 'Errors']], nn_model\n",
    "\n",
    "output_csv_path = '/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_nn.csv'\n",
    "\n",
    "# oos_results, final_nn_model = two_step_model_oos(data, datat, totalList_ahbs, totalList_nn, 'impl_volatility')\n",
    "\n",
    "\n",
    "test_results, nw_error = two_step_model_with_newey_west(data, datat, totalList_ahbs, totalList_nn, impl_volatility, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Training ADHBS model...\n",
      "ADHBS Model (Train) RMSE: 0.31230109153281654, R²: 0.46675377295588727\n",
      "Step 1.1: Generating ADHBS Predictions for Test Data...\n",
      "Step 2: Training XGBoost on ADHBS Errors...\n",
      "XGBoost Model RMSE: 0.15895866851811344, R²: 0.740927432945091\n",
      "Step 2.1: Generating XGBoost Corrections for Test Data...\n",
      "Out-of-Sample RMSE: 0.1359\n",
      "Out-of-Sample R²: 0.8541\n",
      "Newey-West Standard Error (Based on Daily Averages): 0.004442138917918024\n",
      "Results saved to /Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_xgb.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Function to compute Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    \"\"\"\n",
    "    Computes the Newey-West standard error for the series of prediction errors.\n",
    "    \"\"\"\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    return ols_model.bse[0]  # Standard error of the constant term\n",
    "\n",
    "# Step 1: ADHBS Model\n",
    "def adhbs_model(data, features, target, title):\n",
    "    \"\"\"\n",
    "    Fit an ADHBS model and return the predictions and errors.\n",
    "    \"\"\"\n",
    "    y = data[target]\n",
    "    X = sm.add_constant(data[features])  # Add constant for intercept\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Predictions and error calculations\n",
    "    data['ADHBS_Predictions'] = model.predict(X)\n",
    "    data['Errors'] = y - data['ADHBS_Predictions']\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y, data['ADHBS_Predictions']))\n",
    "    r2 = r2_score(y, data['ADHBS_Predictions'])\n",
    "    print(f\"{title} RMSE: {rmse}, R²: {r2}\")\n",
    "\n",
    "    return model, data['Errors']\n",
    "\n",
    "# Step 2: Train XGBoost model to correct errors\n",
    "def train_xgboost(data, features, target, n_estimators=50, max_depth=5, learning_rate=0.1, subsample=1):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model to predict errors.\n",
    "    \"\"\"\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        random_state=42,\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions and evaluate performance\n",
    "    predictions = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2 = r2_score(y, predictions)\n",
    "\n",
    "    print(f\"XGBoost Model RMSE: {rmse}, R²: {r2}\")\n",
    "    return model\n",
    "\n",
    "# Two-step approach with Newey-West standard error\n",
    "def two_step_model_with_newey_west(train_data, test_data, adhbs_features, xgb_features, target, output_csv_path):\n",
    "    \"\"\"\n",
    "    Execute the two-step modeling process (ADHBS + XGBoost) with Newey-West error calculation.\n",
    "    \"\"\"\n",
    "    # Step 1: ADHBS Model\n",
    "    print(\"Step 1: Training ADHBS model...\")\n",
    "    adhbs_model_trained, train_errors = adhbs_model(train_data, adhbs_features, target, \"ADHBS Model (Train)\")\n",
    "\n",
    "    # Predict on the test data using the trained ADHBS model\n",
    "    print(\"Step 1.1: Generating ADHBS Predictions for Test Data...\")\n",
    "    X_test_adhbs = sm.add_constant(test_data[adhbs_features])\n",
    "    test_data['ADHBS_Predictions'] = adhbs_model_trained.predict(X_test_adhbs)\n",
    "    test_data['Errors'] = test_data[target] - test_data['ADHBS_Predictions']\n",
    "\n",
    "    # Step 2: Train XGBoost on ADHBS residuals\n",
    "    print(\"Step 2: Training XGBoost on ADHBS Errors...\")\n",
    "    train_data['Errors'] = train_errors\n",
    "    xgb_model = train_xgboost(train_data, xgb_features, 'Errors')\n",
    "\n",
    "    # Generate corrections for test data\n",
    "    print(\"Step 2.1: Generating XGBoost Corrections for Test Data...\")\n",
    "    X_test_xgb = test_data[xgb_features].values\n",
    "    test_data['XGB_Corrections'] = xgb_model.predict(X_test_xgb).flatten()\n",
    "\n",
    "    # Combine ADHBS predictions and XGBoost corrections\n",
    "    test_data['Final_Predictions'] = test_data['ADHBS_Predictions'] + test_data['XGB_Corrections']\n",
    "\n",
    "    # Evaluate performance\n",
    "    rmse_oos = np.sqrt(mean_squared_error(test_data[target], test_data['Final_Predictions']))\n",
    "    r2_oos = r2_score(test_data[target], test_data['Final_Predictions'])\n",
    "    print(f\"Out-of-Sample RMSE: {rmse_oos:.4f}\")\n",
    "    print(f\"Out-of-Sample R²: {r2_oos:.4f}\")\n",
    "\n",
    "    # Compute Newey-West standard error\n",
    "    test_data['Prediction_Errors'] = test_data[target] - test_data['Final_Predictions']\n",
    "    daily_avg_errors = test_data.groupby('date')['Prediction_Errors'].mean()\n",
    "    nw_std_error = newey_west_standard_error(daily_avg_errors.values, lag=1)\n",
    "    print(f\"Newey-West Standard Error (Based on Daily Averages): {nw_std_error}\")\n",
    "\n",
    "    # Save the results to a CSV\n",
    "    result_df = pd.DataFrame({\n",
    "        'Date': daily_avg_errors.index,\n",
    "        'Daily Avg Errors': daily_avg_errors.values,\n",
    "        'Newey-West Std Error': [nw_std_error] * len(daily_avg_errors)\n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return test_data, nw_std_error\n",
    "\n",
    "# Example usage\n",
    "target = impl_volatility  # Target variable\n",
    "output_csv_path = '/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_xgb.csv'\n",
    "\n",
    "test_results, nw_error = two_step_model_with_newey_west(data, datat, totalList_ahbs, totalList_xgb, target, output_csv_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Training ADHBS model...\n",
      "ADHBS Model (Train) RMSE: 0.31230109153281654, R²: 0.46675377295588727\n",
      "Step 1.1: Generating ADHBS Predictions for Test Data...\n",
      "Step 2: Training Random Forest on ADHBS Errors...\n",
      "Random Forest Model RMSE: 0.1388, R²: 0.8024\n",
      "Step 2.1: Generating Random Forest Corrections for Test Data...\n",
      "Out-of-Sample RMSE: 0.1354\n",
      "Out-of-Sample R²: 0.8552\n",
      "Newey-West Standard Error (Based on Daily Averages): 0.0043\n",
      "Results saved to /Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_rf.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Train Random Forest model to correct errors\n",
    "def train_random_forest(data, features, target, n_estimators=50, max_depth=5, max_features=None, min_samples_split=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model to predict errors.\n",
    "    \"\"\"\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions and evaluate performance\n",
    "    predictions = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2 = r2_score(y, predictions)\n",
    "\n",
    "    print(f\"Random Forest Model RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Two-step approach with Newey-West standard error\n",
    "def two_step_model_with_newey_west_rf(train_data, test_data, adhbs_features, rf_features, target, output_csv_path):\n",
    "    \"\"\"\n",
    "    Execute the two-step modeling process (ADHBS + Random Forest) with Newey-West error calculation.\n",
    "    \"\"\"\n",
    "    # Step 1: ADHBS Model\n",
    "    print(\"Step 1: Training ADHBS model...\")\n",
    "    adhbs_model_trained, train_errors = adhbs_model(train_data, adhbs_features, target, \"ADHBS Model (Train)\")\n",
    "\n",
    "    # Predict on the test data using the trained ADHBS model\n",
    "    print(\"Step 1.1: Generating ADHBS Predictions for Test Data...\")\n",
    "    X_test_adhbs = sm.add_constant(test_data[adhbs_features])\n",
    "    test_data['ADHBS_Predictions'] = adhbs_model_trained.predict(X_test_adhbs)\n",
    "    test_data['Errors'] = test_data[target] - test_data['ADHBS_Predictions']\n",
    "\n",
    "    # Step 2: Train Random Forest on ADHBS residuals\n",
    "    print(\"Step 2: Training Random Forest on ADHBS Errors...\")\n",
    "    train_data['Errors'] = train_errors\n",
    "    rf_model = train_random_forest(train_data, rf_features, 'Errors', n_estimators, max_depth, max_features)\n",
    "\n",
    "    # Generate corrections for test data\n",
    "    print(\"Step 2.1: Generating Random Forest Corrections for Test Data...\")\n",
    "    X_test_rf = test_data[rf_features].values\n",
    "    test_data['RF_Corrections'] = rf_model.predict(X_test_rf).flatten()\n",
    "\n",
    "    # Combine ADHBS predictions and Random Forest corrections\n",
    "    test_data['Final_Predictions'] = test_data['ADHBS_Predictions'] + test_data['RF_Corrections']\n",
    "\n",
    "    # Evaluate performance\n",
    "    rmse_oos = np.sqrt(mean_squared_error(test_data[target], test_data['Final_Predictions']))\n",
    "    r2_oos = r2_score(test_data[target], test_data['Final_Predictions'])\n",
    "    print(f\"Out-of-Sample RMSE: {rmse_oos:.4f}\")\n",
    "    print(f\"Out-of-Sample R²: {r2_oos:.4f}\")\n",
    "\n",
    "    # Compute Newey-West standard error\n",
    "    test_data['Prediction_Errors'] = test_data[target] - test_data['Final_Predictions']\n",
    "    daily_avg_errors = test_data.groupby('date')['Prediction_Errors'].mean()\n",
    "    nw_std_error = newey_west_standard_error(daily_avg_errors.values, lag=1)\n",
    "    print(f\"Newey-West Standard Error (Based on Daily Averages): {nw_std_error:.4f}\")\n",
    "\n",
    "    # Save the results to a CSV\n",
    "    result_df = pd.DataFrame({\n",
    "        'Date': daily_avg_errors.index,\n",
    "        'Daily Avg Errors': daily_avg_errors.values,\n",
    "        'Newey-West Std Error': [nw_std_error] * len(daily_avg_errors)\n",
    "    })\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "    return test_data, nw_std_error\n",
    "\n",
    "# Define hyperparameters for Random Forest\n",
    "n_estimators = 100  # Optimal value based on tuning\n",
    "max_depth = 10      # Optimal value based on tuning\n",
    "max_features = 5    # Optimal value based on tuning\n",
    "min_samples_split = 2  # Optimal value based on tuning\n",
    "\n",
    "# Example usage\n",
    "target = impl_volatility  # Target variable\n",
    "output_csv_path = '/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Notebooks/Notebook_modeling/db_performance_evaluation/ahbs_rf.csv'\n",
    "\n",
    "# Run the two-step model process with Random Forest\n",
    "test_results, nw_error = two_step_model_with_newey_west_rf(data, datat, totalList_ahbs, totalList_xgb, target, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Running ADHBS Model on Training Data...\n",
      "ADHBS Model (Train) RMSE: 0.2877890099145444, R²: 0.4491437787290212\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.1: Generating ADHBS Predictions for Test Data...\n",
      "Step 2: Training Neural Network on Training Data Errors...\n",
      "XGBoost Model RMSE: 0.12376223604556358, R²: 0.8150612642728172\n",
      "Step 2.1: Generating NN Corrections for Test Data...\n",
      "Out-of-Sample RMSE: 0.1093\n",
      "Out-of-Sample R²: 0.8740\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Function to compute Newey-West standard error\n",
    "def newey_west_standard_error(errors, lag=1):\n",
    "    X = np.ones(len(errors))  # Constant term for OLS\n",
    "    ols_model = sm.OLS(errors, X).fit(cov_type='HAC', cov_kwds={'maxlags': lag})\n",
    "    return ols_model.bse[0]  # Standard error of the constant term\n",
    "\n",
    "# Step 1: ADHBS Model\n",
    "def adhbs_model(data, features, target, title):\n",
    "    y = data[target]\n",
    "    X = sm.add_constant(data[features])  # Add constant for intercept\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Predictions and error calculations\n",
    "    data['Predictions'] = model.predict(X)\n",
    "    data['Errors'] = y - data['Predictions']\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y, data['Predictions']))\n",
    "    r2 = r2_score(y, data['Predictions'])\n",
    "    print(f\"{title} RMSE: {rmse}, R²: {r2}\")\n",
    "\n",
    "    # Feature importance\n",
    "    coefs = model.params.abs()\n",
    "    importance = coefs / coefs.sum()\n",
    "    top_features = importance.sort_values(ascending=False).head(10)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features.plot(kind='barh', color='blue', alpha=0.6, edgecolor='black')\n",
    "    plt.title(f\"{title} Feature Importance\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, data['Errors']\n",
    "\n",
    "def train_xgboost(data, features, target, n_estimators=175, max_depth=3, learning_rate=0.1, subsample=1, random_state=42):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model to predict target values (errors in this case).\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the features and target.\n",
    "    - features: List of feature columns.\n",
    "    - target: Target column name (errors in this case).\n",
    "    - n_estimators: Number of boosting rounds.\n",
    "    - max_depth: Maximum tree depth for base learners.\n",
    "    - learning_rate: Learning rate (eta).\n",
    "    - subsample: Subsample ratio of the training instances.\n",
    "    - random_state: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - Trained XGBoost model.\n",
    "    \"\"\"\n",
    "    X = data[features].values\n",
    "    y = data[target].values\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        random_state=random_state,\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions and evaluate performance\n",
    "    predictions = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2 = r2_score(y, predictions)\n",
    "\n",
    "    print(f\"XGBoost Model RMSE: {rmse}, R²: {r2}\")\n",
    "    return model\n",
    "\n",
    "# Hyperparameters for XGBoost\n",
    "# n_estimators = 175\n",
    "# max_depth = 5\n",
    "# learning_rate = 0.1\n",
    "# subsample = 1\n",
    "\n",
    "\n",
    "def two_step_model_oos(train_data, test_data, totalList_ahbs, totalList_xgb, target):\n",
    "    \"\"\"\n",
    "    Execute the two-step modeling process (ADHBS + NN) for out-of-sample predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: Training dataset (in-sample data).\n",
    "    - test_data: Testing dataset (out-of-sample data).\n",
    "    - adhbs_features: Features for the ADHBS model.\n",
    "    - nn_features: Features for the Neural Network.\n",
    "    - target: Target column name (implied volatility in this case).\n",
    "    \n",
    "    Returns:\n",
    "    - Final OOS predictions.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Running ADHBS Model on Training Data...\")\n",
    "    adhbs_model_trained, train_errors = adhbs_model(train_data, totalList_ahbs, target, \"ADHBS Model (Train)\")\n",
    "\n",
    "    # Predict on the test data using the trained ADHBS model\n",
    "    print(\"Step 1.1: Generating ADHBS Predictions for Test Data...\")\n",
    "    X_test = sm.add_constant(test_data[totalList_ahbs])\n",
    "    test_data['ADHBS_Predictions'] = adhbs_model_trained.predict(X_test)\n",
    "\n",
    "    # Calculate residuals for the test data\n",
    "    test_data['Errors'] = test_data[target] - test_data['ADHBS_Predictions']\n",
    "\n",
    "    print(\"Step 2: Training Neural Network on Training Data Errors...\")\n",
    "    train_data['Errors'] = train_errors\n",
    "    xgb_model = train_xgboost(train_data, totalList_xgb, 'Errors')  # Train NN on training data errors\n",
    "\n",
    "    print(\"Step 2.1: Generating NN Corrections for Test Data...\")\n",
    "    X_xgb_test = test_data[totalList_xgb].values\n",
    "    test_data['XGB_Corrections'] = xgb_model.predict(X_xgb_test).flatten()\n",
    "\n",
    "    # Combine ADHBS predictions and NN corrections for final OOS predictions\n",
    "    test_data['Final_Predictions'] = test_data['ADHBS_Predictions'] + test_data['XGB_Corrections']\n",
    "\n",
    "    # Evaluate performance on the out-of-sample data\n",
    "    rmse_oos = np.sqrt(mean_squared_error(test_data[target], test_data['Final_Predictions']))\n",
    "    r2_oos = r2_score(test_data[target], test_data['Final_Predictions'])\n",
    "\n",
    "    print(f\"Out-of-Sample RMSE: {rmse_oos:.4f}\")\n",
    "    print(f\"Out-of-Sample R²: {r2_oos:.4f}\")\n",
    "\n",
    "    return test_data[['date', 'Ticker', target, 'Final_Predictions', 'ADHBS_Predictions', 'XGB_Corrections', 'Errors']], xgb_model\n",
    "\n",
    "oos_results, final_nn_model = two_step_model_oos(data, datat, totalList_ahbs, totalList_xgb, 'impl_volatility')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
