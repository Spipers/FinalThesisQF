{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "# from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "data_train = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_train_tech_nonscaled.parquet')\n",
    "\n",
    "data_test = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/updated_standardization/data_set_test_tech_nonscaled.parquet')\n",
    "\n",
    "firm_data = pd.read_parquet('/Users/sbjpipers/Desktop/FinalThesisQF/FinalThesisQF/Data/Firm_variables/daily_firm_data_median_new.parquet')\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['trading_days_till_exp'] + list(firm_data.columns[2:]) + ['moneyness_squared', 'tau_squared', 'moneyness_tau', 'best_offer_option', 'best_bid_option']\n",
    "# # columns_to_drop = ['trading_days_till_exp']\n",
    "\n",
    "\n",
    "# Drop columns from datasets if they exist\n",
    "data_train = data_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "test_data = data_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure that data_val and data_test have the same column order as data_train\n",
    "data_test = data_test[data_train.columns]\n",
    "\n",
    "data_trains = data_train.copy()\n",
    "\n",
    "# data_train = data_trains[data_trains['date'] < '2020-01-01']\n",
    "# data_validate = data_trains[data_trains['date'] >= '2020-01-01']\n",
    "\n",
    "# (data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'data_train' and 'data_test' are already defined and loaded\n",
    "\n",
    "# # Top features for Call and Put options\n",
    "# # Top features for Call and Put options\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'volume_option', 'prev2_day_iv', 'gold_price']\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'volume_option', 'prev2_day_iv', '5_day_rolling_return_stock']\n",
    "\n",
    "top_features_c =['cp_flag', 'Ticker', 'date', 'impl_volatility','prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond', 'spread_stock', 'spread_vix', 'moneyness', '5_day_rolling_return_stock']\n",
    "top_features_p =['cp_flag', 'Ticker', 'date', 'impl_volatility','prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '5_day_rolling_return_stock', 'RET', 'moneyness', 'PRC_actual', 'CLOSE_vix']\n",
    "\n",
    "\n",
    "# top_features_c = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock', '1Y_bond']\n",
    "# top_features_p = ['cp_flag', 'Ticker', 'date', 'impl_volatility', 'prev_day_iv', 'T', 'vol_stock', 'prev2_day_iv', 'hi-lo_stock']\n",
    "\n",
    "\n",
    "# # Prepare train data for Call and Put options\n",
    "data_train_c = data_train[data_train['cp_flag'] == 'C'][top_features_c]\n",
    "data_train_p = data_train[data_train['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare validation data for Call and Put options\n",
    "# # data_validate_c = data_validate[data_validate['cp_flag'] == 'C'][top_features_c]\n",
    "# # data_validate_p = data_validate[data_validate['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # Prepare test data for Call and Put options\n",
    "data_test_c = data_test[data_test['cp_flag'] == 'C'][top_features_c]\n",
    "data_test_p = data_test[data_test['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # data_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Prepare train data for Call and Put options\n",
    "# data_train_c = data_train[data_train['cp_flag'] == 'C']\n",
    "# data_train_p = data_train[data_train['cp_flag'] == 'P']\n",
    "\n",
    "# # # Prepare validation data for Call and Put options\n",
    "# # # data_validate_c = data_validate[data_validate['cp_flag'] == 'C'][top_features_c]\n",
    "# # # data_validate_p = data_validate[data_validate['cp_flag'] == 'P'][top_features_p]\n",
    "\n",
    "# # # Prepare test data for Call and Put options\n",
    "# data_test_c = data_test[data_test['cp_flag'] == 'C']\n",
    "# data_test_p = data_test[data_test['cp_flag'] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Call Options...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004083 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   3.4s\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   3.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   3.6s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   5.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   6.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   6.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   8.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   8.8s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   8.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=  10.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   5.5s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   5.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.7s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   1.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   6.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   5.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   6.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.3s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014943 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2268\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.654958\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 48231, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.596373\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2264\n",
      "[LightGBM] [Info] Number of data points in the train set: 48232, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.433734\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.0s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.1s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   6.6s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   6.0s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   5.3s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   4.5s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   5.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 72347, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.561687\n",
      "\n",
      "Best Parameters for Call Options: {'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 50, 'num_leaves': 50}\n",
      "\n",
      "Training the model with early stopping for Call Options...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2275\n",
      "[LightGBM] [Info] Number of data points in the train set: 72347, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.561687\n",
      "\n",
      "Performance for Call Options:\n",
      "In-Sample RMSE (Training): 0.0830\n",
      "In-Sample R² (Training): 0.9489\n",
      "Out-of-Sample RMSE (Test): 0.1255\n",
      "Out-of-Sample R² (Test): 0.7232\n",
      "Warning: In-Sample R² is too high (0.9489). Consider increasing regularization or reducing model complexity.\n",
      "Running GridSearchCV for Put Options...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   3.1s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=50, num_leaves=50; total time=   4.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   6.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   6.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=100, num_leaves=50; total time=   6.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   7.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   8.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=150, num_leaves=50; total time=   8.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.0s\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.1s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   3.8s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=  10.9s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.3s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.001, max_depth=20, n_estimators=200, num_leaves=50; total time=   9.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   1.9s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   5.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   5.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=50, num_leaves=50; total time=   2.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=150, num_leaves=50; total time=   6.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.3s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2290\n",
      "[LightGBM] [Info] Number of data points in the train set: 48642, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.700731\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.0s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2286\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.649678\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.4s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=100, num_leaves=50; total time=   4.4s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 48643, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.464087\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=200, num_leaves=50; total time=   8.6s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   5.6s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=150, num_leaves=50; total time=   5.0s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   5.3s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50; total time=   4.5s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 72964, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.604831\n",
      "\n",
      "Best Parameters for Put Options: {'learning_rate': 0.1, 'max_depth': 20, 'n_estimators': 50, 'num_leaves': 50}\n",
      "\n",
      "Training the model with early stopping for Put Options...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2288\n",
      "[LightGBM] [Info] Number of data points in the train set: 72964, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.604831\n",
      "\n",
      "Performance for Put Options:\n",
      "In-Sample RMSE (Training): 0.0856\n",
      "In-Sample R² (Training): 0.9528\n",
      "Out-of-Sample RMSE (Test): 0.1265\n",
      "Out-of-Sample R² (Test): 0.7370\n",
      "Warning: In-Sample R² is too high (0.9528). Consider increasing regularization or reducing model complexity.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Set the seed before training\n",
    "set_seed(42)\n",
    "\n",
    "def prepare_data_with_gridsearch(train_data, test_data, option_type):\n",
    "    \"\"\"\n",
    "    Prepare the data, train LightGBM using GridSearchCV, and evaluate both in-sample and out-of-sample performance.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): The training dataset.\n",
    "    test_data (pd.DataFrame): The testing dataset.\n",
    "    option_type (str): Call or Put option type for labeling the print output.\n",
    "    \"\"\"\n",
    "    # Prepare the train and test data\n",
    "    X_train = train_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Training features\n",
    "    y_train = train_data['impl_volatility']  # Training target\n",
    "    \n",
    "    X_test = test_data.drop(columns=['impl_volatility', 'date', 'Ticker', 'cp_flag'])  # Test features\n",
    "    y_test = test_data['impl_volatility']  # Test target\n",
    "\n",
    "    # Define parameter grid for GridSearchCV with regularization terms\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'max_depth': [ 20],\n",
    "        'num_leaves': [  50],\n",
    "    }\n",
    "\n",
    "    # param_grid = {'colsample_bytree': [1.0], 'learning_rate': [0.1], 'max_depth': [10], 'n_estimators': [100], 'num_leaves': [63], 'subsample': [0.6]}\n",
    "\n",
    "    # Initialize the LGBMRegressor\n",
    "    lgbmodel = LGBMRegressor()\n",
    "\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search = GridSearchCV(estimator=lgbmodel, param_grid=param_grid, \n",
    "                               cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "    print(f\"Running GridSearchCV for {option_type} Options...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and use them for the final model\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"\\nBest Parameters for {option_type} Options: {best_params}\")\n",
    "\n",
    "    # Initialize the best model with early stopping manually\n",
    "    best_LGBM_model = LGBMRegressor(**best_params)\n",
    "\n",
    "    print(f\"\\nTraining the model with early stopping for {option_type} Options...\")\n",
    "    best_LGBM_model.fit(X_train, y_train, \n",
    "                        eval_set=[(X_test, y_test)])\n",
    "\n",
    "    # In-sample (training set) predictions\n",
    "    y_train_pred = best_LGBM_model.predict(X_train)\n",
    "\n",
    "    # Out-of-sample (test set) predictions\n",
    "    y_test_pred = best_LGBM_model.predict(X_test)\n",
    "\n",
    "    # Evaluate In-Sample Performance (on Training Data)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "    # Evaluate Out-of-Sample Performance (on Test Data)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\nPerformance for {option_type} Options:\")\n",
    "    print(f\"In-Sample RMSE (Training): {rmse_train:.4f}\")\n",
    "    print(f\"In-Sample R² (Training): {r2_train:.4f}\")\n",
    "    print(f\"Out-of-Sample RMSE (Test): {rmse_test:.4f}\")\n",
    "    print(f\"Out-of-Sample R² (Test): {r2_test:.4f}\")\n",
    "\n",
    "    # Ensure in-sample R² is less than 0.9\n",
    "    if r2_train >= 0.9:\n",
    "        print(f\"Warning: In-Sample R² is too high ({r2_train:.4f}). Consider increasing regularization or reducing model complexity.\")\n",
    "\n",
    "    return best_LGBM_model\n",
    "\n",
    "# Call the function for Call options data\n",
    "best_model_call = prepare_data_with_gridsearch(data_train_c, data_test_c, 'Call')\n",
    "\n",
    "# Call the function for Put options data (uncomment to run)\n",
    "best_model_put = prepare_data_with_gridsearch(data_train_p, data_test_p, 'Put')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
